---
title: 计算机体系结构期末复习
date: 2025-12-25
---


# 体系结构期末复习（精讲版 + 原文附录）

::: tip 使用说明
- 本文在不删减原文信息的前提下，补充了每个考点的「详解」：**知识点 → 记忆点 → 易错点 → 解题技巧/计算模板**。
- 文末附有 **《原文（未删减）》**，便于对照与查漏补缺。
:::

## 题型与分值结构（按原文）
| 题型 | 数量 | 分值 |
|---|---:|---:|
| 选择题 | 10题 | 20分 |
| 判断题 | 10题 | 20分 |
| 名词解释 | 5题 | 15分 |
| 简答题 | 5题 | 30分 |
| 计算题 | 2题 | 15分 |

::: warning 复习优先级建议
1. **必考且可计算的**：CPU 性能公式 / CPI / Amdahl / 流水线指标 / 对齐与结构体大小 / 加速比与效率  
2. **高频名词解释**：ISA、微架构、局部性、命中率/缺失率/缺失损失、NUMA/ccNUMA、SIMD/SIMT、虚拟化相关概念  
3. **容易混淆的对比题**：冯诺依曼 vs 哈佛、并行 vs 并发、分布式 vs 并行、写直达 vs 写回、SOA vs 微服务、Type1 vs Type2（注意课件/讲义可能存在表述差异）
:::

## 第一章：冯诺依曼结构与 ISA 基础

::: tip 章节导读
本章覆盖通用计算机的基本组织、存储系统层次与局部性、ISA/微架构区分，以及与考试强相关的**性能/对齐/寻址/位运算**等计算与概念题。
:::

### 1. 冯诺依曼结构：基本特点（与哈佛结构对比入口）

#### 原文要点（摘录）
- 由三部分硬件组成：CPU（控制单元、ALU、寄存器、PC）、主存储器、IO 系统
- 在存储器中，指令和数据同等对待；指令执行顺序
- 指令由操作码和地址码组成；指令/数据二进制编码

#### 详解
冯诺依曼结构的“核心抽象”是：**程序（指令）与数据共享同一主存与同一通路**，CPU按“取指—译码—执行”顺序推进。

- **知识点**  
  - *存储程序思想*：程序以指令序列形式存放在主存中；CPU通过PC逐条取出指令执行。  
  - *统一编址*：同一地址空间里既可能是“指令”，也可能是“数据”；是否当作指令由取指/译码阶段决定。  
  - *顺序执行为默认*：除分支/跳转等改变PC的指令外，PC通常递增。

- **记忆点（口号）**  
  - “**指令数据一锅端，一条路进CPU**”→ 冯诺依曼  
  - “**指令/数据两套存储两条路**”→ 哈佛

- **易错点**  
  - 不要把“冯诺依曼=只有一个存储器芯片”理解成物理必须如此。考试常指**逻辑组织/访问通路**层面的统一。  
  - 冯诺依曼强调“指令与数据同等对待”，不等于它们“内容完全一样”；而是**存储与访问机制一致**。

- **答题模板（简答题）**  
  1) 组成：CPU+主存+I/O；2) 存储程序；3) 指令/数据统一存储与编址；4) 顺序执行；5) 二进制表示。


### 2. 哈佛结构：与冯诺依曼的区别、优缺点与应用

#### 原文要点（摘录）
- 指令存储器与数据存储器相分离
- 优势：可分别对指令/数据存储单元优化
- 缺点：不够灵活，需要指定程序/数据存储大小且不能相互调整
- 应用：常用于小型嵌入式系统设计

#### 详解
哈佛结构的关键在“**分离**”：**指令存储器**与**数据存储器**（以及对应总线/Cache）物理或逻辑上分离。

- **知识点**  
  - 指令与数据可并行访问：典型好处是在取指的同时访存取数据（减少结构冲突）。  
  - 常见于 MCU/嵌入式：程序存于Flash，数据在SRAM，特性差异明显。

- **记忆点**  
  - “哈佛=**两套存储，两条通路**；冯诺依曼=**一套存储，一条通路**”。

- **易错点**  
  - 现代CPU常见“改良型哈佛”：**L1 I-Cache 与 L1 D-Cache 分离**，但下层（L2/L3/主存）往往统一；题目问“哈佛 vs 冯诺依曼”要看它问的是**概念模型**还是**现代实现**。

- **考试提示**  
  - 对比题建议按：组织方式 → 性能影响（带宽/冲突）→ 灵活性 → 典型场景 来答。


### 3. 冯诺依曼瓶颈（Von Neumann Bottleneck）

#### 原文要点（摘录）
- 主存储器与CPU控制单元之间只有一条单一路径（物理或逻辑）

#### 详解
所谓“瓶颈”是指：CPU与主存之间的数据/指令传输受限于**带宽与延迟**，导致处理器算力提升快而访存跟不上。

- **知识点**  
  - 典型表现：CPU等待内存（stall），有效CPI上升。  
  - 根因：指令流与数据流争用同一通路（冯诺依曼模型下尤为明显）。

- **解法（常考扩展）**  
  - **缓存层次**：L1/L2/L3，利用局部性缓解主存访问。  
  - **预取/乱序执行/多发射**：把可并行的工作提前做，但本质仍在对抗访存延迟。  
  - **增加带宽**：更宽的总线、更多通道、更高频内存；但成本/功耗约束明显。

- **易错点**  
  - “瓶颈”不等于“只有一根线”；重点是**共享通路导致的带宽竞争**与“内存墙”现象。


### 4. 指令执行周期：取指→译码→执行（Fetch-Decode-Execute）

#### 原文要点（摘录）
- 控制单元用PC确定指令位置并取指
- 译码成ALU可理解的控制信号
- 取操作数到寄存器，ALU执行，结果写回寄存器或存储器

#### 详解
这是“最经典的流程题”。答题时把每一步的**关键部件**说清楚即可。

- **知识点（按部件串起来）**  
  1) **PC**给出下一条指令地址；  
  2) **取指**：从指令存储（或I-Cache）读出指令到IR（指令寄存器）；  
  3) **译码**：控制单元解析操作码/寻址方式，生成控制信号；  
  4) **取数**：按寻址方式读寄存器/访存得到操作数；  
  5) **执行**：ALU/浮点单元/访存单元完成运算或地址计算；  
  6) **写回**：写寄存器或写存储器；更新PC（顺序+分支）。

- **易错点**  
  - 分支指令的执行会改变PC，导致控制冒险；如果题目问流水线，就把这里和分支预测关联起来。


### 5. 计算机体系结构设计思想（8个核心理念）

#### 原文要点（摘录）
- 面向摩尔定律的设计；面向抽象简化设计；加速大概率事件
- 通过并行/流水线/预测提高性能；存储器层次；冗余提高可靠性

#### 详解
这类题通常是“列举 + 解释 + 举例”。建议每条用一两句解释它“解决什么矛盾”。

- **面向摩尔定律**：工艺进步带来晶体管预算变化，设计要预估“设计完成时的工艺”。  
- **抽象简化设计**：ISA、虚拟内存、进程等抽象降低复杂度，便于软硬协同。  
- **加速大概率事件（Common case fast）**：优化80%时间走的路径收益最高（与二八法则、一九法则呼应）。  
- **并行**：多核/向量/多线程，用“同时做更多”提高吞吐。  
- **流水线**：把一条指令拆段并重叠执行，提高吞吐（但引入冒险）。  
- **预测**：分支预测/预取等，用“猜测”换性能，错了付出回滚代价。  
- **存储层次**：寄存器→Cache→内存→外存，平衡速度/容量/成本。  
- **冗余可靠性**：ECC、RAID、双机热备等，牺牲资源换可用性/正确性。

::: warning 易错点
“加速大概率事件”强调的是**策略优先级**，不是说小概率事件不用做；考试中常用它来解释“为什么要缓存/为什么要优化分支预测”。
:::


### 6. 摩尔定律与“内存墙”（Memory Wall）

#### 原文要点（摘录）
- 晶体管数目约每18个月翻一番（同样成本）
- 内存墙：CPU与内存差距扩大；解决：加缓存/多级缓存/优化算法

#### 详解
- **知识点**  
  - 摩尔定律带来CPU算力增长快；但DRAM延迟/带宽增长相对慢，形成**内存墙**。  
  - 结果：程序越来越可能“受内存限制（memory bound）”，而不是“受计算限制（compute bound）”。

- **常见缓解手段（按层次回答）**  
  1) **硬件层**：多级Cache、更大带宽、更好的预取、内存控制器优化；  
  2) **体系结构层**：更深/更宽流水线、乱序与多发射掩蔽延迟；  
  3) **软件层**：数据布局/对齐、缓存友好算法、减少随机访存。

- **易错点**  
  - “摩尔失效了吗”这种问法常考**观点题**：单核频率不再按原速度提升，但晶体管预算仍在增长（转向多核/异构）。答题要体现“趋势变化而非彻底失效”。


### 7. 并发（Concurrency） vs 并行（Parallelism）

#### 原文要点（摘录）
- 并发：同一CPU上通过切换“看起来同时”运行多个程序
- 并行：多个CPU/核心真正同时运行

#### 详解
- **知识点**  
  - 并发强调“**时间上交错**”，并行强调“**空间上同时**”。  
  - 并发可以在单核实现（时间片轮转），并行通常需要多核/多处理器。

- **答题要点**  
  - 并发提升“系统响应性/资源利用率”；并行主要提升“吞吐/加速单个任务”。

- **易错点**  
  - 不要把“多线程”自动等同“并行”。多线程在单核上也可能只是并发。


### 8. 流水线冒险：结构/数据/控制（以及分支预测）

#### 原文要点（摘录）
- 结构冒险：资源冲突；解决：增加资源或停顿插泡
- 数据冒险：数据依赖；解决：插泡、编译/动态调度等
- 控制冒险：分支导致下一条不确定；解决：分支预测或停顿

#### 详解
流水线题通常按“三类冒险 + 解决方法”答；如果再追问，就展开到“转发/旁路、RAW/WAR/WAW、预测准确率影响”。

- **结构冒险（Structural）**  
  - 本质：硬件资源不足导致冲突（例：同一周期既要取指又要访数据且共享存储器）。  
  - 典型解法：I/D分离（改良哈佛）、多端口、复制资源；或插入stall。

- **数据冒险（Data）**  
  - 本质：存在数据依赖（最常考RAW：写后读）。  
  - 典型解法：  
    - **旁路/转发（Forwarding）**：结果还没写回寄存器就直接送给后续阶段；  
    - **插泡（Stall/Bubble）**：硬等到数据可用；  
    - **调度**：编译器重排（静态）或硬件乱序（动态）。

- **控制冒险（Control）**  
  - 本质：分支方向/目标未确定，取指可能取错。  
  - 典型解法：  
    - **预测**：静态/动态预测；  
    - **延迟槽（某些ISA）**或**提前计算分支**；  
    - 预测错的代价：流水线冲刷（flush）→ 罚周期。

::: warning 易错点（考试常问）
- “控制冒险”不只分支指令，异常/中断同样会破坏顺序流。  
- “插泡”和“冲刷”不同：插泡是**没干活的空周期**；冲刷是**已经取错/执行的指令作废**。
:::


### 9. Cache 与存储层次：为什么要多级缓存？L1/L2/L3 的典型差异

#### 原文要点（摘录）
- 顶层最快最小最贵，底层最慢最大最便宜
- L1在CPU内部、最靠近核心；L2更大更慢；L3最大、常为多核共享
- 多级缓存平衡：容量、延迟、命中率

#### 详解
- **知识点**  
  - 设计目标是降低平均访存时间（AMAT），利用**时间局部性**与**空间局部性**。  
  - 多级Cache通过“**小而快 + 大而相对慢**”的组合，提高整体命中率同时控制延迟。

- **AMAT（考试极常用公式）**  
  $\mathrm{AMAT}=T_{hit}+m\times P_{miss}$  
  多级时可展开：  
  $\mathrm{AMAT}=T_{L1}+m_{L1}\times\big(T_{L2}+m_{L2}\times(T_{L3}+m_{L3}\times T_{mem})\big)$

- **L1/L2/L3 常见组织（记忆版）**  
  - **L1**：离执行单元最近，常分 I-Cache/D-Cache，追求极低延迟；  
  - **L2**：容量更大，延迟更高，用来降低L1 miss 造成的损失；  
  - **L3**：更大、常共享，用来降低跨核访问主存的次数（现代CPU通常仍在同一芯片上，只是离核心更远）。

- **易错点**  
  - 原文提到“L3在CPU外部”的说法在现代处理器上并不典型；更稳妥的表述是：**L3通常在同一封装/芯片上但更靠外层并可共享**。考试若按课件记忆则以老师口径为准；若是概念题，建议用“通常/一般”这种不踩雷的措辞。


### 10. 写直达（Write-through）与写回（Write-back）

#### 原文要点（摘录）
- 写直达：同时更新缓存与内存，一致性好但耗时
- 写回：用脏位标记，性能好但复杂且有丢失风险

#### 详解
- **知识点**  
  - **写直达**：每次写Cache行时也写主存。优点是一致性简单；缺点是写流量大、延迟高。  
  - **写回**：只写Cache，并把该行标记为dirty；被替换时才写回主存。优点是写合并/减少带宽；缺点是控制复杂、断电风险需靠写回队列/电源保护等。

- **考试常见“组合问法”**  
  1) 写策略（through / back）  
  2) 写缺失策略（write-allocate / no-write-allocate）  
  常见组合：  
  - write-back + write-allocate（写入先把行载入Cache再改）  
  - write-through + no-write-allocate（写缺失直接写内存，不把行载入）

- **易错点**  
  - 写回不等于“不一致”，它是“延迟一致”；一致性由**脏位+替换写回**保证。


### 11. CPU 性能：响应时间/吞吐率、时钟周期、CPU时间与 CPI（计算题核心）

#### 原文要点（摘录）
- 响应时间：完成单个任务总时间；吞吐率：单位时间任务数量
- 时钟周期=频率倒数；CPU性能公式；CPI是统计量

#### 详解
这一节是**计算题必考区**：能从题干提取 IC/CPI/f 并计算 CPU Time、加速比、性能提升。

- **核心公式（务必背熟）**  
  - 时钟周期：$T_{cycle}=\frac{1}{f_{clock}}$  
  - CPU时间（执行时间）：  
    $T_{CPU}=IC\times CPI\times T_{cycle}=\frac{IC\times CPI}{f_{clock}}$  
    其中：  
    - $IC$：指令条数（Instruction Count）  
    - $CPI$：平均每条指令周期数（Cycles Per Instruction）  
    - $f_{clock}$：主频

- **CPI 的计算套路**  
  如果给出指令类别占比与各自CPI：  
  $CPI=\sum_i (\text{freq}_i\times CPI_i)$

- **响应时间 vs 吞吐率（答题角度）**  
  - 用户更关心响应时间（一次任务快不快）；  
  - 数据中心更关心吞吐率（单位时间处理多少）。

- **计算题解题步骤（模板）**  
  1) 统一单位（GHz↔Hz，ns↔s）  
  2) 算 $T_{cycle}$ 或直接用 $T_{CPU}=IC\times CPI/f$  
  3) 若问“性能提升/加速比”：  
     $\text{Speedup}=\frac{T_{old}}{T_{new}}$  
  4) 若只改主频：$T_{CPU}\propto 1/f$；若只改CPI：$T_{CPU}\propto CPI$；若只改IC：$T_{CPU}\propto IC$。

::: warning 易错点
- “响应时间包含I/O/OS开销”与“CPU时间只算CPU执行”要区分；题目写清楚要算哪一个。  
- 主频提高不一定按比例提升性能：如果CPI或IC变化、或程序受内存限制，提升会打折。
:::


### 12. 基准测试：SPEC 与 Linpack

#### 原文要点（摘录）
- SPEC：由真实应用构成，给单项与总体评价
- Linpack：常用于浮点性能测试

#### 详解
- **知识点**  
  - **基准测试（Benchmark）**是可重复的工作负载，用来衡量系统在某类任务上的性能。  
  - **SPEC**倾向于反映综合/真实应用表现；**Linpack**偏重线性代数计算，常用于超算TOP500排名。

- **考试提示**  
  - 名词解释时：给出“组织/目的/构成/输出指标”四要素即可。  
  - 判断题常考：“SPEC是一个组织/联合体”与“SPEC基准程序”概念区分。


### 13. ISA（指令集体系结构）与微架构（Microarchitecture）

#### 原文要点（摘录）
- ISA：软件与硬件的界面，程序员可见的指令集与行为
- 微架构：CPU内部具体实现方式；同一ISA可有不同微架构表现

#### 详解
- **知识点**  
  - **ISA**规定“能做什么、怎么用、语义是什么”（指令、寄存器、寻址方式、异常模型等）。  
  - **微架构**决定“怎么做得更快”（流水线深度、乱序、Cache结构、预测器等）。

- **经典考法**  
  - 问：为什么ISA是通用体系结构必不可少的抽象层？  
    - 因为ISA让软件可面向稳定接口开发，硬件实现可演进；没有ISA，软件无法驱动硬件。  
  - 问：同一ISA不同CPU性能差异原因？  
    - 微架构不同：发射宽度、缓存、分支预测、频率、执行单元数量等。

- **易错点**  
  - 不要把“ISA=汇编语言”。汇编是ISA的符号表示之一，但ISA还包含行为与约束（如内存模型、异常）。


### 14. 寻址空间（Address Space）与内存空间（Memory Space）

#### 原文要点（摘录）
- 寻址空间：CPU能寻址的最大范围（常以字节为单位）
- 16位地址可寻址64K（字节）

#### 详解
- **知识点**  
  - 寻址空间大小由“**可用地址位数**”决定：若按字节编址，地址位数为 $n$，则最大寻址空间为 $2^n$ 字节。  
  - “内存空间”常指“实际存在/可用的内存容量”，可能小于寻址空间（例如32位CPU不一定装满4GB）。

- **易错点**  
  - 题目要看“字节寻址”还是“字寻址”；单位不同，容量计算不同。  
  - “地址”是标识，不是数据本身。


### 15. 大端/小端（Endianness）

#### 原文要点（摘录）
- 大端：高位字节在前；小端：低位字节在前

#### 详解
- **识别题常用例子**  
  设 32 位数：`0x12 34 56 78`  
  - 大端存储（低地址→高地址）：`12 34 56 78`  
  - 小端存储（低地址→高地址）：`78 56 34 12`

- **易错点**  
  - “字节序”只影响**多字节数据在内存中的排列**，不影响寄存器里数值意义。  
  - 网络字节序通常指大端（若课程涉及可顺带一句）。


### 16. 数据对齐（Alignment）与数据打包（Packing）（选择/计算必出）

#### 原文要点（摘录）
- 打包：排布不浪费空间；对齐：结构体成员按对齐数放置；结构体总大小为最大对齐数的整数倍

#### 详解
这部分常以“结构体大小计算”出题：给出成员类型与对齐规则，算各成员偏移与总大小。

- **概念区分**  
  - **对齐**：让数据起始地址满足某个倍数（如4字节对齐），换取更快/更少访存的硬件访问。  
  - **打包**：尽量不留空洞节省空间，但可能导致未对齐访问变慢或在某些架构上触发异常。

- **通用计算规则（按原文口径整理）**  
  1) 成员从偏移0开始放；  
  2) 每个成员的对齐数 = `min(成员大小, 默认对齐数)`；  
  3) 每个成员起始偏移必须是该对齐数的整数倍，不是就补padding；  
  4) 结构体总大小是**最大对齐数**的整数倍（末尾可能补padding）。

- **解题技巧（强烈建议按表格算）**  
  建表：`成员 | 大小 | 对齐 | 起始偏移 | 结束偏移 | padding`  
  从0累加，遇到不满足对齐就向上取整。

::: warning 易错点
- “默认对齐数”可能由编译器/平台决定（如8或16）；题目若不给，通常按课堂默认。  
- 字符数组与结构体嵌套时，最大对齐数会变化，别漏掉“结构体整体对齐”。
:::


### 17. 寻址模式（Addressing Modes）（会给指令让你算有效地址）

#### 原文要点（摘录）
- 直接寻址：立即数/寄存器/内存地址
- 间接寻址：内存间接、寄存器间接、基址+偏移、基址+索引、PC相对

#### 详解
- **核心是“有效地址 EA 怎么算”**  
  题目通常给寄存器值、偏移量、内存内容，要求算EA或最终操作数。

- **常见模式与EA**  
  - 立即数：操作数=常量（不需要EA）  
  - 寄存器：操作数=寄存器内容  
  - 直接（绝对）寻址：EA=指令给出的地址  
  - 寄存器间接：EA=寄存器内容（寄存器里放地址）  
  - 基址+偏移：EA=R[base] + offset（offset在指令里）  
  - 基址+索引：EA=R[base] + R[index]（常用于数组）  
  - PC相对：EA=PC + offset（常用于分支/跳转）

- **基址寄存器 vs 变址寄存器（原文提问点）**  
  - 基址：常指向“结构/数组基地址”，较稳定；  
  - 变址：常存放“下标×元素大小”的偏移，随循环变化。  
  本质都是寄存器参与EA计算，但“语义/用途”不同。

::: tip 解题技巧
看到 `ld r2, offset(rb)` 这类形式，先把 `EA = R[rb] + offset` 写出来，再按是否需要“再访存一次”（间接）判断。
:::


### 18. 位运算与原码/反码/补码；逻辑移位与算术移位（必考）

#### 原文要点（摘录）
- 正数：原/反/补相同；负数：反码=按位取反；补码=反码+1
- 逻辑移位：补0；算术右移复制符号位；算术左移可能溢出

#### 详解
- **三种编码（重点：补码）**  
  - 原码：符号位 + 绝对值  
  - 反码：负数在原码基础上“符号位不变，数值位按位取反”  
  - 补码：负数在反码基础上“+1”（现代CPU有符号整数主流表示）

- **移位规则（务必分清逻辑/算术）**  
  - 逻辑移位：不管正负都补0（把它当无符号数）  
  - 算术右移：复制符号位（等价于有符号除2取整方向依实现/补码语义）  
  - 算术左移：低位补0，符号位不参与移动；可能溢出

- **溢出判断（常考）**  
  算术左移时，如果移出位与新符号位不一致，往往说明发生溢出（补码范围被破坏）。

::: warning 易错点
- 反码体系里“+0/-0”双零问题；补码没有这个问题。  
- “右移补1”只适用于**算术右移的负数**；逻辑右移永远补0。
:::


### 19. 二八法则与一九法则（性能优化思想题）

#### 原文要点（摘录）
- 二八法则：20%的指令使用80%的时间
- 一九法则：90%的时间在10%的代码

#### 详解
- **知识点**  
  两条都是“程序执行不均匀”的经验法则，用来支撑：**优先优化热点（hot path）**。

- **考试关联**  
  - 为什么“加速大概率事件”更有效？因为热点路径支配执行时间。  
  - 为什么缓存有效？因为热点数据/指令会反复访问（时间局部性）。

- **答题技巧**  
  简答题可写：定位热点（profiling）→ 优化热点（算法/数据结构/局部性/并行化）→ 评估收益（Amdahl）。


### 20. RISC 特点（选择/简答必考）

#### 原文要点（摘录）
- 寄存器多；Load/Store 架构（只对寄存器运算）
- 指令等长、正交；指令/数据Cache分离；便于流水线
- 不足：兼容性/应用范围受限（按原文口径）

#### 详解
- **知识点（RISC为流水线而生）**  
  - **Load/Store**：访存指令与算术指令分离，硬件更易流水化。  
  - **固定长度/少寻址模式**：译码简单、时序稳定。  
  - **寄存器多**：减少访存、降低内存墙影响。  
  - **正交性**：指令规则统一、少特例，便于硬件实现与编译器优化。

- **易错点**  
  - 现代ISA（如x86）即使被称为CISC，也会在微架构内部把复杂指令译成“微操作”走RISC化流水线；考试答概念即可，不必展开到工业细节，除非老师强调。


### 21. 局部性原理（时间/空间）与缓存效果

#### 原文要点（摘录）
- 时间局部性：刚用过的很快还会用；空间局部性：访问附近地址概率高

#### 详解
- **知识点**  
  - 时间局部性支撑“把热点数据留在Cache”；  
  - 空间局部性支撑“按块（cache line）取数”。

- **典型例子**  
  - 循环体重复执行（时间局部性）  
  - 数组顺序遍历（空间局部性）

- **易错点/做题技巧**  
  - 题目问“为什么矩阵乘要按某种循环顺序”时，本质是在考局部性。把“让内层循环沿连续内存访问”写出来就对。


### 22. Cache 基本术语：命中率/缺失率/缺失损失（以及 L1为何小、L2为何大）

#### 原文要点（摘录）
- 命中率 h；缺失率 m=1-h；缺失损失 miss penalty
- L1小追求命中时间；L2大追求降低缺失率

#### 详解
- **术语与公式**  
  - 命中率：$h$  
  - 缺失率：$m=1-h$  
  - 缺失损失：发生缺失后为取回数据所付出的额外时间  
  - 平均访存时间：$$AMAT=T_{hit}+m\times P_{miss}$$

- **为什么 L1 小而快、L2 更大**（高频简答）  
  - L1命中时间在每条指令/每次访存都要付出，必须极低 → 牺牲容量。  
  - L2主要影响“L1缺失时的损失”，更关注降低缺失率 → 可以更大、允许稍慢。

- **考试陷阱**  
  - 有的题会给“命中率提升但命中时间变长”，让你用AMAT判断到底变快还是变慢；务必代公式。


## 第二章：并行计算与流水线（并行与流水线指标常考）

::: tip 章节导读
本章覆盖并行体系结构分类、并行/分布式概念对比、加速比与Amdahl、数据相关性、以及流水线的一系列定义与指标（选择/计算高频）。
:::

### 1. TOP500：常见操作系统、测试集与 FLOPS 单位

#### 原文要点（摘录）
- Linux占比最多；测试集 Linpack
- MFLOPS/GFLOPS/TFLOPS/PFLOPS 等单位换算

#### 详解
- **知识点**  
  - TOP500排名使用的经典基准是 Linpack，输出浮点性能（FLOPS）。  
  - 单位换算（考试常给数量级题）：  
    - 1 MFLOPS = $10^6$ FLOPS  
    - 1 GFLOPS = $10^9$ FLOPS  
    - 1 TFLOPS = $10^{12}$ FLOPS  
    - 1 PFLOPS = $10^{15}$ FLOPS

- **解题技巧**  
  - 题目问“千万亿次/秒”就是 $10^{15}$，对应 **P**（peta）。  
  - 看到“百万亿次/秒”就是 $10^{17}$（不常用单位，但可写成 100 PFLOPS）。


### 2. 并行计算的三个基本条件

#### 原文要点（摘录）
- 并行机（>=2处理器）+ 应用具有并行度（可分解子任务）+ 并行编程环境

#### 详解
- **知识点**  
  1) **硬件**：有多个处理单元与互连网络；  
  2) **算法**：问题可并行分解（并行度）；  
  3) **软件/实现**：用并行编程模型实现（MPI/OpenMP/CUDA等）。

- **易错点**  
  - “有多核”不代表“程序就能加速”；如果串行部分大，Amdahl会限制收益。  
  - 并行度与粒度有关：分解太细会被通信/同步开销吃掉。


### 3. Flynn 分类（SISD/SIMD/MISD/MIMD）

#### 原文要点（摘录）
- 按指令流/数据流多倍性分类
- SISD：单指令单数据；SIMD：单指令多数据；MISD：多指令单数据；MIMD：多指令多数据

#### 详解
- **知识点**  
  - **SIMD**：同一条指令驱动多数据并行（向量指令/NEON/AVX）。  
  - **MIMD**：多核/多处理器，各自执行不同指令流（主流并行机）。

- **易错点**  
  - MISD很少见，考试通常作为干扰项；常见回答：容错/流水线概念上可类比，但工业实现少。


### 4. SIMD vs SIMT（先共同点后不同点，必考）

#### 原文要点（摘录）
- 共同点：单指令流控制多数据并行
- 不同点：SIMT以线程为抽象，更灵活；SIMD要求向量数据更规则

#### 详解
- **共同点**  
  都是在“同一控制路径”下对多份数据做相同/相似操作，追求数据并行。

- **关键区别**  
  - **SIMD**：一条向量指令处理固定宽度向量（如256-bit）；更依赖数据连续与对齐。  
  - **SIMT**：以“线程”组织（GPU的warp/wavefront），硬件以SIMD方式执行一组线程，但允许线程有独立寄存器/地址；分支会导致warp发散降低效率。

- **考试高频点**  
  - “SIMT看起来是多线程，本质是以SIMD方式锁步执行”；  
  - “分支发散/不规则访存会显著降低GPU效率”。

::: tip 结合SPMD
SPMD（单程序多数据）是**程序级**并行描述：同一程序在不同数据上运行，内部可以分支；SIMD/SIMT是**硬件/执行级**的并行形态。两者不是同一维度。
:::


### 5. NUMA vs ccNUMA

#### 原文要点（摘录）
- NUMA：共享内存但访问延迟非均匀；优点降低带宽瓶颈；缺点访问时间差异
- ccNUMA：在NUMA基础上加入Cache并用一致性协议保持一致

#### 详解
- **知识点**  
  - **NUMA**：物理内存分布在各节点，本地访问快、远程访问慢。  
  - **ccNUMA**：加入Cache一致性（coherent cache），让编程模型更像SMP，但需要一致性协议支持。

- **考试提示**  
  - 简答题常要求：优点（可扩展、带宽提升）+ 缺点（非均匀延迟、数据本地化困难）+ 应对（数据本地化、绑定线程/内存、NUMA-aware分配）。

- **易错点**  
  - ccNUMA不是“没有非均匀性”，而是“对程序员更透明但仍需关注数据放置”。


### 6. 向量机、标量机、阵列机：概念与差异

#### 原文要点（摘录）
- 向量机：向量数据表示与向量指令；标量机：无向量表示；阵列机：多处理单元阵列由单一控制部件控制

#### 详解
- **知识点**  
  - **标量（Scalar）**：一次操作一个数据元素。  
  - **向量（Vector）**：一次操作一串元素（向量寄存器+向量指令）。  
  - **阵列（Array Processor）**：多处理单元并列，常在同一控制下做相同操作，属于数据并行的一种硬件组织方式。

- **易错点**  
  - “向量机”强调指令与数据类型；“阵列机”强调硬件由大量处理单元组成。  
  - 有些教材把阵列机视为一种实现SIMD/向量思想的方式，答题以课程口径为准。


### 7. Socket / Core / Thread / Cluster（概念题）

#### 原文要点（摘录）
- socket=CPU插槽；core=物理核心；thread=逻辑执行上下文（超线程）；cluster=多机集群

#### 详解
- **知识点**  
  - Socket：主板上安装CPU的物理插槽  
  - Core：独立的执行单元（算术/寄存器/流水线等）  
  - Thread：同一核心上共享执行资源的多个硬件线程（SMT/Hyper-Threading）  
  - Cluster：多台机器通过网络互连协同计算（分布式/集群）

- **易错点**  
  - 超线程不是“翻倍算力”，它主要提升资源利用率，对内存受限程序收益有限。


### 8. 并行计算 vs 分布式计算（必考对比）

#### 原文要点（摘录）
- 并行：单系统多处理器解决同一问题；分布式：多系统松散耦合

#### 详解
- **知识点**  
  - 并行计算更强调“加速单个问题/缩短执行时间”，通常通信频繁、细粒度。  
  - 分布式更强调“可扩展/可靠/高可用”，通信相对粗粒度，关注长期运行稳定性。

- **答题模板**  
  定义 → 耦合程度（紧/松）→ 通信开销（低/高）→ 目标（时间/可用性）→ 典型场景（多核 vs 集群）。


### 9. 加速比与效率（计算题）

#### 原文要点（摘录）
- S=Ts/Tp；E=S/P

#### 详解
- **知识点**  
  - 加速比：$S=\frac{T_s}{T_p}$
  - 效率：$E=\frac{S}{P}$，衡量处理器利用率

- **计算题套路**  
  1) 先明确 $T_s$（串行时间）与 $T_p$（并行时间）  
  2) 算 $S$  
  3) 给出处理器数 $P$ 后算 $E$

- **易错点**  
  - $T_p$ 往往包含通信/同步开销；题目可能暗含这些项。  
  - 可能出现“超线性加速”（$S>P$），多因缓存/搜索空间剪枝等效应；若题目出现不必慌，先按公式算。


### 10. 阿姆达尔定律（Amdahl's Law）（必考）

#### 原文要点（摘录）
- 若串行部分占 1/f，则加速比不可能超过 f（原文表述）

#### 详解
更标准的写法是用可并行比例 $p$ 表示：

- **公式**  
  若串行比例为 $(1-p)$，并行比例为 $p$，使用 $N$ 个处理器：  
  $S(N)=\frac{1}{(1-p)+\frac{p}{N}}$
  当 $N\to\infty$：  
  $S_{max}=\frac{1}{1-p}$

- **答题要点**  
  - 串行部分决定上限：哪怕无限核也无法突破。  
  - 用于解释“为什么并行收益会递减”。

- **易错点**  
  - 把“串行比例”与“串行时间”混用。比例是相对量，题目可能给绝对时间，也可能给百分比。


### 11. 数据相关性（RAW/WAR/WAW）（必考）

#### 原文要点（摘录）
- 写后读（RAW）、读后写（WAR）、写后写（WAW）

#### 详解
- **三种相关性（与流水线/乱序强相关）**  
  - RAW（Read After Write）：真相关，后指令读前指令写的结果，必须保持顺序。  
  - WAR（Write After Read）：反相关，后指令写会覆盖前指令要读的值，乱序时要小心。  
  - WAW（Write After Write）：输出相关，两条写同一目的，乱序会导致最终值错误。

- **考试提示**  
  - 若题目配合“寄存器重命名”，通常是在考：重命名可消除 WAR/WAW，但不能消除 RAW。


### 12. 并行粒度：粗粒度 vs 细粒度

#### 原文要点（摘录）
- 粒度由计算/通信比例决定；粗粒度通信少计算多；细粒度交互频繁

#### 详解
- **知识点**  
  - 粗粒度更适合分布式/集群（通信昂贵）；  
  - 细粒度更常见于共享内存/线程级并行。

- **易错点**  
  - 粒度并非越细越好：细粒度带来同步与通信开销，可能让效率下降。


### 13. 流水线术语：段/深度/瓶颈/通过时间/排空时间；静态/动态、线性/非线性、顺序/乱序

#### 原文要点（摘录）
- 段=子过程功能部件；深度=段数；最长段是瓶颈
- 通过时间=第1个任务从进入到输出；排空时间=最后任务从进入到输出
- 分类：静态/动态；线性/非线性；顺序/乱序

#### 详解
这部分常出“名词解释”和“选择题辨析”。

- **关键定义**  
  - 段（stage）：流水线的一个子功能部件  
  - 深度：段数  
  - 瓶颈：耗时最长的段（决定最小节拍）  
  - 通过时间：第一件产品出线时间  
  - 排空时间：最后一件产品出线时间

- **分类速记**  
  - 静态 vs 动态：同一时刻能否支持多种连接方式  
  - 线性 vs 非线性：是否存在反馈回路  
  - 顺序 vs 乱序：输出顺序是否必须等于输入顺序

::: warning 易错点
“乱序流水线”是输出顺序可变；与“乱序执行（OOO）”概念相近但不完全等同，考试按课件定义答。
:::


### 14. 流水线指标：吞吐率、加速比、效率（必考，选择/计算）

#### 原文要点（摘录）
- 吞吐率=单位时间完成任务数；加速比=顺序时间/流水线时间；效率=设备利用率

#### 详解
- **吞吐率（Throughput）**  
  $TP=\frac{n}{T_k}$
  其中 $n$ 为任务数，$T_k$ 为完成 $n$ 个任务的总时间。

- **加速比（Speedup）**  
  $S=\frac{T_s}{T_k}$

- **效率（Efficiency / Utilization）**  
  直观理解：流水线段在总时间里“真正忙碌”的比例。常见近似：  
  $E=\frac{S}{k}$
  其中 $k$ 为流水线段数（深度），在很多教材/题型中采用该关系（以课堂公式为准）。

- **计算题技巧**  
  1) 先写出“非流水线”总时间：各段时间相加乘以任务数  
  2) 流水线总时间：通过时间 + (n-1)×节拍（节拍由最长段决定）  
  3) 再代入加速比/吞吐率/效率

::: tip 一句话
流水线提升的是**吞吐率**；单个任务的“通过时间”未必变短，甚至可能变长。
:::


### 15. MPI / OpenMP / CUDA：适用场景与特点

#### 原文要点（摘录）
- MPI：消息传递、可扩展、可移植；OpenMP：共享内存多线程；CUDA：GPU线程级并行

#### 详解
- **MPI（消息传递）**  
  - 面向分布式内存或集群；显式通信（点对点/集合通信）；扩展性强。  
- **OpenMP（共享内存）**  
  - 面向单机多核；通过编译指导/库函数/环境变量实现并行；上手快。  
- **CUDA（GPU加速）**  
  - 大规模数据并行；显式管理 Host↔Device 数据传输；用 kernel + grid/block/thread 组织执行。

- **考试常见问法**  
  “某场景用哪个？”  
  - 单机多核：OpenMP  
  - 多机集群：MPI  
  - 矩阵/图像/深度学习等高数据并行：CUDA


### 16. CPU vs GPU：设计目标与性能特征

#### 原文要点（摘录）
- CPU偏低延迟与复杂控制；GPU偏高吞吐与大规模并行

#### 详解
- **CPU**  
  - 强控制与分支处理能力；大缓存、复杂乱序；适合不规则任务与低延迟响应。  
- **GPU**  
  - 大量简单核心与高带宽；通过线程并行隐藏延迟；适合规则数据并行（矩阵、卷积等）。

- **易错点**  
  - GPU快的前提是：并行度足够 + 访存模式友好（合并访问）+ 分支少。


### 17. CUDA：Host / Device / Grid / Block / Thread（概念题）

#### 原文要点（摘录）
- Host=CPU端；Device=GPU端；kernel在grid上启动；grid含多个block；block含多个thread；block内可共享内存通信

#### 详解
- **执行层次**  
  - Host 调 kernel  
  - kernel 启动一个 grid  
  - grid 由多个 block 构成  
  - block 由多个 thread 构成  
  - block 内可用 shared memory 通信/同步；block 间通常不直接同步（需通过全局同步或分多次kernel）。

- **考试提示**  
  - 题目问“为什么要分block”：硬件调度与资源限制（共享内存/寄存器）+ 同步域。


## 第三章：云计算与虚拟化相关技术

::: tip 章节导读
本章常以“特征/模型/服务模式”以及“虚拟化/特权级/敏感指令/SDN/容器/微服务”形式出选择、判断与简答。
:::

### 1. 云计算：五个特征、四个部署模型、三个服务模式（必考）

#### 原文要点（摘录）
- 原文列出：五大基础特征、四个部署模型、三种服务模式，并给出IaaS/PaaS/SaaS定义

#### 详解
这里建议按 NIST 常用表述记忆（考试通常采用该版本）。

- **五个基本特征（五连背）**  
  1) 按需自助服务（On-demand self-service）  
  2) 广泛网络接入（Broad network access）  
  3) 资源池化（Resource pooling，多租户）  
  4) 快速弹性伸缩（Rapid elasticity）  
  5) 可计量服务（Measured service）

- **四个部署模型（四选背）**  
  - 公有云（Public）  
  - 私有云（Private）  
  - 混合云（Hybrid）  
  - 社区云（Community）

- **三个服务模式（I/P/S）**  
  - **IaaS**：租算力/网络/存储，用户管OS与应用  
  - **PaaS**：租平台（中间件/运行时/容器编排），用户管应用与数据  
  - **SaaS**：租软件服务，用户管业务配置与数据使用

::: warning 易错点
- “部署模型”与“服务模型”不要混：前者讲云的归属与边界，后者讲交付层级。  
- 题目可能让你按“用户负责边界”画责任划分，建议用“从下到上：网络→存储→计算→OS→中间件→应用→数据”来说明。
:::


### 2. 虚拟化分类（软件/硬件；半虚拟化/全虚拟化；Type1/Type2）

#### 原文要点（摘录）
- 软件虚拟化：QEMU；硬件虚拟化：Intel VT/AMD VT
- 半虚拟化：修改Guest OS配合VMM（Xen等）；全虚拟化：Guest OS不改（VMware等）
- 原文对Type1/Type2给出一套描述与例子

#### 详解
- **分类维度一：软件 vs 硬件辅助**  
  - 软件虚拟化：主要靠指令翻译/模拟；兼容性强但开销大。  
  - 硬件虚拟化：CPU提供trap/模式支持（VT-x/AMD-V），减少翻译开销。

- **分类维度二：半虚拟化 vs 全虚拟化**  
  - 半虚拟化：Guest OS“知道自己在虚拟化里”，用hypercall等配合VMM，性能更好但需改OS。  
  - 全虚拟化：Guest OS无感，VMM负责截获与模拟，移植性好但复杂度/开销更高（现代硬件辅助后开销显著降低）。

- **分类维度三：Type1 vs Type2（强烈建议按“教科书标准”记）**  
  - **Type 1（bare-metal）**：Hypervisor直接跑在硬件上（例：VMware ESXi、Xen）。  
  - **Type 2（hosted）**：Hypervisor跑在宿主机OS上（例：VMware Workstation、VirtualBox）。  

::: warning 易错点（非常高频）
原文中“Type1/Type2”的描述与很多教科书标准**可能相反**。考试请优先以老师课件/课堂口径为准；如果题目要求“概念性解释”，建议使用“bare-metal/hosted”的描述方式，避免只写Type编号而踩坑。
:::


### 3. 0/1/3 模型与 x86 特权级（Ring0~Ring3）

#### 原文要点（摘录）
- x86支持4种特权级：ring0最高、ring3最低；2/3不常用
- Xen：ring0运行Xen，Guest OS降到ring1，应用ring3（0/1/3模型）

#### 详解
- **知识点**  
  - 传统OS：内核在 ring0，应用在 ring3。  
  - 早期x86虚拟化难点：某些“敏感但非特权”的指令在低特权级不trap。  
  - Xen的解决：把Guest OS放到ring1，并修改内核替换敏感指令（半虚拟化），形成 0/1/3 模型。

- **扩展：ring -1（硬件辅助虚拟化）**  
  现代VT-x/AMD-V常引入“root/non-root”或所谓“ring -1”的执行环境，让hypervisor拥有更高控制权，并能捕获敏感行为，提高可移植性与性能。

- **易错点**  
  - “ring -1”是概念叫法，不是x86原生的第5个环；答题写“硬件提供root mode/VMX root”更稳。


### 4. 特权指令 vs 敏感指令（必考）

#### 原文要点（摘录）
- 特权指令：只能在最高特权级运行，管理关键资源
- 敏感指令：虚拟化时必须在最高特权级运行；x86存在敏感但非特权指令导致完全虚拟化困难

#### 详解
- **定义对比**  
  - **特权指令**：只有内核态/最高特权级可执行，否则触发异常。  
  - **敏感指令**：在虚拟化环境中，如果不由hypervisor控制就可能破坏隔离/正确性，因此必须被捕获或在高特权执行。

- **关键考点（x86为什么难）**  
  - 在一些体系结构（很多RISC）中：敏感指令 ⊆ 特权指令，因此能靠trap实现完全虚拟化。  
  - 在传统x86上：存在“敏感但非特权”指令，不会trap → VMM无法捕获 → 需要二进制翻译或半虚拟化/硬件辅助。

- **解题技巧（简答）**  
  先给定义，再点名“x86敏感非特权”问题，最后写解决路线：半虚拟化（修改OS）/硬件辅助（VT-x）。


### 5. SDN：特点与核心思想（简答题高频）

#### 原文要点（摘录）
- SDN：软件定义网络；OpenFlow实现可编程能力
- 特点：可编程；控制平面与数据平面分离；逻辑集中控制

#### 详解
- **知识点**  
  - 传统网络设备：控制与转发耦合在交换机/路由器中，策略难下发、难统一编排。  
  - SDN：把控制逻辑上移到控制器（Controller），设备只负责数据平面转发；通过南向接口（如OpenFlow）下发规则。

- **答题模板**  
  概念 → 产生背景（网络难以灵活编程）→ 三大特征（分离/集中/可编程）→ 好处（自动化、弹性、统一策略）→ 典型协议（OpenFlow）。


### 6. 容器：特点与与虚拟机对比

#### 原文要点（摘录）
- 容器是一种沙盒技术：隔离应用且可迁移；docker vs VM

#### 详解
- **知识点（容器=OS级虚拟化）**  
  - 通过 **namespace** 做隔离（进程/网络/文件系统等视图隔离）  
  - 通过 **cgroups** 做资源限制（CPU/内存/IO配额）  
  - 共享宿主机内核，启动更快、密度更高。

- **容器 vs 虚拟机（对比题套路）**  
  - 隔离层级：VM虚拟硬件+独立内核；容器共享内核  
  - 启动速度：容器快  
  - 资源开销：容器更小  
  - 安全隔离：VM通常更强（边界更硬）  
  - 适用：容器适合微服务与弹性部署；VM适合强隔离/异构OS。

::: warning 易错点
容器不是“比VM更安全/更隔离”，它更轻量但隔离边界更依赖内核机制与配置。
:::


### 7. SOA vs 微服务（高频对比题）

#### 原文要点（摘录）
- SOA：可组合、可复用的面向服务范式；微服务是更细粒度的服务化实践

#### 详解
- **共同点**  
  都是把系统拆成服务，通过接口通信实现解耦与复用。

- **关键区别（建议按5维对比）**  
  1) **粒度**：SOA常较粗，微服务更小、更聚焦业务能力  
  2) **治理方式**：SOA常强调集中治理/ESB；微服务强调自治团队与去中心化治理  
  3) **部署形态**：SOA可能共享数据库/平台；微服务强调独立部署、独立扩缩容  
  4) **通信方式**：SOA常用ESB/集中消息；微服务常用轻量协议（HTTP/gRPC）+事件驱动  
  5) **数据边界**：微服务推崇“服务私有数据”，通过API协作，避免共享库/共享表耦合

- **易错点**  
  - “微服务=SOA”不完全对：微服务是SOA理念的一种演进与工程化落地，强调组织与交付方式（DevOps、持续交付）匹配。


