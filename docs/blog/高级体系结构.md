---
title: 计算机体系结构期末复习
date: 2025-12-25
---

# 计算机体系结构期末复习（VitePress 版）

::: tip 使用说明
- 本文将原始复习材料按章节与考点重排，并对**每个小节**补充「详解」（考点/知识点/记忆点/易错点/题型套路）。
- 末尾附有「原始文档提取全文」以确保信息**不遗漏**（用于对照/查漏补缺）。
:::

## 题型与分值分布

| 题型 | 数量 | 分值 |
| --- | ---: | ---: |
| 选择题 | 10 题 | 20 分 |
| 判断题 | 10 题 | 20 分 |
| 名词解释 | 5 题 | 15 分 |
| 简答题 | 5 题 | 30 分 |
| 计算题 | 2 题 | 15 分 |



---

# 第一章 冯诺依曼结构与体系结构设计思想

## 1. 冯诺依曼结构的基本特点，与哈佛结构的区别

### 要点
- 冯诺依曼：由三部分硬件组成：**CPU**（控制单元、ALU、寄存器、程序计数器）、**主存储器**、**I/O 系统**。
- 与哈佛结构的核心区别：**指令与数据是否共享存储与通路**。

::: details 详解
**考点**：对比题（选择/简答）高频，尤其是“存储组织/总线通路/性能瓶颈/适用场景”。  
**知识点**：  
- 冯诺依曼：**指令与数据同存同取**（同一存储、同一路径），更灵活、编程模型简单。  
- 哈佛：**指令存储器与数据存储器分离**（可并行取指与访数），吞吐更高。  
**记忆点**：  
- 冯：**“一存一通路”**；哈佛：**“两存两通路”**。  
**易错点**：  
- 把“缓存 I/D 分离（改良哈佛）”误认为完整哈佛；实际上很多现代 CPU 是**冯诺依曼 ISA/内存模型 + 改良哈佛缓存**。  
**题型套路**：简答对比至少写 4 点：存储组织、通路/带宽、灵活性、典型应用（嵌入式/MCU 常见哈佛）。
:::

---

## 2. 顺序执行指令的处理能力

### 要点
- 具有**按顺序执行指令**的处理能力（默认顺序语义）。

::: details 详解
**考点**：经常与“流水线、乱序执行、并行”一起考，强调“程序可见语义”。  
**知识点**：  
- “顺序”是指**从程序员视角的执行结果符合顺序语义**；实现上可以用流水线/乱序，但必须保证可见结果等价。  
**易错点**：把“实现乱序”当成“语义乱序”。  
**答题模板**：先写“冯诺依曼模型假设顺序执行”，再补“现代实现可能乱序但对外等价”。
:::

---

## 3. 冯诺依曼瓶颈

### 要点
- 主存与 CPU 控制单元之间存在**单一路径**（物理或逻辑），称“冯诺依曼瓶颈”。

::: details 详解
**考点**：名词解释/简答常考，常与“缓存、预取、带宽/延迟、内存墙”联动。  
**知识点**：瓶颈本质是**带宽与延迟**受限：取指与访数共享通路，导致争用。  
**记忆点**：**“一条路要跑两种车（指令车/数据车）”**。  
**易错点**：把瓶颈仅理解为“主存慢”，忽略“通路共享 + 带宽有限”。  
**延伸**：多级缓存、指令/数据 cache 分离、预取、增加内存通道、NUMA 等都是缓解手段。
:::

---

## 4. 指令执行周期（取指 → 译码 → 执行）

### 要点
- 执行周期：**取指 → 译码 → 执行**  
- 取指：PC 定位指令地址；译码：控制器解析；执行：取操作数、ALU 运算、写回寄存器/存储器。

::: details 详解
**考点**：选择/判断常考顺序与关键部件（PC、IR、控制单元、ALU、寄存器）。  
**知识点**：  
- 取指阶段常涉及 **PC 自增/更新**；  
- 译码阶段生成控制信号；  
- 执行阶段可能包含访存、写回。  
**易错点**：把“写回”忘记写；或者把“译码”说成“把指令变成 ALU 能理解的语言”但不提控制信号/微操作。  
**答题建议**：若题目问“执行周期包含哪些动作”，用**动词**写：取、译、读、算、写、改 PC。
:::

---

## 5. 冯诺依曼模型的特征

### 要点
1) 以运算器为中心  
2) 存储器中指令与数据同等对待  
3) 按地址访问、按顺序线性编址的一维结构、单元位数固定  
4) 指令执行顺序性  
5) 指令由操作码 + 地址码组成  
6) 指令和数据二进制表示、二进制运算

::: details 详解
**考点**：名词解释/简答，要求“列点完整”。  
**记忆点（口诀）**：**“运算为心、指数同存、线性编址、顺序执行、码分两段、二进制算”**。  
**易错点**：  
- “以运算器为中心”容易被忽略（与现代“以存储/数据流为中心”的讨论形成对比）。  
- 把“线性编址”写成“二维/多维”。  
**拓展**：现代体系结构常通过缓存/流水线/并行等突破性能，但很多软件抽象仍继承该模型。
:::

---

## 6. 哈佛模型（Harvard Architecture）

### 要点
- 组织：处理器、**指令存储器**、**数据存储器**、I/O  
- 区别：指令与数据存储/访问分离  
- 优势：可分别优化指令/数据存储单元  
- 缺点：不够灵活，需要预设指令/数据存储大小且难动态调整  
- 应用：常用于小型嵌入式系统

::: details 详解
**考点**：对比题 + 应用场景（MCU、DSP）。  
**知识点**：  
- 分离带来潜在并行：**取指与访数可并行**；  
- 也便于采用不同字宽/不同存储工艺。  
**易错点**：  
- 现代 CPU 常见“改良哈佛”：L1 I$ 与 D$ 分离，但更高层仍统一地址空间。考试需按课程口径表述。  
**答题建议**：优缺点对称写（性能/灵活性）。
:::

---

## 7. 计算机体系结构设计思想（8 个核心理念）

### 要点
- 面向摩尔定律的设计  
- 面向抽象简化设计  
- 加速大概率事件（common case fast）  
- 通过并行提高性能  
- 通过流水线提高性能  
- 通过预测提高性能  
- 存储器层次（速度/容量/价格梯度）  
- 通过冗余提高可靠性

::: details 详解
**考点**：简答题高频，常要求“解释其中某 2~3 条并举例”。  
**知识点**：  
- **Common case fast**：优化热点路径收益更高。  
- **并行/流水线/预测**：分别对应吞吐、阶段化、减少等待/分支损失。  
- **存储层次**：利用局部性，把“常用”放到快层。  
- **冗余可靠**：ECC、RAID、双机热备。  
**记忆点**：**“摩抽常、并流预、层次冗”**（摩尔/抽象/常见；并行/流水/预测；层次/冗余）。  
**易错点**：把“并行”和“流水线”混为一谈；并行是“多件同时做”，流水线是“分段连续做”。
:::

---

## 8. 摩尔定律、内存墙与应对

### 要点
- 摩尔定律：晶体管数量约每 18 个月翻倍（同成本）  
- 设计必须预测**产品完成时**的工艺水平  
- “摩尔失效？”：单芯片单核规律变化，但多核等仍可视作延续  
- **内存墙**：CPU 与内存速度差距扩大  
- 解决：增大/优化缓存、多级缓存结构、缓存算法优化等

::: details 详解
**考点**：名词解释（摩尔定律/内存墙）+ 简答（解决思路）。  
**知识点**：  
- 内存墙不仅是“内存慢”，更是**CPU 提速更快**导致差距扩大。  
- 缓解手段分两类：  
  1) **把数据搬近**：多级 cache、预取、NUMA 本地化；  
  2) **把并发做高**：多通道内存、并行访存、带宽扩展。  
**易错点**：把“增大 cache”当万能，忽略命中率/延迟/功耗与一致性开销。  
**简答套路**：定义 → 现象（差距扩大）→ 影响（stall）→ 解决（层次化 + 并发带宽）。
:::

---

## 9. 并发 Concurrency 与并行 Parallelism

### 要点
- 并发：同一 CPU 上“看起来同时”，靠时间片切换（Web 服务器、多任务 OS）  
- 并行：多 CPU/多核“真正同时”（指令级并行 SIMD、多核、集群）

::: details 详解
**考点**：选择/判断常见陷阱。  
**记忆点**：并发强调“**结构上可同时**”，并行强调“**物理上同时**”。  
**易错点**：把“多线程”必然等同并行；单核多线程是并发不是并行。  
**答题建议**：给 1 个系统例子 + 1 个硬件例子，区分更稳。
:::

---

## 10. 流水线冒险（结构/数据/控制）与分支预测

### 要点
1) **结构冒险**：资源冲突  
- 解法：增加资源；停顿/插入气泡  
2) **数据冒险**：数据依赖导致等待  
- 解法：插入气泡；编译优化/静态调度；动态调度  
3) **控制冒险**：分支导致取指不确定  
- 解法：动态分支预测；停顿  
分支预测：静态（编译期固定）与动态（运行期更新）

::: details 详解
**考点**：简答常考“分类 + 举例 + 解决”，也可能考“哪种方法属于哪类”。  
**知识点**：  
- 数据冒险三大依赖：RAW（读后写真相关）、WAR（写后读反相关）、WAW（写后写输出相关）。  
- 常见硬件解法还包括：**转发/旁路（forwarding）**、**寄存器重命名**（解决 WAR/WAW）。  
- 控制冒险核心是**分支代价**（错误预测需要 flush）。  
**易错点**：  
- 把“插入气泡”写成“插入 NOP”但不说明其目的；  
- 只会写“预测”不会写“预测错的代价”。  
**题型套路**：看到“冒险”题，先写分类，再逐类给 1 个例子 + 1~2 个解决方案即可拿满分。
:::

---

## 11. 缓存的目的与 L1/L2/L3（<Badge type="danger" text="必考" />）

### 要点
- 存储层次：顶层快小贵，底层慢大便宜  
- L1：CPU 内部，最快，容量小；分 ICache 与 DCache  
- L2：较大较慢，可能每核私有或共享  
- L3：更大更慢，通常多核共享  
- 多级缓存为了平衡：**容量、延迟、命中率**

### 多级缓存对比表

| 层级 | 典型位置 | 典型共享方式 | 目标侧重 | 备注 |
| --- | --- | --- | --- | --- |
| L1 | 核内 | 每核私有 | 极低延迟 | 常分 I$/D$ |
| L2 | 核内/核旁 | 私有或共享 | 降低 L1 miss penalty | 延迟明显高于 L1 |
| L3 | 片上更外层 | 多核共享 | 降低整体 miss rate | 也称 LLC |

::: details 详解
**考点**：  
- “缓存目的/为什么要分级/每级位置与特点/为什么 L1 小 L2 大”是高频简答；也可能做选择题“哪个描述正确”。  
**知识点**：  
- 缓存利用局部性：时间局部性、空间局部性。  
- 多级原因：  
  - L1 追求**最短命中时间**（越大越慢）；  
  - L2/L3 追求**更低缺失率**（更大更易命中）。  
**记忆点**：**“L1 要快，L2/L3 要装得多”**。  
**易错点**：  
- 把 L3 说成“CPU 外部”；现代主流桌面/服务器多为**片上共享 LLC**，但课程口径可能强调“更外层/更远”。建议答题用“更外层/更远离核心”更稳。  
**简答套路**：目的（缩短平均访存时间 AMAT）→ 位置/特点 → 为什么分级（延迟-容量折中）。
:::

---

## 12. 写直达（Write-Through）与写回（Write-Back）

### 要点
- 写直达：同时更新缓存与内存；一致性简单但写开销大  
- 写回：仅写缓存并置脏位，替换时再写回；性能好但一致性/实现更复杂，存在数据丢失风险（断电等）

::: details 详解
**考点**：名词解释/选择题常考；也会考“脏位/写缓冲”。  
**知识点**：  
- 写直达常配合 **写缓冲（write buffer）** 降低写入阻塞。  
- 写回必须维护 **dirty bit**，替换时产生写回流量。  
**易错点**：  
- 写回并非“不写内存”，而是“**延迟写**”。  
- 可靠性风险通常与**断电/崩溃前未写回**有关，工程上可通过电容/电池/日志等缓解。  
**题型套路**：对比题写“性能 vs 一致性复杂度”两条主线即可。
:::

---

## 13. CPU 性能：响应时间、吞吐率与性能公式（<Badge type="danger" text="必考" />）

### 要点
- 响应时间（Response time）：完成某任务的总时间（用户关心），与性能成反比  
- 吞吐率（Throughput）：单位时间完成任务数量（数据中心管理员关心），与性能成正比  
- 升级单处理器：可能同时改善响应时间与吞吐率  
- 增加多处理器处理独立任务：提升吞吐率，不一定缩短单任务响应时间  
- 时钟周期 = 频率倒数

### CPU 性能核心公式（定量原理）

> 现代体系结构课程最常用的三段式：**指令数 × CPI × 时钟周期**。

$$
T_{CPU} = IC \times CPI \times T_{cycle}
$$

等价形式：

$$
T_{CPU} = \frac{IC \times CPI}{f_{clock}}
$$

其中：
- $IC$：Instruction Count（指令条数）
- $CPI$：Cycles Per Instruction（平均每条指令周期数）
- $T_{cycle}$：Clock Cycle Time（时钟周期）
- $f_{clock}$：Clock Rate（主频）

::: details 详解
**考点**：计算题必备（至少 1 题会用到），也可能考“CPI 是统计量/概率量”。  
**易错点（高频扣分）**：  
1) 把“主频越高一定越快”当结论，忽略 $IC$ 与 $CPI$；  
2) 单位混乱：GHz、ns、cycles、seconds；  
3) 忽略“不同指令类别 CPI 不同”，需要加权平均：  
$$
CPI_{avg}=\sum_i CPI_i \times \text{freq}_i
$$  
**解题套路**：  
- 先把频率换成周期或反之（统一单位）；  
- 再算 $CPI_{avg}$（若给了指令比例）；  
- 最后代入 $T_{CPU}$，并做数量级检查（毫秒/秒是否合理）。  
**记忆点**：性能优化三旋钮：**降 IC、降 CPI、升频率（降周期）**。
:::

---

## 14. 基准测试程序：SPEC 与 Linpack

### 要点
- SPEC：标准性能评估联合体的基准程序集合，覆盖多种真实应用，给单项与总体评价  
- Linpack：常用于浮点性能测试（也用于 TOP500）

::: details 详解
**考点**：名词解释/简答；常问“SPEC 的特点/为什么权威”“Linpack 测什么”。  
**知识点**：  
- SPEC 强调“真实负载集合 + 统一规则”，结果更能反映综合性能；  
- Linpack 主要度量线性代数求解（密集矩阵）下的浮点吞吐。  
**易错点**：把“跑分”当成泛化结论；应说明“不同基准代表不同工作负载”。  
**答题建议**：写清“测试对象/场景/输出指标”即可。
:::

---

## 15. ISA（指令集体系结构）

### 要点
- ISA：程序员可见的实际指令集，区分软件与硬件的界限，也称指令系统  
- 没有 ISA：软件无法使用硬件；计算机也难称“通用计算机”

::: details 详解
**考点**：名词解释、简答“ISA 与微架构区别”。  
**知识点**：  
- ISA 定义：指令、寄存器、寻址模式、内存模型、异常/中断等“对软件可见”的规范。  
- ISA 是抽象层：允许同一 ISA 在不同微架构上实现（兼容与演进）。  
**易错点**：把 ISA 当成“CPU 具体实现细节”；ISA 是**规范**，微架构才是**实现**。  
**记忆点**：ISA = **软件看到的合同**，微架构 = **硬件如何履约**。
:::

---

## 16. 微架构（Microarchitecture）

### 要点
- 微架构：CPU 内部的具体设计与组织方式  
- 影响：时钟频率、指令执行效率、缓存层次、分支预测等  
- 关系：微架构把 ISA 指令翻译成内部微操作并执行，不同微架构性能可差异很大

::: details 详解
**考点**：经常与 ISA 对比出题。  
**知识点**：  
- 同一 ISA（如 x86-64）可有多代微架构（不同流水线深度、乱序窗口、分支预测、cache 结构）。  
- 微架构优化常围绕：**ILP（指令级并行）**、**内存访问隐藏**、**功耗与面积**。  
**易错点**：把“微码/微指令”与“微架构”混同；微码是实现手段之一。  
**答题模板**：定义 → 影响因素 → 与 ISA 关系（同 ISA 多实现）。
:::

---

## 17. 寻址空间与内存空间

### 要点
- 寻址空间：CPU 对内存寻址的能力（可访问最大内存容量）  
- 通常按字节寻址；16 位地址可访问 64KB  
- 内存地址是“标识”，通过地址找到数据  
- 内存空间大小 = 可寻址地址数量

::: details 详解
**考点**：计算题/选择题会考“位数 → 可寻址容量”。  
**知识点**：若地址宽度为 $n$ 位、按字节寻址，则最大空间：  
$$
\text{MaxMemory} = 2^n \text{ bytes}
$$  
**易错点**：  
- 把“字寻址/字节寻址”搞混，导致容量差一个“字长倍数”。  
- 把“物理内存大小”与“可寻址空间”混淆（例如 32 位 OS 的虚拟地址空间限制）。  
**答题建议**：先写“按什么单位寻址”，再算 $2^n$。
:::

---

## 18. 大端字节序与小端字节序

### 要点
- 大端：高位字节在前、低位字节在后（更符合人类阅读）  
- 小端：低位字节在前、高位字节在后（如 0x1122 的存储顺序）

::: details 详解
**考点**：选择题常考“给定内存字节序列判断数值”。  
**记忆点**：  
- 大端：**大头在前**；小端：**小头在前**。  
**易错点**：  
- 把“字节序”误当成“位序”；字节序讨论的是**字节排列**。  
**解题套路**：给出内存从低地址到高地址的字节序列：  
- 小端：低地址是最低有效字节（LSB）。  
- 大端：低地址是最高有效字节（MSB）。
:::

---

## 19. 数据对齐与数据打包（<Badge type="danger" text="必考" />）

### 要点
- 操作数打包：内存排布尽量不浪费空间  
  - 好处：省空间；减少寄存器与内存之间搬运次数  
- 操作数对齐：结构体大小/偏移计算常考  
  - 相对偏移从 0 开始  
  - 成员对齐数 = min(成员大小, 默认对齐数)  
  - 结构体总大小是最大对齐数的整数倍

::: details 详解
**考点**：选择题必出 + 计算题高频（结构体大小、成员偏移）。  
**知识点（通用规则，按课程口径）**：  
1) 每个成员的起始地址必须是其对齐数的整数倍；  
2) 成员之间可能插入 padding；  
3) 结构体总大小向最大对齐数对齐。  
**易错点**：  
- 只算成员之和不算 padding；  
- 忘记“结构体整体也要对齐”；  
- 把“默认对齐数”与“成员对齐数”混淆。  
**解题套路**：画表格：`offset → 放成员 → 对齐补齐 → 更新 offset`，最后再做整体对齐。  
**记忆点**：**“成员先对齐，整体再对齐”**。
:::

---

## 20. 寻址模式

### 要点
- 直接寻址：立即数、寄存器、内存地址  
- 间接寻址：  
  - 内存间接（指针）  
  - 寄存器间接  
  - 基址寄存器 + 偏移量（也可互换表述）  
  - 基址 + 索引（两个寄存器相加）  
  - PC 相对寻址（分支跳转）  
- 字节寻址是主流；ARMv8/MIPS 要求对齐，x86/RISC-V 不强制但对齐更快

::: details 详解
**考点**：选择题（识别寻址方式）、简答题（列出并举例）。  
**知识点**：  
- **基址+偏移**：结构体字段访问（`base + field_offset`）。  
- **基址+索引**：数组访问（`base + i*elem_size`，有些 ISA 直接支持缩放索引）。  
- **PC 相对**：位置无关代码（PIC）与分支跳转常用。  
**易错点**：  
- 把“基址寄存器存偏移、变址寄存器存基址”写反；考试按老师定义写，但结论是：**一个提供基准，一个提供变化量**。  
**答题建议**：每种方式配一个“典型用途”会更稳。
:::

---

## 21. 位运算与原码/反码/补码、移位与溢出（<Badge type="danger" text="必考" />）

### 要点
- 逻辑移位：左/右移，空位补 0  
- 算术移位：看作有符号补码，右移复制符号位（等价除以 2），左移空位补 0（等价乘以 2，符号位不参与移位）  
- 原/反/补码：  
  - 正数三者相同  
  - 负数：反码=符号位不变其余取反；补码=反码 + 1  
- 溢出：算术左移时若移出位（除符号位外）与符号位不匹配则溢出

::: details 详解
**考点**：计算题与选择题常见；尤其是“补码表示范围”“移位后数值”。  
**知识点**：  
- $n$ 位补码范围：$[-2^{n-1}, 2^{n-1}-1]$。  
- 判断溢出：加法看“同号相加异号结果”，移位看符号扩展与移出位。  
**易错点**：  
- 把算术右移当成逻辑右移（负数必须补 1）；  
- 负数求补码时忘记“末位 +1”；  
- 左移乘 2 并不总成立（可能溢出）。  
**解题套路**：  
1) 先明确“逻辑/算术”“原/反/补码”；  
2) 画出位串并按规则补位；  
3) 必要时把结果再转回十进制核对。
:::

---

## 22. 二八法则与一九法则

### 要点
- 二八法则：20% 指令占用 80% 时间  
- 一九法则：90% 时间执行 10% 代码

::: details 详解
**考点**：名词解释 + 与“common case fast / 局部性 / 性能优化”联动。  
**知识点**：两者都是经验规律，用于说明“热点显著”，优化热点收益最大。  
**易错点**：把比例当成精确物理定律；答题要写“经验规律/经验法则”。  
**记忆点**：**“少数热点决定大部分时间”**。
:::

---

## 23. RISC 的特点（<Badge type="danger" text="必考" />）

### 要点
- 扩展寄存器文件（寄存器更多）  
- Load/Store 架构：仅加载/存储访问内存，其余对寄存器操作  
- 指令正交、等长、减少特例  
- 独立 I/D cache（改良哈佛）  
- 通常每机器周期执行一条指令  
- 简单寻址方式与指令格式  
- 不足：兼容性与适用范围可能受限

::: details 详解
**考点**：选择题/简答题高频，常要求“列特点 + 为什么利于流水线”。  
**知识点**：  
- RISC 目标：简化译码与控制，提升流水线效率与频率。  
- Load/Store 让访存与运算分离，便于流水线分段与转发。  
**易错点**：  
- 把“RISC 指令少”绝对化；关键是**规则化/正交化**，不是简单“少”。  
- “每周期一条”是典型目标，不是所有实现都严格达到。  
**记忆点**：**“寄存器多、访存少、指令规整、流水线友好”**。
:::

---

## 24. 局部性原理（时间/空间）

### 要点
- 时间局部性：近期用过将来还会用（循环/迭代）  
- 空间局部性：用到某处附近也可能用到（数组/顺序访问）

::: details 详解
**考点**：缓存设计的理论基础，几乎所有 cache 题都会提到。  
**易错点**：  
- 把“空间局部性”写成“访问地址相同”；那是时间局部性。  
**答题建议**：每类局部性配一个例子（for 循环、数组遍历）。  
**延伸**：预取器（prefetcher）主要利用空间局部性与访问模式。
:::

---

## 25. 缓存基本术语：命中率、缺失率、缺失损失

### 要点
- 命中率 $h$：在缓存中找到数据的概率  
- 缺失率 $m=1-h$  
- 缺失损失（miss penalty）：发生缺失带来的时间损失

::: details 详解
**考点**：名词解释 + 计算（AMAT）。  
**知识点（常用公式）**：平均访存时间（AMAT）常写为：  
$$
AMAT = T_{hit} + m \times \text{MissPenalty}
$$  
多级 cache 可递归展开（按课程讲法写即可）。  
**易错点**：把“缺失率”当成“缺失次数”；要分清概率与次数。  
**答题建议**：写出符号定义再给公式，表达更完整。
:::

---

## 26. 缓存设计原理：为什么 L1 小、L2 更大？（<Badge type="danger" text="必考" />）

### 要点
- 缓存大小影响访问时间：希望 L1 尽量小以降低命中时间  
- 降低缺失率希望容量更大：L2 主要考虑缺失率  
- L2 的访问时间主要影响“L1 缺失损失”

::: details 详解
**考点**：简答题常问“为什么分层/为什么 L1 小”。  
**知识点**：  
- L1 命中在最频繁路径上（common case），延迟最关键；  
- L2/L3 更大可显著降低 miss rate，减少去内存的次数。  
**易错点**：只写“L1 快、L2 慢”不给原因。要写“延迟随容量上升、命中率随容量上升”这对矛盾。  
**记忆点**：**“L1 控延迟，L2/L3 控缺失”**。
:::

---

# 第二章 并行计算

## 1. TOP500：操作系统、测试集与 FLOPS 单位

### 要点
- TOP500 中 Linux 占比最多  
- 测试集：Linpack  
- 浮点单位：MFLOPS=$10^6$，GFLOPS=$10^9$，TFLOPS=$10^{12}$，PFLOPS=$10^{15}$（千万亿次/秒）

::: details 详解
**考点**：选择题（单位换算/数量级）、简答题（TOP500 用什么测）。  
**易错点**：把 TFLOPS 与 PFLOPS 的数量级写错（差 $10^3$）。  
**记忆点**：M(百万)→G(十亿)→T(万亿)→P(千万亿)，每级 $\times 10^3$。  
**答题建议**：写“每秒多少次浮点运算”，单位含义更完整。
:::

---

## 2. 并行计算的三个基本条件

### 要点
1) 并行机：≥2 个处理器 + 互连网络  
2) 问题有并行度：可分解为并行子任务（并行算法设计）  
3) 并行编程：在并行环境实现并行算法（如 CUDA、OpenMP）

::: details 详解
**考点**：简答题常考，写全三条并补一句解释即可。  
**易错点**：把“并行机”只写成“多核 CPU”，忘了互连与通信。  
**答题套路**：硬件条件 + 算法条件 + 软件实现条件（结构完整）。
:::

---

## 3. Flynn 分类法（<Badge type="danger" text="必考" />）

### 要点
- 概念：指令流 IS、数据流 DS、多倍性  
- 四类：  
  - SISD：单指令单数据  
  - SIMD：单指令多数据  
  - MISD：多指令单数据  
  - MIMD：多指令多数据  
- SIMD：同一指令并行处理不同数据  
- MIMD：不同处理器对不同数据执行不同指令（主流并行机类型）

::: details 详解
**考点**：选择题必考（给结构判断类别），简答也常考“解释四类”。  
**易错点**：  
- 把多核 CPU 说成 SIMD；多核一般是 **MIMD**（每核执行不同指令流）。  
- MISD 现实中少见，答题可写“概念存在、应用少”。  
**记忆点**：看“指令流数 × 数据流数”。  
**答题建议**：给每类一个例子（SISD=单核顺序；SIMD=向量指令；MIMD=多核/集群）。
:::

---

## 4. SIMD 与 SIMT 的区别（<Badge type="danger" text="必考" />）

### 要点
- 共同点：单指令流思想（同一条指令控制多个执行单元）  
- SIMT：Single Instruction Multiple Threads，是 SIMD 的扩展，更灵活，线程可有分支  
- SIMD 常见于 CPU 向量化；SIMT 常见于 GPU/CUDA  
- SIMT 数据可分散寻址，但分散会降低并行访问效率  
- 相关概念：SPMD（单程序多数据）

::: details 详解
**考点**：简答题常要求“先写共同点再写差异”，这类题最容易丢分。  
**知识点**：  
- SIMD：显式向量寄存器/向量指令（如 AVX），要求数据更“规整”。  
- SIMT：编程模型是线程，硬件以 warp/wavefront 方式锁步执行；遇到分支会“分歧（divergence）”。  
- SPMD：程序级视角，多个处理单元跑同一程序但处理不同数据；可基于 MIMD/SIMT 等实现。  
**易错点**：  
- 把 SIMT 当成真正“每线程独立执行”；实际上同一 warp 内常锁步。  
**记忆点**：SIMD=**向量**，SIMT=**线程外衣的向量**。
:::

---

## 5. NUMA 与 ccNUMA

### 要点
- NUMA：共享内存，但访问延迟随内存物理位置不同（非均匀）  
  - 优点：缓解带宽瓶颈，可扩展更多处理器  
  - 缺点：访问时间差异  
- ccNUMA：每处理器加 cache + 缓存一致性协议（cache-coherent）

::: details 详解
**考点**：名词解释 + 简答“为什么需要数据本地化”。  
**知识点**：  
- NUMA 的核心优化方向：**数据尽量放在访问它的处理器本地内存**。  
- ccNUMA 的“cc”强调硬件保证 cache coherence，使共享内存编程更像 SMP。  
**易错点**：把 NUMA 说成“分布式内存”；NUMA 仍属于共享地址空间（通常）。  
**答题建议**：写出“本地/远程访问代价不同”这句话很关键。
:::

---

## 6. 阵列机、向量机与标量机

### 要点
- 向量处理机：支持向量数据表示与向量指令（典型：Cray 等）  
- 标量处理机：不支持向量表示/指令的流水线处理机  
- 阵列处理机：多个处理单元构成阵列，单控制部件控制多个处理单元对各自数据做相同操作（也可视作向量机一种实现方式）

::: details 详解
**考点**：选择题（概念区分）、简答题（特点与典型案例）。  
**知识点**：  
- 向量机强调“一个向量指令驱动流水线对向量元素序列处理”。  
- 阵列机强调“多个处理单元并排并行处理”，控制通常更集中。  
**易错点**：把“GPU”简单等同阵列机；GPU 更接近大规模 SIMD/SIMT。  
**答题建议**：用“控制方式（集中/分散）+ 数据组织（向量/标量）”来区分。
:::

---

## 7. socket / core / thread / cluster

### 要点
- socket：主板 CPU 插槽  
- core：处理器内独立执行硬件单元（寄存器/计算单元等）  
- thread：超线程的逻辑执行单元（独立执行上下文，共享 core 内资源）  
- cluster：多台机器互连形成的计算集群

::: details 详解
**考点**：选择题常以“概念混淆”出题。  
**易错点**：  
- 把 thread 当成物理核心；超线程通常只是复制部分前端/上下文，执行资源仍共享。  
- cluster 与 多核不同：cluster 是多机（分布式）。  
**记忆点**：socket（座）→ core（核）→ thread（线），cluster（群）。
:::

---

## 8. 分布式计算 vs 并行计算（<Badge type="danger" text="必考" />）

### 要点
- 并行计算：单系统多处理器求解同一问题（紧耦合、细粒度、低开销、追求短执行时间）  
- 分布式计算：多系统由调度程序松散耦合处理相关问题（松耦合、粗粒度、强调可用性/可靠性）

::: details 详解
**考点**：简答题常考“区别 + 目的”。  
**知识点**：  
- 并行：目标是**加速单个问题**。  
- 分布式：目标常包括**扩展性/容错/高可用**，不一定加速单任务。  
**易错点**：把 Hadoop/Spark 这类系统当成“并行机”；它们多为分布式并行框架，需要按语境答。  
**答题建议**：从“耦合程度、通信频率、目标侧重点”三条线写区别。
:::

---

## 9. 加速比与效率（<Badge type="danger" text="必考" />）

### 要点
- 加速比：$S = T_S / T_P$  
- 效率：$E = S / P$（P 为处理器数）

::: details 详解
**考点**：计算题/名词解释。  
**易错点**：  
- 用错分子分母；加速比必须是“串行时间/并行时间”。  
- 效率范围通常在 $(0,1]$，超过 1 需要解释（超线性加速可能来自缓存效应）。  
**解题套路**：先算 $S$ 再算 $E$，并检查是否在合理范围。
:::

---

## 10. 阿姆达尔定律（Amdahl's Law）（<Badge type="danger" text="必考" />）

### 要点
- 若程序中串行部分比例为 $f$（或并行部分为 $1-f$），加速比上限受限  
- 常见写法（串行比例 $f$）：  
$$
S(P)=\frac{1}{f+\frac{1-f}{P}}
$$

::: details 详解
**考点**：必考定律，常要求“推导/解释含义/算上限”。  
**易错点**：  
- 把 $f$ 当成并行比例导致公式写反；考试务必看题目定义。  
- 忽略 $P\to \infty$ 的极限：$S_{\max}=1/f$。  
**记忆点**：**“串行是硬上限”**。  
**答题套路**：写公式 → 取极限 → 解释“再多核也被串行部分卡住”。
:::

---

## 11. 数据相关性（<Badge type="danger" text="必考" />）

### 要点
- 写后读（RAW：Read After Write）  
- 读后写（WAR：Write After Read）  
- 写后写（WAW：Write After Write）

::: details 详解
**考点**：流水线/乱序执行相关，选择题常考缩写。  
**知识点**：  
- RAW：真相关，必须保持；  
- WAR/WAW：名义相关，可通过**寄存器重命名**消除。  
**易错点**：把 RAW/WAR/WAW 的英文顺序写错；建议背中文：  
- RAW = **先写后读**（读依赖写结果）。  
:::

---

## 12. 并行粒度：粗粒度与细粒度（<Badge type="danger" text="必考" />）

### 要点
- 并行粒度由通信频率决定，是“计算/通信”比例的定性度量  
- 粗粒度：两次通信间计算多  
- 细粒度：通信频繁

::: details 详解
**考点**：名词解释/选择题，常与“开销、可扩展性”联动。  
**知识点**：  
- 粗粒度通常更易扩展（通信占比低）。  
- 细粒度需要低延迟互连与高效同步机制。  
**易错点**：把粒度理解成“数据大小”；更准确是“任务间交互频率/同步开销”。  
**答题建议**：补一句“粒度影响并行效率与可扩展性”。
:::

---

## 13. 流水线：段、深度、瓶颈、通过/排空时间与分类（<Badge type="danger" text="必考" />）

### 要点
- 段/级：流水线子过程及功能部件  
- 深度：段数  
- 段时间应尽量相等；最长段是瓶颈  
- 通过时间：第一个任务从进入到输出所需时间  
- 排空时间：最后一个任务从进入到输出所需时间  
- 分类：  
  - 静态/动态流水线  
  - 线性/非线性（是否有反馈回路）  
  - 顺序/乱序（输出顺序是否与输入一致）

::: details 详解
**考点**：概念题与选择题高频；尤其“瓶颈段决定节拍”。  
**知识点**：  
- 若每段时间分别为 $t_1,\dots,t_k$，流水线节拍通常由 $\max(t_i)$ 决定。  
- 动态流水线更灵活但控制复杂；非线性流水线需要解决调度/争用。  
**易错点**：  
- 把“通过时间/排空时间”混淆；记住“通过=第一件出来，排空=最后一件出来”。  
**答题建议**：分类题按“是否支持多功能并行、是否有反馈、输出是否乱序”三条维度写即可。
:::

---

## 14. 流水线指标：吞吐率、加速比、效率（<Badge type="danger" text="必考" />）

### 要点
- 吞吐率：单位时间完成任务数  
- 加速比：同批任务顺序时间与流水时间之比  
- 效率：设备实际使用时间 / 总运行时间（利用率）

::: details 详解
**考点**：计算题或选择题（给公式判断/求值）。  
**常见写法（示意）**：若流水线深度为 $k$、节拍为 $t$，连续处理 $n$ 个任务时间近似：  
$$
T \approx (k+n-1)t
$$  
顺序时间约为：  
$$
T_s \approx nk t
$$  
则加速比近似：  
$$
S \approx \frac{nkt}{(k+n-1)t}=\frac{nk}{k+n-1}
$$  
**易错点**：  
- 忘记“填充/排空”带来的 $(k-1)t$ 开销。  
- 把吞吐率当成加速比。  
**答题建议**：若题目给“各段时间不同”，先取最大段时间作为节拍（按课程口径）。
:::

---

## 15. MPI / OpenMP / CUDA：适用场景与特点

### 要点
- MPI：消息传递接口（函数库），可移植、可扩展，支持点对点/聚合通信  
- OpenMP：共享内存多线程（编译指导 + 函数调用 + 环境变量）  
- CUDA：GPU 加速，Host/Device 模型，线程级并行；grid→block→thread；block 内可用 shared memory 同步通信

::: details 详解
**考点**：简答题“各自适合什么系统/编程模型”。  
**知识点**：  
- MPI：典型用于**分布式内存/集群**。  
- OpenMP：典型用于**共享内存多核**（单机）。  
- CUDA：典型用于**GPU 加速**（大量数据并行）。  
**易错点**：  
- 把 OpenMP 当成分布式；OpenMP 默认共享内存。  
- 把 MPI 当成语言；它是标准/库。  
**答题建议**：写“内存模型（共享/分布式）+ 并行粒度 + 通信方式”即可。
:::

---

## 16. CPU 与 GPU 的区别

### 要点
- GPU 浮点能力/内存带宽/并行性更强；控制逻辑弱  
- CPU 低延迟设计；GPU 大吞吐设计  
- 类比：GPU 像大量“小学生”做简单运算；CPU 像“教授”做复杂控制

::: details 详解
**考点**：简答题常考“结构差异导致的适用场景”。  
**知识点**：  
- CPU：复杂分支、低延迟、强单线程/控制流。  
- GPU：规则数据并行、高吞吐、对访存模式敏感（合并访问）。  
**易错点**：认为 GPU “任何任务都更快”；分支发散、随机访存会显著掉速。  
**答题建议**：最后补一句“适用场景：矩阵运算/图像处理/深度学习等”。
:::

---

## 17. CUDA 术语：Host / Device / Grid / Block / Thread

### 要点
- Host：CPU 侧  
- Device：GPU 侧  
- Kernel 启动形成 grid；grid 包含多个 block；block 包含多个 thread  
- 同一 block 内可共享内存与同步；不同 block 间通常不能直接同步通信

::: details 详解
**考点**：选择题（概念层次）、简答题（执行模型）。  
**易错点**：  
- 误以为不同 block 可随意共享 shared memory；shared memory 是 block 内作用域。  
- 误以为线程就是 CPU 的线程；CUDA 线程是 GPU 的轻量执行实体。  
**答题建议**：写清层次关系：**grid → block → thread**，并指出通信边界（block 内）。
:::

---

# 第三章 云计算相关技术

## 1. 云计算：五个特征、四个部署模型、三个服务模式（<Badge type="danger" text="必考" />）

### 要点（按常见课程口径补全）
- 五大特征（典型表述）：  
  1) 按需自助（On-demand self-service）  
  2) 广泛网络接入（Broad network access）  
  3) 资源池化（Resource pooling）  
  4) 快速弹性（Rapid elasticity）  
  5) 可度量服务（Measured service）
- 四个部署模型（常见）：公有云、私有云、混合云、社区云  
- 三种服务模式：IaaS、PaaS、SaaS

### 服务模式要点
- IaaS：以虚拟化形式交付计算/存储等基础设施（例：OpenStack）  
- PaaS：交付平台/中间件/开发运行环境（例：Docker、K8S、Hadoop、Azure）  
- SaaS：通过互联网交付应用软件（例：问卷/业务系统等）

::: details 详解
**考点**：简答题必背清单，要求“列举 + 解释 + 举例”。  
**记忆点（五特征）**：**“自助、网络、池化、弹性、计量”**。  
**易错点**：  
- 把“部署模型”与“服务模式”混淆；部署模型描述“云怎么部署”，服务模式描述“交付到哪一层”。  
- 举例写错层：如把 Docker 当 IaaS（更接近 PaaS/容器平台）。  
**答题模板**：先列清单，再每条一句解释，最后每个服务模式给 1 个例子。
:::

---

## 2. 虚拟化的分类（<Badge type="danger" text="必考" />）

### 要点
- 软件虚拟化：纯软件模拟/翻译（例：QEMU）  
- 硬件虚拟化：硬件提供辅助（例：Intel VT/AMD-V）  
- 半虚拟化：Guest OS 感知虚拟化并修改以配合 VMM（例：Xen 早期模式、KVM-PowerPC）  
- 全虚拟化：Guest OS 无需修改，敏感指令由 VMM 捕获处理（例：VMware、VirtualBox、KVM-x86）  
- Type1 / Type2（原文按课程表述给出）：  
  - “Type1：在宿主机 OS 上运行（如 VMware Workstation）”  
  - “Type2：直接运行在硬件上（如 Xen、VMware ESXi）”

::: warning 重要提示
工业界更常见的命名是：**Type-1 = 裸金属（bare-metal）**，**Type-2 = 宿主机（hosted）**。  
如果课堂/老师按原文那套口径讲授，考试请**以老师口径为准**，但答题时可以用更稳妥的描述：  
- “宿主机型（hosted）虚拟化” vs “裸金属型（bare-metal）虚拟化”。
:::

::: details 详解
**考点**：分类题 + 对比题（效率/可移植性/是否改 Guest OS）。  
**知识点**：  
- 半虚拟化本质是把“被动捕获”变成“主动通知”（hypercalls）。  
- 硬件辅助虚拟化让敏感指令更易捕获，减少 VMM 复杂度。  
**易错点**：  
- 把“全虚拟化”理解成“最强性能”；其实全虚拟化更强调兼容/透明，性能依赖硬件辅助与实现。  
**答题建议**：每类给 1 个代表例子即可。
:::

---

## 3. 0/1/3 模型与特权级（ring）

### 要点
- x86 支持 4 个特权级：ring0 最高（OS/驱动），ring3 最低（应用），ring1/2 较少使用  
- Xen 半虚拟化：VMM 在 ring0，Guest OS 被“降权”到 ring1，应用仍在 ring3（0/1/3 模型）  
- 硬件辅助虚拟化：引入更高特权级（常称 ring -1）承载 Hypervisor

::: details 详解
**考点**：简答题常考“为什么需要 0/1/3”“VT-x 如何改进”。  
**知识点**：  
- 0/1/3 的动机：解决 x86 上“敏感但不特权”指令难以捕获的问题，通过修改 Guest OS 把这些操作替换为 hypercall。  
- ring -1：让 Guest OS 仍可认为自己在 ring0，但实际被硬件隔离在非 root mode。  
**易错点**：把 ring -1 当成“比 ring0 更高权限的 OS”；它是硬件给 Hypervisor 的新模式。  
**答题建议**：画一句“VMM/HV 在哪一环，Guest OS 在哪一环”即可加分。
:::

---

## 4. 特权指令与敏感指令（<Badge type="danger" text="必考" />）

### 要点
- 特权指令：只能在最高特权级运行，用于管理关键系统资源  
- 敏感指令：虚拟化时必须在最高特权级运行；RISC 中特权指令≈敏感指令，但 x86 存在“敏感但不特权”的指令，导致完全虚拟化困难  
- 半虚拟化（Xen）：修改 Guest OS 用替换/主动通知解决捕获问题  
- 硬件虚拟化（VT/AMD-V）：硬件协助捕获敏感指令，提升可移植性

::: details 详解
**考点**：名词解释 + 为什么 x86 难虚拟化（经典问法）。  
**易错点**：  
- 把“敏感指令”当成“特权指令的别名”；在 x86 里二者不完全重合才是关键。  
**答题套路**：定义两者 → 说明关系（RISC 重合、x86 不重合）→ 引出半虚拟化与硬件辅助的解决。
:::

---

## 5. SDN 的特点（<Badge type="danger" text="必考" />）

### 要点
- SDN：Software Defined Network（软件定义网络）  
- OpenFlow 是实现 SDN 的一种协议  
- 特点：  
  1) 网络可编程  
  2) 控制平面与数据平面分离  
  3) 逻辑集中控制（控制器）

::: details 详解
**考点**：简答题：通常要求“概念 + 三特点 + 带来的好处”。  
**知识点**：  
- 分离带来：统一策略下发、集中管理、快速迭代。  
- 可编程化使网络像软件一样灵活（自动化运维、弹性调度）。  
**易错点**：把 SDN 等同 OpenFlow；OpenFlow 只是协议之一。  
**答题建议**：最后补一句“好处：灵活、自动化、易管理”。
:::

---

## 6. 容器（Docker）特点与与虚拟机区别

### 要点
- 容器：一种沙盒技术，可隔离运行应用，且易迁移（镜像/分发）  
- 容器 vs 虚拟机：容器更轻量、启动更快、资源开销更小（共享宿主内核）

::: details 详解
**考点**：简答题/选择题常考“容器为什么轻量”。  
**知识点**：  
- VM 虚拟化的是硬件（Guest OS 独立）；容器虚拟化的是 OS 级隔离（共享内核）。  
- 容器隔离常依赖：namespace、cgroup（课程口径未提也可不写）。  
**易错点**：把容器当成“安全性一定更弱”；实际取决于隔离机制与配置，但答题可写“隔离粒度不同”。  
**答题建议**：从“是否包含 Guest OS、启动时间、资源开销、隔离层次”对比。
:::

---

## 7. SOA 与微服务的区别（原文仅给出 SOA 概念，以下补全对比）

### 要点
- SOA：面向服务的设计范式，强调服务可组合、可复用，以支撑组织级战略目标  
- 微服务：将系统拆分为一组小而自治的服务，围绕业务能力构建，独立部署与演进

::: details 详解
**考点**：简答题常考“共同点/区别/适用场景”。  
**共同点**：都强调“服务化、接口契约、解耦”。  
**主要区别（答题建议写 4~6 条）**：  
1) **粒度**：SOA 通常更粗粒度；微服务更细粒度、单一职责更强。  
2) **治理方式**：SOA 常配合 ESB/集中治理；微服务更偏“去中心化治理”，用轻量协议（HTTP/gRPC）与基础设施（服务发现、网关）。  
3) **部署与演进**：微服务强调独立部署、独立扩缩；SOA 在传统企业系统中可能更集中。  
4) **数据管理**：微服务更强调“每服务自管数据”，避免共享数据库强耦合。  
5) **组织匹配**：微服务常与 DevOps/持续交付结合；SOA 更偏架构治理与集成。  
**易错点**：把微服务理解成“SOA 的简单重命名”；微服务在工程实践上更强调自治与交付。  
**记忆点**：SOA 重“**组合复用与治理**”，微服务重“**小、自治、独立部署**”。  
:::

---

# 期末冲刺：高频必考清单（建议考前 1 小时复盘）

::: tip 一章（体系结构）
- 冯诺依曼 vs 哈佛、冯诺依曼瓶颈、执行周期  
- Cache：目的、L1/L2/L3、写直达/写回、命中率/缺失率/AMAT  
- CPU 性能公式（IC/CPI/频率）、CPI 加权  
- 数据对齐/打包、寻址模式、补码与移位溢出  
- RISC 特点、局部性、二八/一九法则
:::

::: tip 二章（并行）
- Flynn 分类、SIMD vs SIMT、NUMA/ccNUMA  
- 加速比/效率、阿姆达尔定律、数据相关性  
- 流水线指标（吞吐/加速/效率）  
- MPI/OpenMP/CUDA 适用场景
:::

::: tip 三章（云计算）
- 云计算五特征/四模型/三服务模式  
- 虚拟化分类、特权/敏感指令、0/1/3 与 ring -1  
- SDN 三特点、容器 vs VM、SOA vs 微服务
:::

---

# 附录：原始文档提取全文（用于对照，确保不遗漏）

::: details 点击展开原始文本（docx 提取）
```text
题型：

	选择题10题-20分

	判断题10题-20分

	名词解释5题-15分

	简答题5题-30分

	两道计算题-15分



第一章冯诺依曼结构

1.冯诺依曼的基本特点，与哈佛结构的区别

冯诺依曼：由三部分硬件组成：CPU （中央处理单元）：控制单元、算数逻辑单元（ALU）、若干寄存器、程序计数器；主存储器；IO 系统

2.具有执行顺序指令的处理能力

3.主存储器和CPU控制单元包含一条单一路径（物理或逻辑的）叫冯.诺依曼瓶颈。

4、执行周期：取指->译码->执行。

控制单元从存储器中取下一条程序指令，使用程序计数器确定指令所在的位置。

对提取的指令进行译码，变成ALU能够理解的语言。

从存储器中取出执行指令所需的各种操作数的数据，并把它们放入CPU的寄存器中。

ALU执行指令，并将执行的结果存放到寄存器或者存储器中。

5、冯. 诺依曼模型的特征

（1）以运算器为中心。

（2）在存储器中，指令和数据同等对待。

（3）存储器是按地址访问、按顺序线性编址的一维结构，每个单元的位数是固定的。

（4）指令的执行是顺序的。

（5）指令由操作码和地址码组成。

（6）指令和数据均以二进制编码表示，采用二进制运算

6、哈佛模型、

1.组织方式：处理器、指令存储器、数据存储器和IO设备

2.与冯.诺依曼模型的区别：在程序和数据的存储和访问方式上有所不同，指令与数据存储器相分离

3.优势：可分别对于数据存储单元和指令存储单元进行优化

4.缺点：不够灵活，需要指定数据、程序存储单元的大小，且不能相互调整

5.应用：常用于小型嵌入式系统设计



计算机体系结构设计思想（8个核心理念）

面向摩尔定律的设计

面向抽象简化设计

加速大概率事件（作用）：加速大概率事件（common case fast）远比优化小概率事件更能提高性能。大概率事件通常比小概率事件简单，从而易于提高。

通过并行提高性能

通过流水线提高性能

通过预测提高性能

存储器层次（从哪到哪）（哪个快慢）

通过冗余提高可靠性



2.摩尔定律（内存墙，如何解决？）

定义：集成电路芯片上所集成的晶体管数目每隔18个月就翻一番（同样成本）

特点：计算机设计者必须预测其设计完成时的工艺水平，而不是设计开始的

2.摩尔失效了吗？

- 单一芯片发展规律“变化”了

- 多核、内存墙等角度理解仍旧可理解为另一种摩尔定律

-内存墙：硬件和软件发展存在巨大间隔，更多的则是资源过剩，而不是资源不足

内存墙 CPU与内存差距（添加缓存大小、优化缓存算法、多级缓存结构解决）





3.并行并发例子

Concurrency并发：并发，是在同一个cpu上同时（不是真正的同时，而是看来是同时，因为cpu要在多个程序间切换）运行多个程序。（Web服务器、多任务操作系统）

Parallelism并行：并行，是每个cpu运行一个程序。例子（指令级并行 SIMD）（多核）（机群）

4.流水线冒险

1. 结构冒险（Structural Hazards）

结构冒险是指由于硬件资源冲突导致的流水线阻塞。例如，当多条指令需要同一硬件资源时，就会产生结构冒险。

解决方法一：增加硬件资源。通过增加资源来减少资源竞争，例如设置独立的指令存储器和数据存储器或设置独立的指令Cache和数据Cache。

解决方法二：流水线停顿（Stall）。在完成前一条指令对资源的访问时，暂停取后一条指令的操作，即插入空泡（Bubble）。

2. 数据冒险（Data Hazards）

数据冒险是指指令执行需要等待前一条指令的结果，即数据依赖。

解决方法一：插入空泡（Bubbles）。在指令间增加空泡周期，等待数据写回。

解决方法三：编译优化调度（静态调度）和动态调度。通过编译器优化指令顺序或在运行时动态调度来避免数据冒险。

3. 控制冒险（Control Hazards）

控制冒险是指由于分支指令导致CPU无法判断下一步需要执行哪些指令。

解决方法一：动态分支预测。通过动态分支预测指令进行解决，减少因分支预测错误导致的流水线刷新。

解决方法二：流水线停顿（Stall）。在分支指令的结果未确定前，暂停流水线，直到分支结果确定。

分支预测：

①静态分支预测：基于编译器的编译信息对分支指令进行预测，预测信息不再改变

②动态分支预测：依据程序运行的实时信息，不断对预测信息进行更新，具有较高的预测准确率

5.缓存的目的？一级缓存，二级缓存，三级缓存物理位置，结构特点？区别？（必考）（原文档）



在存储器层次中，速度最快、容量最小并且每位价格最昂贵的存储器处于顶层，而速度最慢、容量最大且每位价格最便宜的存储器处于最底层。

高速缓存一般设三级，因为（位置，特点）



1. **L1 Cache（一级缓存）**：

   - **位置**：位于CPU内部，与处理器核心紧密集成。

   - **特点**：

     - **速度**：最快，因为它是最接近处理器核心的缓存层。

     - **容量**：相对较小，通常在128 KB到2 MB之间。

     - **类型**：分为数据缓存（L1 DCache）和指令缓存（L1 ICache），分别存储处理器核心正在处理的数据和正在执行的指令。

     - **访问速度**：几乎和访问寄存器文件一样快。



2. **L2 Cache（二级缓存）**：

   - **位置**：通常位于CPU内部或与CPU紧密耦合，但在核心之外。

   - **特点**：

     - **速度**：较快，但比L1缓存慢。

     - **容量**：比L1大，通常在256 KB到32 MB之间。

     - **共享性**：可以为每个核心独有，也可以是共享的。

     - **访问时间**：比访问L1缓存的时间长5倍，但仍比访问主存快5~10倍。



3. **L3 Cache（三级缓存）**：

   - **位置**：通常位于CPU外部或与多个CPU共享，位于处理器芯片的更外层。

   - **特点**：

     - **速度**：相对较慢，但容量最大。

     - **容量**：比L1和L2大，通常在1 MB到128 MB之间。

     - **共享性**：所有CPU内核共享L3缓存，有助于减少核心之间的数据传输时延。

多级缓存的存在是为了在缓存大小、响应速度（延迟）和数据命中率之间找到最佳平衡。L1缓存虽然速度非常快，但容量较小。L2和L3缓存提供了更大的存储容量，尽管速度相对较慢，但仍比主内存快，这样可以利用局部性原理，即程序具有访问局部区域里的数据和代码的趋势，通过让高速缓存里存放可能经常访问的数据，大部分的内存操作都能在快速的高速缓存中完成。

6.写直达，写回是什么？特点？



写直达：保证缓存与内存数据的一致性；同时更新缓存与内存耗时久；可靠性高

写回：需要脏位标记复杂性高；存在数据丢失的风险；性能较好

7.CPU性能（必考，计算题）

①响应时间（response time）：执行时间，是计算机完成某任务所需的总时间，包括硬盘访问、内存访问、IO活动、操作系统开销和CPU执行时间等（少量计算机）（性能成反比）(用户关心）

②吞吐率(throughput)：带宽，表示单位时间内完成的任务数量。（数据中心）（性能成正比）（数据处理中心的管理员关心）

基本概念和特点，关联性

解决方案：

1.将计算机中的处理器升级。（可以同时改进响应时间和吞吐率。）

2.增加多个处理器来分别处理独立的任务，如搜索任务。（不会使任务完成的更快，只会增加其吞吐率。）

响应时间和吞吐率是相互影响的

当需要处理更多的任务时，系统可能需要令后续请求排队。在这种情况下，随着吞吐率的增加，可同时改进响应时间或执行时间最小化，因为缩短了排队等待时间。

时钟周期：时钟频率的倒数，为计算机一个时钟间隔的时间，通常是处理器时钟，一般为常数。（clock cycle），CPU在一个时钟周期内仅完成一个最基本的动作。

定量原理/ 定量定律 -- CPU 性能公式

主时钟

习惯上总是以主时钟（时钟周期） 来表示一个CPU的速度。



CPU的主频即时钟的频率： 

CPU时间

一个程序在CPU 上运行所需的时间T CPU 表示为：







（3）平均指令执行周期数 CPI



	

	如果在同一个处理机上运行不同的程序，所得的CPI也可能是不同的。所以，作为衡量CPU运行速度的指标CPI，通常是一种概率统计的结果。

	



例题原文档

8.基准测试程序（SPEC，Linpack）

SPEC（Standard Performance Evaluation Corporation）基准测试程序、Linpack

①SPEC是由若干个工作站生产商发起成立的标准性能评估联合体的简称，是基准程序测试方面影响最大的一个组织。

②由各种不同的真实应用程序构成，能比较全面地反映计算机在各个方面的处理性能 。

③对于测试结果的处理可以分为两个方面：

1）对每个测试程序的运行结果给出一个衡量标准。

2）对全部测量结果给出一个总体评价。

Linpack 

9.ISA是什么？它有什么特点？

指令集体系结构（ISA）-指代程序员可以看到的实际指令集，ISA的作用相当于区分软件和硬件的界限，也可称为指令系统。

ISA是通用体系结构中必不可少的抽象层？为什么？

- 没有它，软件无法使用计算机硬件

- 没有它，一台计算机不能称为“通用计算机”





10.微架构是什么？有什么特点？（有问题）



微架构：是CPU内部的具体设计和组织方式

特点：

性能特征决定：微架构决定了CPU的性能特征，包括时钟频率、指令执行效率、缓存层次和分支预测等	

指令集与微架构的关系：微架构负责将指令集中的指令翻译成微操作，这些微操作在处理器内部执行。不同的微架构可能会让相同指令集的CPU展现出截然不同的性能表现

11.什么是寻址空间？什么是内存空间？

寻址空间是指CPU对内存进行寻址的能力，通俗地说，就是CPU能够访问的最大内存容量。

通过是以字节作为访问单位（也可以是字）16位地址可以访问的内存空间64K。

- 内存地址是内存当中存储数据的一个标识，并不是数据本身，通过内存地址可以找到内存当中存储的数据

带有地址的区间（内存空间）内存空间大小就是寻址能力，即能访问到多少个地址

12.大端字节序是什么？小端字节序是什么？（会认）

大端字节序：高位字节在前，低位字节在后，这是人类读写数值的方法。

小端字节序：低位字节在前，高位字节在后，即以0x1122形式储存。

13.什么是数据对齐？什么是数据打包？（选择题必出，计算题）

操作数打包：操作数排布在内存中时保证没有空间被浪费



操作数打包的好处:

（1）节省空间（空间）

（2）减少在处理器寄存器和内存之间来回搬运该数据结构（包含变量a和b）所需的访存次数（时间）

操作数对齐:（选择、计算）结构体大小



相对偏移量从0开始

对齐数为结构体成员自身大小和默认的对齐数的最小值

3、结构体总大小，是最大对齐数的整数倍

例题在ppt

14.寻址模式

①直接寻址: 立即值（常量值）、寄存器（操作数）、内存地址（3）

②间接寻址:

a.内存间接取址（类似C语言的指针）内存和寄存器放的地址

b.寄存器间接取址   

c.基址（寄存器）+偏移量（内存）    互换也可以【基址（内存）+偏移量（寄存器）】

- 减少将整个内存地址放到操作数位置的情况

- 例子：比如C语言中的结构体中的字段

- 指令中的内存地址为一个寄存器（基址寄存器）的内容与一个偏移量（以立即值的形式包含于指令中）的和

- 形式：ld r2,offset(rb)

d.基址+索引：允许从两个寄存器获得有效地址，并将其加和

- 数组取值（基址寄存器+变址寄存器-索引）

e.程序计数器相对寻址

- 分支跳转

- 例：beq r1,r2, offset ---offset=pc+offset, 如果r1==r2,则跳转到pc+offset, 否则执行beq后面的语句

Q:变址和基址寄存器的区别？

都是从寄存器中取地址。

基址寄存器开头位置固定的。

通常变址存放的是偏移量，本质是一致的。

寄存器存的不一样。基址存变量，变址存偏移量。

变址数固定，偏移量放在寄存器。



变址寻址：变址寄存器是偏移量，形式地址A为基地址    循环

基址寻址：基址寄存器是基地址，形式地址是偏移量     多道程序



③存储器寻址：当前主流计算机使用字节寻址来访问存储器的操作数。

- ARMv8, MIPS是要求对齐的，而x86,RISC-v则不要求，但是对齐对增加访问速度有好处。



15. 位计算，原码反码补码计算（必考）

位操作指令：对于给定的二进制数据中的某一位进行置位或复位。（有可能考题）

(1)逻辑移位指令，将数位简单的向左或向右移动指定的位数，在相反的一端补以相同位数的0。

例子，8位寄存器，11110000，左移1位，结果是11100000； 若右移1位，结果是 01111000

计算方法：移动补0





算数移位指令一般用于执行乘以2或者除以2的操作，它将数据作为有符号二进制补码形式来看待，最左边一位不移位，因为它代表符号位。



原/反/补码转换：

正整数：原码、反码和补码是一样的

负数：

	原码转换为反码(反过来同)：符号位不变，数值位分别“按位取反”
2) 原码转换为补码(反过来同)：符号位不变，数值位按位取反,末位再加1



算术移位： 

正数：原/反/补码 左移右移都补0

负数：

原码：左移右移补0

反码：左移右移都补1

补码：左移补0，右移补1w

算数右移，复制符号位到其右边。等价于除以2

▪ 00001110（+14） 00000111（+7）

▪ 11111110（-2） 11111111 （-1）求补码的方法？

算数左移，数位向左移，空位补0，符号位不参与移位。等价于乘以2。

▪00000011（+3） 00000110（+6）

▪11111111（-1） 11111110（-2）

如果移掉的最后一位（除了符号位）跟符号位不匹配，则会发生溢出。

▪10111111（-65）进行算数左移1位的操作，结果为

11111110（-2），移出位为0，跟符号位不同，发生溢出。



16. 什么是二八法则？什么是一九法则？

二八法则：20%的指令使用80%的时间

一九法则：程序执行时间的90%都是在执行程序中10%的代码。

17. RISC的特点（必考，选择或简答）

寄存器方面：

①扩展的寄存器文件  （加大寄存器的数量）

②加载/存储架构-通常去除访问存储器的权利，仅能对寄存器操作   寄存器不如内存通用性强，速度更快但对cpu依赖性更强，因此牺牲兼容性。（充分利用寄存器）

指令简化方面：

③正交的机器指令

• CISC不断增加指令，变得混乱，很多特殊指令，造成译码和执行指令都耗时较多

• 去掉特例，要求指令等长，平等对待CPU资源，另外，指令要求独立，这就叫做正交

④独立的指令和数据高速缓存

• 缓存中指令和数据分开设计(改良型的哈佛结构）

• 指令会按照顺序访问

• 数据不一定按照顺序访问

方便流水线设计方面：

⑤通常，每机器周期执行一条机器指令

⑥简单的寻址方式和指令格式

不足：

⑦不足：对之前版本的兼容性和应用范围受限（仅能对寄存器操作   寄存器不如内存通用性强，速度更快但对cpu依赖性更强，因此牺牲兼容性。）



18. 局部性原理，一九法则，案例

程序的时间局部性：程序即将用到的信息很可能就是目前正在使用的信息。比如迭代算法。

   时间局部性：如果程序中的某条指令一旦被执行，则不久的将来该指令可能再次被执行。   循环

• 程序的空间局部性：程序即将用到的信息很可能与目前正在使用的信息在空间上相邻或者临近。比如数组。

    空间局部性：一旦一个指令或一个存储单元被访问，那么它附近的单元也将很快被访问。               数组（概念、案例、经验规则）





19. 基本术语（命中率，缺失率，缺失损失）

命中率（h）则是CPU在缓存中成功找到数据的概率。

缺失率(m)是CPU没有在缓存中找到数据的概率，等于1-h。

缺失损失（miss penalty）指的是由于在分级存储体系的任何一层中发生缺失而造成的时间损失。

. 缓存设计的原理，为什么一级缓存小？二级缓存更大？（必考）

设计思路缓存设计原理 

• 缓存大小对访问时间有直接的影响，希望缓存尽量小，这是第一级主要考虑的内容

• 降低缺失率，希望缓存容量大一些，所以第二级缓存主要考虑缺失率。况且L2的访问时间只是会影响缺失损失（第一级缺失）





第二章  并行计算

1. Top 500中用的什么操作系统比较多？测试集是什么？浮点计算的单位？

Linux操作系统占比最多

测试集Linpack

千万亿是什么、P是什么、皮特、百万亿次？

一个MFLOPS（megaFLOPS）等于每秒一百万（=10^6）次的浮点运算，

一个GFLOPS（gigaFLOPS）等于每秒十亿（=10^9）次的浮点运算，

一个TFLOPS（teraFLOPS）等于每秒万亿（=10^12）次的浮点运算，(1太拉)

一个PFLOPS（petaFLOPS）等于每秒千万亿（=10^15）次的浮点运算，



	2. 并行计算的基本条件	

	并行计算的三个基本条件：

①并行机：至少包含两台或两台以上处理器，这些处理机通过互连网络相互连接，相互通信。

②应用问题必须具有并行度：应用可以分解为多个子任务，这些子任务可以并行地执行。将一个应用分解为多个子任务的过程，称为并行算法的设计。

③并行编程: 在并行机提供的并行编程环境上，具体实现并行算法，编制并行程序，并运行该程序，从而达到并行求解应用问题的目的。

程序：cuda   openAl 

	

3. flynn分类（必考）

 Flynn分类法（美国Flynn于1966年提出）特点

①三个重要概念：

▪ 指令流（Instruction Stream）：机器执行的指令序列；

▪ 数据流（Data stream）：由指令处理的数据序列；

▪ 多倍性（Multiplicity）：在系统最窄的部件上，处于同一时间单位内，最多可并行执行的指令条数或处理的数据个数。

②分类：

⑴ 单指令流单数据流（SISD——Single Instruction stream Single Data stream）

⑵ 单指令流多数据流（SIMD——Single Instruction stream Multiple Data stream）

⑶ 多指令流单数据流（MISD——Multiple Instruction stream Single Data stream）

⑷ 多指令流多数据流（MIMD——Multiple Instruction stream Multiple Data stream）

③各类结构的概念性框图:

▪  IS ：指令流             ▪  DS ：数据流

▪  CS ：控制流             ▪  CU ：控制部件

▪  PU ：处理部件           ▪  MM 和 SM：存储器



         

           

▪ SIMD：按同一条指令，并行机的各个不同的功能部件同时对不同的数据进行不同的处理，

▪ MIMD：不同的处理器可同时对不同的数据执行不同的指令，目前所有并行机均属于这一类。



4. SIMD和SIMT的区别（必考）（先说共同点，再说不同）

共同点：；单指令流（补充）CUDA-GPU  通常是使用的SIMT Single Instruction Multiple Threads （单指令多线程），是SIMD 的扩展SIMT 的好处就是无需开发者费力把数据凑成合适的矢量(向量）长度，并且SIMT 允许每个线程有不同的分支。

SIMT 比SIMD 更灵活，允许一条指令的多数据分开寻址；SIMD是必须连续在一起的片段。 SIMT 形式上是多线程，本质上还是一个线程，只不过数据可以零散的分散开。但是如果你真的将数据分散开的话，执行效率上又会大打折扣，因为不满足并行访问的要求。通常应用于GPU CUDA 编程。

                 SIMD是CPU上用的，SIMT是GPU上用的

SPMD单程序多数据(SPMD)架构?

SIMD 是从指令级上看的，这意味着SIMD 处理的多数据是执行相同的操作，比如都执行加法。而SPMD 是从程序级上看的，这意味着处理的多数据不一定是执行相同的操作，因为程序里面可以有分支等，即执行路径可以是多条。一句话，SIMD 是多个数据执行相同的操作，SPMD 是多个数据可以执行不同的操作也可以执行相同的操作。不一定是用于CUDA编程。





5. NUMA和ccNUMA的区别

NUMA（Non Uniform Memory Access, 非均匀内存）与ccNUMA的特点

▪ 内存是共享的，即所有的处理器都可以访问该内存（优缺点？）

▪ 优点：内存中某些内存块的物理位置相较于其他处理器的距离更接近于其所关联的处理器，降低了内存的带宽瓶颈，允许系统拥有更多的处理器

▪ 缺点：带来了处理器访问内存时间上的差异，依赖于内存位置。

▪ 解决办法：每个处理器加一个cache, 增加保持缓存一致性协议，叫做ccNUMA

特点：逻辑上讲，在ccNUMA系统上编程类似于在SMP上编程。为了提高性能，更需要关注数据本地化问题和缓存效果

6. 阵列机和向量机的区别，特点（有问题）

a.向量处理机 （Vector Processor）

- 向量由一组有序、具有相同类型和位数的元素组成。

- 在流水线处理机中，设置向量数据表示和相应的向量指令，称为向量处理机。

- 典型的向量机：银河系列I-II，Cray系列超级计算机

b.标量处理机（ Scalar Processor）

- 不具有向量数据表示和相应的向量指令的流水线处理机，称为标量处理机。



c. 阵列处理机（Array Processor）

- 核心:一个由多个处理单元构成的阵列

- 采用资源重复的方法,设置较多的处理单元来提高并行性

- 用单一的控制部件来控制多个处理单元对各自的数据进行相同的运算和操作。

- 也属于向量机的一种方式

- 典型案例：MasPar MP-1计算机



7. socker，cluster，thread区别和特点

socket 就是主板上的CPU 插槽; Core 就是socket 里独立的一组程序执行的硬件单元，比如寄存器，计算单元等;区别概念

▪Thread ：就是超线程hyperthread 的概念，逻辑的执行单元，独立的执行上下文，但是共享core内的寄存器和计算单元。



8. 分布式计算和并行计算的区别（必考）

并行计算：具有多个处理器的单个系统处理同一问题。

分布式计算：许多系统由调度程序松散耦合以处理相关问题

分布式计算VS 并行计算

并行计算的目的是利用多处理器求解单个问题，而分布式计算则更多的是提供方便，包括高可用性、可靠性等。

▪ 并行计算中，通常，处理器间交互频繁，具有细粒度和低开销的特征，注重短的执行时间；分布式计算，通常，处理器间的交互不频繁，交互粒度是粗粒度（任务的独立性更强），注重长的正常运行时间。

▪ 并行计算可以被看作分布式计算的一个特定的紧密耦合的形式，分布式计算可以被视为并行计算的松散耦合形式。



9. 加速比，效率（必考，名词解释或计算题）

顺序执行程序（串行）执行时间除以计算同一问题的并行程序的执行时间，即加速比 = T S / T P，T S 是顺序执行的时间，T P 是并行执行的时间。

效率 (Efficiency)

是增加的处理器被有效利用情况的度量。它定义为加速比与处理器数目的比率即：

E = S / P



10. 阿姆达尔定律（必考）

如果一个程序中的1/f是串行的，那么加速比不可能超过f。（手推）



11. 数据相关性（必考）

写后读





读后写

                                                                           



写后写





12. 并行粒度，粗粒度，细粒度（必考，名词解答或选择）

并行粒度 (Parallel Granularity )：由线程或者进程间的交互频率决定，是计算与通信频率比例的定性度量。

- 粗粒度并行(Coarse-grained parallelism)：线程或者进程两次通信之间，进行相对大量的计算工作。

- 细粒度并行(Fine-grained parallelism) ：指的是线程或者进程间交互频繁的计算。



13. 流水线段，深度，瓶颈，通过时间，排空时间流水线分类（必考）

流水线中的每个子过程及其功能部件称为流水线的级或段，段与段相互连接形成流水线。流水线的段数称为流水线的深度。

流水线中各段的时间应尽可能相等，否则将引起流水线堵塞、断流。

• 时间最长的段将成为流水线的瓶颈。

通过时间：第一个任务从进入流水线到流出结果所需的时间。

• 排空时间：最后一个任务从进入流水线到流出结果所需的时间。

提到的流水线分类

静态流水线：在同一时间内多功能流水线中的各段只能按同一种功能的连接方式工作。

对于静态流水线来说，只有当输入的是一串相同的运算任务时，流水的效率才能得到充分的发挥。

- 动态流水线：在同一时间内，多功能流水线中的各段可以按照不同的方式连接，同时执行多种功能。

 优点: 灵活，能够提高流水线各段的使用率，从而提高处理速度。

 缺点: 控制复杂。

- 静、动态流水线时空图的对比



④按照流水线中是否有反馈回路来进行分类：

- 线性流水线：流水线的各段串行连接，没有反馈回路。数据通过流水线中的各段时，每一个段最多只流过一次。

- 非线性流水线：流水线中除了有串行的连接外，还有反馈回路。

- 非线性流水线的调度问题

• 确定什么时候向流水线引进新的任务，才能使该任务不会与先前进入流水线的任务发生冲突——争用流水段。



⑤根据任务流入和流出的顺序是否相同来进行分类：

- 顺序流水线：流水线输出端任务流出的顺序与输入端任务流入的顺序完全相同。每一个任务在流水线的各段中是一个跟着一个顺序流动的。

- 乱序流水线：流水线输出端任务流出的顺序与输入端任务流入的顺序可以不同，允许后进入流水线的任务先完成（从输出端流出）。也称为无序流水线、错序流水线、异步流水线。



14. 流水线的指标（必考，选择）

吞吐率：在单位时间内流水线所完成的任务数量或输出结果的数量。



n ： 任务数

T k ： 处理完成n个任务所用的时间

流水线的加速比

加速比：完成同样一批任务，不使用流水线所用的时间与使用流水线所用的时间之比。

- 假设：不使用流水线（即顺序执行）所用的时间为T s ，使用流水线后所用的时间为T k ，则该流水线的加速比为：



流水线的效率

流水线的效率：流水线中的设备实际使用时间与整个运行时间的比值，即流水线设备的利用率。

▪ 由于流水线有通过时间和排空时间，所以在连续完成n个任务的时间内，各段并不是满负荷地工作。



列一大串指标看哪个不是流水线的，对流水线某个概念进行名词解释



15. MPI，OpenMP，CUDA适用于什么？有什么特点？

消息传递接口编程：MPI

- MPICH，OpenMPI，LAM MPI

- 点对点通信，聚合通信，进程组、通信器等设计

MPI（Message Passing Interface），消息传递接口

定义：一个函数库，而非一种程序语言；消息传递并行编程规范的代表；

消息传递编程模型

特点：很好的可移植性；很好的可扩展性；容易使用，提供明确、通用的函数接口；

完备的异步通信功能



②共享内存编程接口：OpenMP

- 编译指导       - 函数调用      - 环境变量

OpenMP是什么？？

- 多线程并行应用程序界面；

- 显示地指导编译器如何以及何时利用应用程序中的并行性。



③典型加速单元：CUDA

- NVIDIA GPU 或者显卡

CUDA 执行模型结构特点、结构层次

▪ 将CPU作为主机端（称之为Host）,

▪ 而GPU通常是作为设备来看待的（称之为Device）

▪ 数据要从主机端传输到设备端，才能进行计算。

▪ CUDA的基本思想是尽量开发线程级并行。

▪ kernel函数是怎么执行的。一个kernel程式会有一个grid，grid底下又有数个block，每个block是一个thread群组。在同一个block中thread可以通过共享内存（shared memory）来通信，同步。而不同block之间的thread是无法通信的。



16. CPU与GPU的区别

GPU和CPU对比计算有什么不同



- GPU的浮点计算能力/内存带宽远超CPU

- GPU数据吞吐量远超CPU

- GPU的并行性远超CPU（多线程、流水线等并行性）

- GPU逻辑控制能力比CPU弱

- CPU 基于低延时的设计，而GPU是基于大的吞吐量设计

- GPU的运算速度取决于雇了多少“小学生”（大量简单运算），CPU的运算速度取决于请了多么厉害的“教授”（复杂的计算）。



17.  host，device，grid

将CPU作为主机端（称之为Host）,

▪ 而GPU通常是作为设备来看待的（称之为Device）

kernel函数是怎么执行的。一个kernel程式会有一个grid，grid底下又有数个block，每个block是一个thread群组。在同一个block中thread可以通过共享内存（shared memory）来通信，同步。而不同block之间的thread是无法通信的。



第三章  云计算相关技术

1. 云计算的五个特征，四个模型，三个服务模式（必考）

五大基础特征



四个部署模型



三种服务模式



服务模型的概念

①IaaS（Infrastructure as a Service, 基础设施即服务）

- 将海量的 硬件资源集中到一起，并以虚拟化的形式出现。通过IaaS管理平台将不同类别的资源统一管理，并将这些资源交付给最终用户。

- 一般情况下，供应商提供计算、存储能力，用户根据自身需求租用适宜的资源，并对其租用的部分进行周期性的付费。

- 案例：OpenStack

②PaaS（Platform as a Service, 平台即服务）

- 是把服务平台打包成一个服务，向用户提供该服务的一种模式。通常来说，PaaS定位于中间件，提供定制化研发平台服务。

- 典型案例：docker, K8S, Hadoop ,Microsoft Windows Azure.

③SaaS (Software as a Service, 软件即服务）

- 通过Internet交付软件的模式，用户不用购置信息系统，而是向SaaS服务提供商租用信息系统，并通过Web浏览器登陆、操作该信息系统，完成企业的生产、经营和管理行为。

- 典型案例：特定应用，调查问卷系统



2. 虚拟化的分类（必考）

虚拟化的分类

①虚拟化的分类1

a.软件虚拟化：就是通过软件模拟VMM层，通过纯软件的环境来模拟执行客户机的指令典型的软件虚拟化可以是QEMU。

b.硬件虚拟化：计算机硬件本身提供能力让客户机指令独立执行，而不完全需要VMM截获重定向。典型的是X86的Intel VT技术。VMM运行在root mode,拥有完整的硬件访问控制权限，大多数情况不需要VMM截获并翻译，而是在受限的硬件环境下运行。

②虚拟化的分类2

a.半虚拟化（Para-Virtualization)

- 让客户机意识到自己是运行在虚拟化环境中，并做相应的修改以配合VMM

- 本质上，半虚拟化弱化了对虚拟机特殊指令的被动截获要求，将其转化为客户机操作系统的主动通知。半虚拟化需要修改客户机操作系统的源代码来实现主动通知。

- 起初是为了解决x86体系结构上完全虚拟化的困难，由于需要修改OS，工作效率相对完全虚拟化要高很多。典型的有Xen、KVM-PowerPC。

b.全虚拟化客户机

- 客户机操作系统完全不需要改动，敏感指令在操作系统和硬件之间被VMM捕捉，增加了VMM的复杂度。

- 即所抽象的VM具有完全的物理特性，移植性非常好。但是缺点是效率不高。典型的有VMware，Virtualbox，Virtual PC，KVM-x86。

③虚拟化的分类3

a. type1虚拟化

- Hypervisor 运行在一个宿主机OS上，

- 如Vmware Wokrwation.

- 客户机是在宿主机OS的一个抽象，通常抽象为进程。



物理机上首先安装常规的操作系统，比如Redhat、Ubuntu 和Windows Hypervisor作为OS上的一个程序模块运行，并对管理虚拟机进行管理。 KVM 、VirtualBox 和VMWare Workstation 都属于这个类型。

b.type2虚拟化

- 虚拟化层直接运行在硬件之上，没有所谓宿主机操作系统

- 典型为Xen，Vmware ESX

	- hypervior 也叫做native或者bare metal，直接运行在硬件之上。	

	

Hypervisor直接安装在物理机上，多个虚拟机在Hypervisor上运行。 Hypervisor  实现方式一般是一个特殊定制的Linux系统。Xen和VMWare的ESXi都属于这个类型

3. 013模型

特权级



x86 架构支持4种特权级别(用户态/系统态）,ring 0是最高级（操作系统OS/drivers）， ring 3最低 (应用程序application) 。（ 2,3不常用）





4. 特权指令，敏感指令（必考）

特权指令

特权指令是一些操作和管理关键系统资源的指令，这些指令只有在最高特权级上才能够运行。

敏感指令

传统的机器只有特权指令和普通指令两种概念，而在有虚拟化的机器上则还有一个独特的概念：敏感指令。敏感指令是指在虚拟化时必须要在最高特权级运行的指令。在RISC体系中，所有的特权指令都是敏感指令，因此可以支持完全虚拟化。但是在x86体系的虚拟化中，部分敏感指令并不是特权指令，那么这些非特权指令的敏感指令在系统运行过程中就不会引发异常，也就无法被捕获并在最高特权级执行。而这些敏感指令在非最高特权级下运行和在最高特权级下运行会有不同的结果，这对于一个要求性能可靠的计算机不能绝对容忍的。因此，半虚拟化技术，以Xen为先例，主要解决的就是这个问题——如何捕获非特权指令的敏感指令。如何进行模型转换

半虚拟化原理



▪ Xen 采用修改 Guest OS 内核的方法对这些有缺陷的指令进行替换，

▪ Xen 运行在最高特权级的 ring0 ，操作系统被特权解除，运行在 ring1 ，ring3 运行应用程序，构成虚拟机系统中的“ 0/1/3 模型”怎么变



 硬件虚拟化硬件辅助怎么操作RING-1

上述的半虚拟化方法是在最开始没有硬件虚拟化技术支持的情况下的一种解决方案。不过为了克服半虚拟化带来的不便，如：修改Guest OS、性能开销。现在Intel、AMD都在硬件层面上支持虚拟化了，就是我们熟知的Intel VT、 AMD VT 等技术。在硬件虚拟化技术的支持下，那些原本有缺陷的指令能够直接通过硬件被捕获，也就不需要修改Guest OS内核，从而提高了系统的可移植性。



	5. SDN的特点（必考，简答题）	

SDN(简答题)

▪ 概念

- （Software Defined Netrork）软件定义网络

- 2006年，以斯坦福大学教授Nike Mckewn为首的团队提出了OpenFlow的概念，并基于OpenFlow技术实现网络的可编程能力（OpenFlow只是实现SDN的一个协议），是网络像软件一样灵活编程，SDN技术应运而生。

SDN使得网络可编程化，这就使得网络在满足用户的需求方面更加灵活

SDN将控制功能从网络交换设备中分离出来，将其移入到逻辑上独立的控制环境------网络控制系统中

SDN特点：①网络开发可编程   ②控制平面与数据平面的分离   ③逻辑上的集中控制

	

6. 容器的特点

容器（docker） 与虚拟机的区别

▪ 一种沙盒技术， 可以将应用运行在其中，与外界隔离 这个沙盒可以被方便地“转移”。



7. SOA和微服务的区别

服务组合SOA的特点

- 服务组合SOA，面向服务是一种设计范式，用户创建解决方案的逻辑单元，这些逻辑单元可组合、可复用，以支持实现面向服务计算的特定战略目标和收益。

区别
```
:::
