---
title: 人工智能题库
date: 2025-12-20
---

# 人工智能基础课专项题库（完整整理版）
**含：原题完整呈现｜原文答案整理｜本题分析｜参考答案（含计算）**


## 一、逻辑回归计算题专项题库（含答案与解析）

### 计算题 1（25 分）

#### 题目（完整呈现）
某二分类任务中，逻辑回归模型参数为 $w_0=-1.5,\; w_1=0.6,\; w_2=0.4$，逻辑函数为 sigmoid 函数
$$
\sigma(z)=\frac{1}{1+e^{-z}},
$$
其中线性回归输出
$$
z=w_0+w_1x_1+w_2x_2。
$$
现有测试样本 $x=(x_1=4,x_2=2)$，真实标签 $y=1$（正例）。要求：

1. 计算该样本的 $z$ 值与预测概率 $\hat{y}$；
2. 计算其交叉熵损失（损失函数 $L(y,\hat{y})=-y\ln\hat{y}-(1-y)\ln(1-\hat{y})$）；
3. 简述逻辑回归中 sigmoid 函数的核心作用，以及为何交叉熵损失比均方误差更适合逻辑回归的优化。

#### 原文答案
原文未提供该题答案与解析。

#### 本题分析
- 本题考查逻辑回归的基本计算链路：**线性输出 $z$ → sigmoid 得到概率 $\hat{y}$ → 交叉熵计算损失**。
- 交叉熵是逻辑回归在伯努利分布假设下的负对数似然（MLE），与优化目标一致；均方误差更适合回归场景。

#### 参考答案（含计算）
1) 线性输出：
$$
z=-1.5+0.6\times 4+0.4\times 2=-1.5+2.4+0.8=1.7
$$
预测概率：
$$
\hat{y}=\sigma(1.7)=\frac{1}{1+e^{-1.7}}\approx 0.8455
$$

2) 因为 $y=1$，损失化简为 $L=-\ln(\hat{y})$：
$$
L(1,\hat{y})=-\ln(0.8455)\approx 0.1678
$$

3) 简述：
- sigmoid 将 $z\in(-\infty,+\infty)$ 映射到 $(0,1)$，可解释为正类概率；
- 交叉熵对分类概率的惩罚更匹配（尤其在误分且置信度高时惩罚大），梯度信号更稳定。

---

### 计算题 2（25 分）

#### 题目（完整呈现）
某二分类任务的训练数据如下表（标签“1”为正例，“0”为负例），逻辑回归模型参数为 $w_0=-0.8,\; w_1=0.3,\; w_2=0.5$，sigmoid 函数为
$$
\sigma(z)=\frac{1}{1+e^{-z}},
\quad z=w_0+w_1x_1+w_2x_2。
$$
现有测试样本 $x=(x_1=5,x_2=3)$，真实标签 $y=0$。

训练数据如下：

| 样本序号 | 标签 $y$ | $x_1$ | $x_2$ |
|---:|---:|---:|---:|
| 1 | 1 | 3 | 4 |
| 2 | 1 | 4 | 5 |
| 3 | 0 | 1 | 2 |
| 4 | 0 | 2 | 3 |

要求：

1. 计算样本 $x$ 的 $z$ 值与预测概率 $\hat{y}$；
2. 计算其交叉熵损失；
3. 若设定分类阈值为 0.5，该样本的预测类别是什么？说明阈值调整对二分类任务中精确率和召回率的影响（以阈值增大为例）。

#### 原文答案
原文未提供该题答案与解析。

#### 本题分析
- 本题除了计算，还考察阈值与 **Precision/Recall trade-off**。
- 阈值增大意味着更谨慎地判正：通常 **FP 降低 → Precision 上升**；同时 **FN 增加 → Recall 下降**。

#### 参考答案（含计算）
1) 线性输出：
$$
z=-0.8+0.3\times 5+0.5\times 3=-0.8+1.5+1.5=2.2
$$
预测概率：
$$
\hat{y}=\sigma(2.2)=\frac{1}{1+e^{-2.2}}\approx 0.9002
$$

2) 因为 $y=0$，损失化简为 $L=-\ln(1-\hat{y})$：
$$
L(0,\hat{y})=-\ln(1-0.9002)\approx 2.3051
$$

3) 阈值 0.5 下：
- $\hat{y}\approx 0.9002>0.5$，预测类别为 **1（正类）**。

阈值增大的一般影响：
- Precision 倾向上升（更少样本被判为正，FP 通常下降）；
- Recall 倾向下降（更多真实正类被判为负，FN 通常上升）。

---

### 计算题 3（25 分）

#### 题目（完整呈现）
某二分类任务中，逻辑回归模型参数为 $w_0=-2.0,\; w_1=0.7,\; w_2=0.2,\; w_3=0.4$（含 3 个特征），sigmoid 函数为
$$
\sigma(z)=\frac{1}{1+e^{-z}},
$$
其中
$$
z=w_0+w_1x_1+w_2x_2+w_3x_3。
$$
现有测试样本 $x=(x_1=2,x_2=6,x_3=4)$，真实标签 $y=1$。要求：

1. 计算该样本的 $z$ 值与预测概率 $\hat{y}$；
2. 计算其交叉熵损失；
3. 逻辑回归输出是“样本属于正类的概率”，请说明如何基于该概率实现多分类任务（以三分类为例）。

#### 原文答案
原文未提供该题答案与解析。

#### 本题分析
- 计算部分与前两题一致，只是维度更高。
- 多分类常见标准实现：**One-vs-Rest（OvR）** 或 **Softmax（多项逻辑回归）**。

#### 参考答案（含计算）
1) 线性输出：
$$
z=-2.0+0.7\times 2+0.2\times 6+0.4\times 4=-2.0+1.4+1.2+1.6=2.2
$$
预测概率：
$$
\hat{y}=\sigma(2.2)\approx 0.9002
$$

2) 因为 $y=1$，损失为：
$$
L(1,\hat{y})=-\ln(0.9002)\approx 0.1051
$$

3) 三分类实现（举例）：
- OvR：训练 3 个二分类器，输出 $P(y=k\mid x)$，预测取最大者；
- Softmax：
  $$
  P(y=k\mid x)=\frac{e^{z_k}}{\sum_{j=1}^3 e^{z_j}}
  $$
  得到三类概率分布。

---

### 计算题 4（25 分）

#### 题目（完整呈现）
某二分类任务中，逻辑回归模型经训练后得到参数 $w_0=-1.2,\; w_1=0.4,\; w_2=0.6$，sigmoid 函数为
$$
\sigma(z)=\frac{1}{1+e^{-z}},
\quad z=w_0+w_1x_1+w_2x_2。
$$
现有两个测试样本：样本 A $x_A=(x_1=3,x_2=2)$（真实标签 $y_A=1$），样本 B $x_B=(x_1=1,x_2=1)$（真实标签 $y_B=0$）。要求：

1. 分别计算样本 A 和样本 B 的 $z$ 值、预测概率 $\hat{y}_A$ 和 $\hat{y}_B$；
2. 分别计算两个样本的交叉熵损失，并比较两个损失值的大小，说明损失值大小的物理意义；
3. 简述逻辑回归模型参数 $w_1$ 和 $w_2$ 的符号对特征与正类概率的影响。

#### 原文答案
原文未提供该题答案与解析。

#### 本题分析
- 同一组参数下，损失大小反映“预测概率与真实标签匹配程度”。
- $y=1$ 时希望 $\hat{y}$ 越大越好；$y=0$ 时希望 $\hat{y}$ 越小（即 $1-\hat{y}$ 越大）越好。

#### 参考答案（含计算）
1) 样本 A：
$$
z_A=-1.2+0.4\times 3+0.6\times 2=-1.2+1.2+1.2=1.2
$$
$$
\hat{y}_A=\sigma(1.2)\approx 0.7685
$$

样本 B：
$$
z_B=-1.2+0.4\times 1+0.6\times 1=-1.2+0.4+0.6=-0.2
$$
$$
\hat{y}_B=\sigma(-0.2)\approx 0.4502
$$

2) 损失：
$$
L_A=-\ln(\hat{y}_A)\approx 0.2633,
\qquad
L_B=-\ln(1-\hat{y}_B)\approx 0.5981
$$
且通常 $L_B>L_A$ 表示样本 B 对负类的置信度不足，惩罚更大。

3) 参数符号影响：
- 若 $w_i>0$，则 $x_i$ 增大使 $z$ 增大，$\hat{y}$ 增大（更倾向正类）；
- 若 $w_i<0$，则相反。

---

## 二、人工智能基础课程简答题复习题库（含答案与解析）

### 一、简答题（每小题 5 分，共 25 分）

---

### 1. 人工智能的定义、发展阶段与目标差异

#### 题目（完整呈现）
简述人工智能的定义、发展阶段，智能的核心特征，以及“模拟人类智能”与“实现理性行为”的核心差异。

#### 原文答案（整理排版）
- **定义**：人工智能是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学，旨在让机器具备类似人类的感知、推理、学习、决策等能力。
- **发展阶段**：
  1. 孕育期（1946-1955）：数理逻辑、计算理论等基础学科发展，为 AI 奠定理论基础；
  2. 诞生期（1956）：达特茅斯会议提出“人工智能”概念，标志着 AI 正式诞生；
  3. 早期发展期（1956-1974）：符号主义方法主导，出现逻辑推理、专家系统雏形；
  4. 低谷期（1974-1980）：技术瓶颈与资金短缺，AI 发展陷入停滞；
  5. 复苏期（1980-1993）：专家系统商业化应用，机器学习算法逐渐兴起；
  6. 稳健发展期（1993-2010）：统计学习、神经网络技术逐步成熟，大数据积累为后续爆发奠定基础；
  7. 爆发期（2010 至今）：深度学习突破，在图像识别、自然语言处理等领域取得突破性进展，AI 广泛落地。
- **智能的核心特征**：感知能力、记忆与学习能力、推理与决策能力、自适应能力。
- **目标差异**：
  1. 模拟人类智能：追求机器行为与人类思维过程一致性，侧重“形似/神似”；
  2. 实现理性行为：强调在环境中做出最优决策，侧重“结果最优”。

#### 本题分析
答题结构建议固定为：**定义一句话 + 阶段分点 + 智能特征 + 目标差异对照**。目标差异可用“过程一致性 vs 结果最优性”收束。

#### 参考答案（更紧凑版）
人工智能研究使机器具备感知、学习、推理与决策能力；发展经历孕育、诞生、早期发展、低谷、复苏、稳健与深度学习爆发阶段；智能特征包括感知、学习记忆、推理决策、自适应；模拟人类智能强调过程机制类人，理性行为强调目标达成与结果最优。

---

### 2. 谓词逻辑中量词

#### 题目（完整呈现）
谓词逻辑中“量词”的作用是什么？全称量词（$\forall$）和存在量词（$\exists$）的含义分别是什么？用谓词公式表示“有些学生喜欢机器学习课程”。

#### 原文答案（整理排版）
- 量词的作用：用于约束谓词中个体变元的取值范围，使谓词能够表达具有普遍性或存在性的命题。
- 全称量词（$\forall$）：表示“对于所有的/任意一个”，约束的变元在其取值范围内所有个体都满足谓词条件。
- 存在量词（$\exists$）：表示“存在某个/至少有一个”，约束的变元在其取值范围内至少有一个个体满足谓词条件。
- 谓词公式表示：设 $S(x)$ 表示“$x$ 是学生”，$L(x,y)$ 表示“$x$ 喜欢 $y$”，机器学习课程为常量 $M$，则：
$$
\exists x\bigl(S(x) \land L(x,M)\bigr)
$$

#### 本题分析
关键点是“量词限定变量作用域”。最后一问的“有些”必须用 $\exists$ 而不是 $\forall$。

#### 参考答案
同原文答案；必要时可补充 $x$ 的论域（如“所有人”）以严谨化表述。

---

### 3. k 近邻（kNN）算法

#### 题目（完整呈现）
简述 k 近邻（kNN）算法的核心思想、工作流程，以及 $k$ 值对模型泛化能力的影响。

#### 原文答案（整理排版）
- 核心思想：“近朱者赤，近墨者黑”，一个样本的类别由其周围 $k$ 个距离最近的训练样本类别投票决定。
- 工作流程：
  1. 计算测试样本与所有训练样本之间的距离（常用欧氏距离、曼哈顿距离）；
  2. 按距离从小到大排序，选取前 $k$ 个距离最近的训练样本（邻居）；
  3. 统计 $k$ 个邻居中各类别出现次数（投票）；
  4. 将出现次数最多的类别作为测试样本预测类别。
- $k$ 值影响：
  1. $k$ 过小：对噪声敏感，易过拟合；
  2. $k$ 过大：忽略局部特征，易欠拟合；
  3. 最优 $k$：通过交叉验证选取，平衡过拟合与欠拟合。

#### 本题分析
除流程外，常见加分点：kNN 属于“惰性学习/基于实例学习”，预测时开销大；且对特征尺度敏感，通常需要标准化。

#### 参考答案
在原文基础上补充：当特征量纲不同需做归一化；可用 KD-Tree/Ball-Tree 或近似最近邻方法加速查询。

---

### 4. 梯度下降及 BGD/SGD/MBGD

#### 题目（完整呈现）
解释梯度下降的核心原理，以及批量梯度下降（BGD）、随机梯度下降（SGD）和小批量梯度下降（MBGD）的主要区别。

#### 原文答案（整理排版）
- 核心原理：梯度下降是一种迭代优化算法，沿损失函数的梯度负方向更新模型参数，使损失逐步减小并收敛。
- 区别：
  1. BGD：每次迭代使用全部训练样本计算梯度，收敛稳定但计算量大；
  2. SGD：每次迭代使用 1 个样本更新，速度快但波动大；
  3. MBGD：每次迭代使用小批量样本更新，兼顾稳定与效率，实践常用。

#### 本题分析
建议给出通用更新式以体现理解：
$$
\theta \leftarrow \theta - \eta\nabla_\theta J(\theta)
$$
并指出三者本质差异是“梯度估计方差 vs 计算成本”的权衡。

#### 参考答案
同原文，并可补充：SGD 常配合动量、学习率衰减或 Adam 等自适应方法以提高稳定性。

---

### 5. 特征选择与嵌入式方法

#### 题目（完整呈现）
什么是特征选择？特征选择的主要目的有哪些？简述嵌入式特征选择方法的原理，并举例说明。

#### 原文答案（整理排版）
- 定义：特征选择是从原始特征集中筛选对模型预测有用的子集，剔除冗余、无关特征的过程。
- 主要目的：
  1. 减少计算量，提高训练与预测效率；
  2. 降低模型复杂度，避免过拟合；
  3. 简化模型解释性；
  4. 去除噪声特征，提升预测精度。
- 嵌入式原理：将特征选择与模型训练融为一体，模型在训练中自动学习特征重要性并筛选关键特征。
- 举例：L1 正则化逻辑回归（不重要特征系数收缩至 0）；决策树通过信息增益/基尼系数选择分裂特征。

#### 本题分析
本题常与“过滤式/包裹式/嵌入式”对比出现。嵌入式强调训练中完成选择，兼顾效率与效果。

#### 参考答案
同原文；可补充例子：Lasso、Elastic Net、XGBoost/Random Forest 的特征重要性筛选。

---

## 三、二、计算题（25 分）

### 题目（完整呈现）
某二分类任务中，逻辑回归模型参数为 $w_0=-2,\; w_1=0.8,\; w_2=0.4$，逻辑函数为
$$
\sigma(z)=\frac{1}{1+e^{-z}},
\quad z=w_0+w_1x_1+w_2x_2。
$$
现有测试样本 $x=(x_1=3,x_2=4)$，真实标签 $y=0$。

要求：
1. 计算该样本的 $z$ 值与预测概率 $\hat{y}$；
2. 计算其交叉熵损失（$L(y,\hat{y})=-y\ln\hat{y}-(1-y)\ln(1-\hat{y})$）；
3. 说明逻辑回归输出概率的本质，以及为何能处理二分类任务。

### 原文答案（整理排版，并更正明显公式笔误）
1) 计算 $z$ 值与预测概率：

$$
z=w_0+w_1x_1+w_2x_2 = -2 + 0.8\times 3 + 0.4\times 4 = -2 + 2.4 + 1.6 = 2.0
$$

$$
\hat{y}=\sigma(z)=\frac{1}{1+e^{-2}}\approx \frac{1}{1+0.1353}\approx 0.8808
$$

2) 计算交叉熵损失：

已知 $y=0$，代入损失函数：
$$
L(0,0.8808) = -\ln(1-0.8808)=-\ln(0.1192)\approx 2.120
$$

3) 逻辑回归处理二分类任务的原理：

- $\hat{y}$ 表示样本属于正类（标签 1）的概率，$1-\hat{y}$ 表示属于负类（标签 0）的概率；
- sigmoid 将线性输出 $z$ 映射到 $(0,1)$；
- 设定阈值（如 0.5）将概率转为类别；
- 交叉熵能有效衡量预测概率与真实标签差异，通过梯度下降优化参数。

### 本题分析
- 原文出现 “$\sigma(z)=\frac{1}{1+e^{-i}}$” 的明显笔误（应为 $e^{-z}$），此处已统一更正。
- 原文损失值 $2.120$ 为近似值。若按更精确的 $\hat{y}$ 计算，数值会略有差异（见参考答案）。

### 参考答案（更精确数值）
1) $z=2.0$，$\hat{y}=\sigma(2)\approx 0.8808$。

2) 因为 $y=0$：
$$
L(0,\hat{y})=-\ln(1-\hat{y})\approx 2.1269
$$

3) 输出概率本质：对 $P(y=1\mid x)$ 的参数化建模（对数几率/伯努利模型）；阈值决策将概率映射为二元类别。

---

## 四、三、分析题（25 分）：支持向量机（SVM）

### 题目（完整呈现）
支持向量机（SVM）是二分类任务的经典模型，结合其核心机制分析以下问题：

1. 解释“支持向量”的定义，以及其在 SVM 中的作用；
2. 当训练数据存在噪声导致线性不可分，SVM 如何通过“软间隔”技术解决？简述软间隔的核心参数及其意义；
3. 对比 SVM 与决策树在分类任务中的核心差异（从模型原理、特征依赖性、过拟合风险三个维度）。

### 原文答案（整理排版，公式已 LaTeX 化）
1) 支持向量的定义与作用：
- 定义：在 SVM 最优分离超平面两侧，距离超平面最近的训练样本点称为支持向量（包括正类支持向量和负类支持向量）。
- 作用：支持向量决定最优分离超平面的位置，是模型的关键样本。移除非支持向量样本，最优超平面通常不会改变；模型复杂度由支持向量数量决定。

2) 软间隔技术解决线性不可分：
- 核心思路：引入松弛变量 $\xi_i(\xi_i\ge 0)$ 衡量违反程度，使“间隔最大化”与“违反程度最小化”平衡。
- 优化目标与约束（原文整理）：
$$
\min \ \frac{1}{2}\lVert w\rVert_2^2 + C\sum_i \xi_i
$$
约束：
$$
y_i(w\cdot x_i+b)\ge 1-\xi_i,\quad \xi_i\ge 0
$$
- $C$ 的意义：惩罚系数。$C$ 越大惩罚越重，模型更倾向严格分类，易过拟合；$C$ 越小惩罚越轻，允许更多违反，易欠拟合。

3) SVM 与决策树对比（原文表格整理）

| 对比维度 | 支持向量机（SVM） | 决策树 |
|---|---|---|
| 模型原理 | 最大化间隔的最优超平面（可配核函数） | 贪心递归分裂特征构建树 |
| 特征依赖性 | 对尺度敏感（需归一化）；不直接给出特征重要性 | 对尺度不敏感；解释性强 |
| 过拟合风险 | 小样本高维下通常稳健；复杂核可能过拟合 | 树过深易过拟合，需剪枝/限深 |

### 本题分析
得分点集中在：**支持向量决定超平面**、**软间隔用 $\xi_i$ 与 $C$ 平衡结构风险与经验风险**、以及按三维度对比模型差异。

### 参考答案（补充更“考试化”表述）
- 支持向量对应 KKT 条件下拉格朗日乘子非零的样本，是决策函数的关键支撑点；
- 软间隔等价于在最大间隔框架下引入 hinge loss 松弛，$C$ 控制惩罚力度；
- 决策树偏高方差模型，需要剪枝；SVM 偏最大间隔思想，适合小样本高维。

---

## 五、四、设计题（25 分）：用户留存预测系统

### 题目（完整呈现）
某互联网公司计划构建“用户留存预测系统”，用于根据用户的注册时长、日均使用时长、点击功能模块次数、付费行为等数据，预测用户 30 天内是否会留存（二分类任务：留存为 1，流失为 0）。请设计该系统的核心方案：

1. 选择合适的机器学习模型，说明选择理由；
2. 设计数据预处理与特征工程的完整流程；
3. 说明模型训练、评估的关键步骤，以及该场景下“精确率”的重要性。

### 原文答案（整理排版）
#### 1) 模型选择：逻辑回归与梯度提升树（XGBoost/LightGBM）组合模型
- 逻辑回归：简单、训练快、可解释性强，适合作为基准模型；可捕捉一定线性关系。
- 梯度提升树：可学习非线性关系与特征交互，预测精度高，对缺失/异常有鲁棒性。
- 组合：以 XGBoost 保证精度，逻辑回归提供解释或融合。

#### 2) 数据预处理与特征工程流程
（1）数据清洗  
- 缺失值：数值型中位数填充；分类特征众数填充；关键缺失样本删除。  
- 异常值：IQR 规则截断；逻辑异常（如日均使用时长超过 24 小时）删除。  

（2）特征工程  
- 编码：二分类 0/1；多分类 One-hot。  
- 构造：比率特征、统计特征、时间特征。  
- 归一化：逻辑回归使用 Z-score 标准化；树模型可不必。  
- 特征选择：相关系数过滤 + XGBoost 特征重要性筛选。

#### 3) 模型训练与评估
- 数据划分：训练/验证/测试 = 7:2:1（分层抽样）。  
- 类别平衡：过采样正例 + 欠采样负例。  
- 调参：逻辑回归调 $C$；XGBoost 调学习率、树深等。  
- 融合：加权融合（示例）：
$$
\hat{p} = 0.7\hat{p}_{xgb} + 0.3\hat{p}_{lr}
$$
- 指标：
  - Accuracy
  - Precision：$\mathrm{Precision}=\frac{TP}{TP+FP}$
  - Recall：$\mathrm{Recall}=\frac{TP}{TP+FN}$
  - F1：
    $$
    F1=\frac{2\cdot \mathrm{Precision}\cdot \mathrm{Recall}}{\mathrm{Precision}+\mathrm{Recall}}
    $$
- 精确率重要性：系统对“预测为留存”的用户投放运营资源；精确率低会造成营销成本浪费，精确率高提升投入产出比。

### 本题分析
- 本题属于端到端方案题：模型选择、数据清洗、特征工程、训练评估必须闭环。
- 业务解释要落到成本：在资源投放场景下 FP 的代价通常更高，因此常强调 Precision 或 Precision@K。
- 若需要更完整工程方案，可补充线上 A/B、阈值/Top-K 策略与预算约束。

### 参考答案（更工程化补充）
- 明确标签定义与时间窗：例如用注册后前 7 天行为预测未来 30 天留存，避免标签泄露；
- 引入 PR-AUC/ROC-AUC、分人群稳定性评估；
- 线上采用 A/B 测试，指标可用留存提升、ROI、投放成本；
- 阈值策略可根据预算选 Top-K 或优化 Precision@K。

---

（整理完）
