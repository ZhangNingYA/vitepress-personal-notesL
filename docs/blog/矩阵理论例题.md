---
title: 矩阵理论 书本例题
date: 2025-12-24
---
# 矩阵理论于应用例题
## 例 2.3.1 平面旋转变换与矩阵表示

### 题目描述

定义平面旋转变换 $T: \mathbb{R}^2 \rightarrow \mathbb{R}^2$，满足：

$$
T(x) = \begin{bmatrix} \cos \varphi & -\sin \varphi \\ \sin \varphi & \cos \varphi \end{bmatrix} x, \quad \forall x = [x_1, x_2]^\top \in \mathbb{R}^2
$$

### 题目解答（原书推导过程）

取定 $\mathbb{R}^2$ 中的标准基 $e_1, e_2$，则有：

$$
\begin{aligned}
T(e_1) &= \begin{bmatrix} \cos \varphi & -\sin \varphi \\ \sin \varphi & \cos \varphi \end{bmatrix} \begin{bmatrix} 1 \\ 0 \end{bmatrix}
= \begin{bmatrix} \cos \varphi \\ \sin \varphi \end{bmatrix}
= [e_1, e_2] \begin{bmatrix} \cos \varphi \\ \sin \varphi \end{bmatrix} \\
T(e_2) &= \begin{bmatrix} \cos \varphi & -\sin \varphi \\ \sin \varphi & \cos \varphi \end{bmatrix} \begin{bmatrix} 0 \\ 1 \end{bmatrix}
= \begin{bmatrix} -\sin \varphi \\ \cos \varphi \end{bmatrix}
= [e_1, e_2] \begin{bmatrix} -\sin \varphi \\ \cos \varphi \end{bmatrix}
\end{aligned}
$$

根据式 (2.3.2) 得：

$$
T(e_1, e_2) = [e_1, e_2] \begin{bmatrix} \cos \varphi & -\sin \varphi \\ \sin \varphi & \cos \varphi \end{bmatrix}
$$

因此，平面旋转变换 $T$ 在标准基 $e_1, e_2$ 下的矩阵为：

$$
A = \begin{bmatrix} \cos \varphi & -\sin \varphi \\ \sin \varphi & \cos \varphi \end{bmatrix}
$$

---

### 💡 深度解析与学习指南

#### 1. 这道题的核心逻辑是什么？
这道题看似在“循环论证”（已知矩阵求矩阵），实际上它在演示**线性变换矩阵的构造原理**。
它在告诉你一个核心定理：
> **线性变换 $T$ 对应的矩阵 $A$，其第 $j$ 列就是基向量 $e_j$ 变换后的结果 $T(e_j)$。**

* **第1列**：就是 $X$ 轴单位向量 $(1,0)$ 旋转后的坐标 $(\cos \varphi, \sin \varphi)$。
* **第2列**：就是 $Y$ 轴单位向量 $(0,1)$ 旋转后的坐标 $(-\sin \varphi, \cos \varphi)$。
* **拼接**：把这两个列向量拼在一起，就得到了旋转矩阵 $A$。

#### 2. 知识点巩固
* **标准基 ($e_1, e_2$)**：在二维平面中，通常指 $e_1=\begin{pmatrix}1\\0\end{pmatrix}$ 和 $e_2=\begin{pmatrix}0\\1\end{pmatrix}$。
* **线性性质**：线性变换由它对基底的作用完全决定。一旦你知道了基变成了什么，你就知道了整个空间变成了什么。
* **矩阵乘法的几何意义**：$Ax$ 本质上是矩阵 $A$ 的列向量的线性组合。

#### 3. 易错点警示 ⚠️
* **列与行的混淆**：
    * **错误做法**：算出 $T(e_1)$ 后，把它横着写成了矩阵的第一行。
    * **正确做法**：$T(e_1)$ 必须是矩阵的**第一列**（竖着写）。
* **符号错误**：
    * 在旋转矩阵中，右上角是 $-\sin \varphi$，左下角是 $\sin \varphi$。很多同学容易记反。
    * **记忆技巧**：看 $e_2$（Y轴）。Y轴逆时针旋转 $\varphi$ 后，会倒向左边（X轴负方向），所以 $X$ 坐标（也就是矩阵右上角元素）必须是负的 ($-\sin \varphi$)。

#### 4. 做题技巧（如何快速写出变换矩阵）
以后遇到求“镜像”、“投影”或“旋转”矩阵的题目，**不要死记硬背矩阵公式**，使用**“基底跟踪法”**：

1.  **画图**：画出坐标轴。
2.  **看 $e_1$**：想象 $(1,0)$ 经过变换后去了哪里？写出新坐标，作为矩阵**第1列**。
3.  **看 $e_2$**：想象 $(0,1)$ 经过变换后去了哪里？写出新坐标，作为矩阵**第2列**。
4.  **完成**：这就直接写出了矩阵 $A$。

#### 5. 关于图中 $[e_1, e_2] [...]$ 这种写法的补充
你可能会觉得图中这一步很多余：
$$
= [e_1, e_2] \begin{bmatrix} \cos \varphi \\ \sin \varphi \end{bmatrix}
$$
这其实是更严谨的数学表达，意思是“向量是基底的线性组合”。
* 在标准基下，坐标向量 $\begin{bmatrix} x \\ y \end{bmatrix}$ 实际上代表 $x \cdot e_1 + y \cdot e_2$。
* 这道题为了展示理论的严密性，特意把基底写了出来。但在实际计算中，我们通常直接写坐标列向量即可。

## 例 2.3.5 线性变换的基变换与相似矩阵

### 题目描述

设平面旋转变换 $T \in \mathcal{L}(\mathbb{R}^2)$，其定义为
$$
T(x) = \begin{bmatrix} \cos \varphi & -\sin \varphi \\ \sin \varphi & \cos \varphi \end{bmatrix} x, \quad \forall x \in \mathbb{R}^2
$$
式中：$\varphi$ 为旋转角。

### 题目解答（原书推导过程）

经计算知，
$$
T(e_1, e_2) = [e_1, e_2] \begin{bmatrix} \cos \varphi & -\sin \varphi \\ \sin \varphi & \cos \varphi \end{bmatrix}
$$
故 $T$ 在 $\mathbb{R}^2$ 标准基 $e_1, e_2$ 下的矩阵为
$$
A = \begin{bmatrix} \cos \varphi & -\sin \varphi \\ \sin \varphi & \cos \varphi \end{bmatrix}
$$

若选取向量 $\zeta_1 = [\cos \varphi, -\sin \varphi]^\top$ 和 $\zeta_2 = [\sin \varphi, \cos \varphi]^\top$ 为 $\mathbb{R}^2$ 的一组基，则有
$$
\begin{aligned}
T(\zeta_1) &= \begin{bmatrix} 1 \\ 0 \end{bmatrix} \\
T(\zeta_2) &= \begin{bmatrix} 0 \\ 1 \end{bmatrix}
\end{aligned}
$$
$$
T(\zeta_1, \zeta_2) = I_2 = [\zeta_1, \zeta_2] \begin{bmatrix} \cos \varphi & -\sin \varphi \\ \sin \varphi & \cos \varphi \end{bmatrix}
$$
故 $T$ 在 $\mathbb{R}^2$ 基 $\zeta_1, \zeta_2$ 下的矩阵为
$$
B = \begin{bmatrix} \cos \varphi & -\sin \varphi \\ \sin \varphi & \cos \varphi \end{bmatrix}
$$

定义从基 $e_1, e_2$ 到 $\mathbb{R}^2$ 基 $\zeta_1, \zeta_2$ 的过渡矩阵 $P$ 为
$$
P = \begin{bmatrix} \cos \varphi & -\sin \varphi \\ \sin \varphi & \cos \varphi \end{bmatrix}
$$
则 $B = P^{-1}AP$ 成立。

---

### 💡 深度解析：这道题到底在玩什么？

#### 1. 核心目的：验证“相似矩阵”公式
这道题其实是在做一个**实验**。它先用笨办法算出新基底下的矩阵 $B$，然后用公式 $B = P^{-1}AP$ 算一遍，发现两者相等。
它想告诉你：**线性变换在不同基底下的矩阵是“相似”的**。

#### 2. 详细拆解（帮你读懂省略的步骤）

**第一部分：在旧基（标准基）下**
* 这部分和上一题（2.3.1）一模一样。
* $T$ 是旋转，在标准基 $e_1, e_2$ 下的矩阵就是那个经典的旋转矩阵 $A$。

**第二部分：在新基 $\zeta$ 下（最容易晕的地方）**
题目选取了一组很特殊的基 $\zeta_1, \zeta_2$。
* **计算 $T(\zeta_1)$**：
    $$
    A \zeta_1 = \begin{bmatrix} \cos \varphi & -\sin \varphi \\ \sin \varphi & \cos \varphi \end{bmatrix} \begin{bmatrix} \cos \varphi \\ -\sin \varphi \end{bmatrix} = \begin{bmatrix} \cos^2 + \sin^2 \\ \sin \cos - \cos \sin \end{bmatrix} = \begin{bmatrix} 1 \\ 0 \end{bmatrix} = e_1
    $$
* **计算 $T(\zeta_2)$**：
    同理算出结果是 $\begin{bmatrix} 0 \\ 1 \end{bmatrix} = e_2$。

**关键点来了：为什么 $B$ 还是那个旋转矩阵？**
根据定义，矩阵 $B$ 的列向量是 $T(\zeta_1)$ 和 $T(\zeta_2)$ **在 $\zeta$ 基底下的坐标**。
题目中写道：
$$
T(\zeta_1) = e_1 = \cos \varphi \cdot \zeta_1 + \sin \varphi \cdot \zeta_2
$$
$$
T(\zeta_2) = e_2 = -\sin \varphi \cdot \zeta_1 + \cos \varphi \cdot \zeta_2
$$
（这步它省略了，但这就是为什么矩阵 $B$ 长成那样的原因：你需要把 $e_1, e_2$ 用 $\zeta_1, \zeta_2$ 表示出来）。

#### 3. 易错点与坑 ⚠️

* **基变换矩阵 $P$ 的定义方向**：
    这是线性代数里最容易搞混的地方！
    * 有的书定义 $P$ 满足 $(\text{新基}) = (\text{旧基}) P$。
    * 有的书（如本题似乎隐含的逻辑）在验证 $B = P^{-1}AP$ 时，你需要非常清楚 $P$ 是把**新坐标转旧坐标**还是反过来。
    * **做题技巧**：只要看到 $B = P^{-1} A P$，就要记住 $P$ 的列向量通常就是**新基向量在旧基下的坐标**（或者反之，取决于教材约定，必须死磕教材定义）。

* **矩阵 $B$ 和 $A$ 相等是巧合吗？**
    * 在这道题里，**是的，这很特殊**。
    * 因为选取的这组新基 $\zeta$，本质上就是把旧基 $e$ 反向旋转了一下。
    * 通常情况下，换一组基，$B$ 和 $A$ 长得是不一样的，但它们**相似**（即特征值相同，行列式相同）。

#### 4. 知识点巩固（背下来！）
如果 $A$ 是线性变换在基 $e$ 下的矩阵，想求在基 $\zeta$ 下的矩阵 $B$：
1.  **方法一（定义法）**：算 $T(\zeta_i)$，然后把结果写成 $\zeta$ 的线性组合。
2.  **方法二（公式法）**：写出过渡矩阵 $P$，直接算 $P^{-1}AP$。

这道题展示了这两种方法殊途同归。

## 定理 2.2.4 线性映射的值空间和核空间

### 定义描述

设 $T \in \mathscr{L}(V, W)$，定义：
* **核空间 (Null Space / Kernel)**:
    $$N(T) = \{ x \in V \mid T(x) = \mathbf{0} \}$$
* **像空间 / 值空间 (Range / Image)**:
    $$R(T) = \{ y \in W \mid y = T(x), \forall x \in V \}$$

则 $N(T)$ 是 $V$ 的子空间，$R(T)$ 是 $W$ 的子空间。
我们称 $N(T)$ 是线性映射 $T$ 的核空间（或零空间），$R(T)$ 是线性映射 $T$ 的像空间（或值空间）；并称 $\dim N(T)$ 为线性映射 $T$ 的零度（或亏），$\dim R(T)$ 为线性映射 $T$ 的秩。

### 证明过程

**证明：**

**(1) 证明 $R(T)$ 是 $W$ 的子空间**
当 $x = \mathbf{0}$ 时，$Tx = \mathbf{0}$。故 $\mathbf{0} \in R(T)$，即 $R(T)$ 是 $W$ 的非空子集。
对任意向量 $\alpha, \beta \in R(T)$ 和 $k \in F$，有：
$$
\begin{aligned}
T(\alpha) + T(\beta) &= T(\alpha + \beta) \in R(T) \\
k T(\alpha) &= T(k\alpha) \in R(T)
\end{aligned}
$$
*(注：此处原文逻辑略有跳跃，严谨说法应为：存在 $u, v \in V$ 使得 $T(u)=\alpha, T(v)=\beta$，则 $\alpha+\beta = T(u)+T(v) = T(u+v) \in R(T)$)*
由线性子空间定义可知，$R(T)$ 为 $W$ 的子空间。

**(2) 证明 $N(T)$ 是 $V$ 的子空间**
因为 $T(\mathbf{0}) = \mathbf{0}$，故 $\mathbf{0} \in N(T)$，即 $N(T)$ 是 $V$ 的非空子集。
对任意向量 $\alpha, \beta \in N(T)$ 和 $k \in F$，有
$$
\begin{aligned}
T(\alpha + \beta) &= T(\alpha) + T(\beta) = \mathbf{0} + \mathbf{0} = \mathbf{0} \\
T(k\alpha) &= k T(\alpha) = k \mathbf{0} = \mathbf{0}
\end{aligned}
$$
因此，$N(T)$ 是 $V$ 的子空间。

---

### 💡 深度解析：如何形象理解这两个空间？

#### 1. 直观比喻（手电筒与影子）
想象从三维空间向二维地面打光（投影变换）。
* **核空间 $N(T)$ 是什么？**
    * 它是被“压扁消失”的部分。比如垂直于地面的光线，投射下去变成了一个点（零）。所有在垂直方向上的向量，都属于核空间。
    * **关键词**：**消失、归零**。如果 $N(T)$ 只有零向量，说明这个变换“不丢失信息”（单射）。
* **像空间 $R(T)$ 是什么？**
    * 它是“影子能覆盖的区域”。整个三维物体投射下来，可能只铺满了地面的一部分。这部分就是像空间。
    * **关键词**：**覆盖范围、输出能力**。

#### 2. 知识点强调
* **子空间判定三要素**：这道题的证明过程是标准的“子空间判定”模板，以后做证明题直接套用：
    1.  **非空**（通常验证零向量是否在里面）。
    2.  **加法封闭**（$a+b$ 还在里面）。
    3.  **数乘封闭**（$ka$ 还在里面）。

* **秩-零度定理 (Rank-Nullity Theorem)**：
    虽然图片里没写，但这是紧接着这个定理的最重要结论，一定要背下来：
    $$\dim V = \dim N(T) + \dim R(T)$$
    * **通俗解释**：输入维数 = 损失的维数 + 保留的维数。
    * *比如把3D物体压成2D照片（投影）：输入3维 = 垂直方向损失1维 + 平面保留2维。*

#### 3. 做题技巧
* **求 $N(T)$**：本质就是解齐次线性方程组 $Ax=0$。解出来的基础解系就是核空间的基。
* **求 $R(T)$**：本质就是求矩阵 $A$ 的**列向量组的极大无关组**。列空间就是像空间。
# 定理 2.2.4 线性映射的值空间和核空间

## 定义描述

设 $T \in \mathscr{L}(V, W)$，定义：

* **核空间 (Null Space / Kernel)**:
    $$
    N(T) = \{ x \in V \mid T(x) = \mathbf{0} \}
    $$
* **像空间 / 值空间 (Range / Image)**:
    $$
    R(T) = \{ y \in W \mid y = T(x), \forall x \in V \}
    $$

则 $N(T)$ 是 $V$ 的子空间，$R(T)$ 是 $W$ 的子空间。

我们称 $N(T)$ 是线性映射 $T$ 的核空间（或零空间），$R(T)$ 是线性映射 $T$ 的像空间（或值空间）；并称 $\dim N(T)$ 为线性映射 $T$ 的零度（或亏），$\dim R(T)$ 为线性映射 $T$ 的秩。

## 证明过程

**(1) 证明 $R(T)$ 是 $W$ 的子空间**

1.  **非空性**：当 $x = \mathbf{0}$ 时，$Tx = \mathbf{0}$。故 $\mathbf{0} \in R(T)$，即 $R(T)$ 是 $W$ 的非空子集。
2.  **封闭性**：对任意向量 $\alpha, \beta \in R(T)$ 和 $k \in F$（这意味着存在 $u, v \in V$ 使得 $T(u)=\alpha, T(v)=\beta$），有：
    $$
    \begin{aligned}
    T(u) + T(v) &= T(u + v) \implies \alpha + \beta \in R(T) \\
    k T(u) &= T(ku) \implies k\alpha \in R(T)
    \end{aligned}
    $$

由线性子空间定义可知，$R(T)$ 为 $W$ 的子空间。

**(2) 证明 $N(T)$ 是 $V$ 的子空间**

1.  **非空性**：因为 $T(\mathbf{0}) = \mathbf{0}$，故 $\mathbf{0} \in N(T)$，即 $N(T)$ 是 $V$ 的非空子集。
2.  **封闭性**：对任意向量 $\alpha, \beta \in N(T)$（这意味着 $T(\alpha)=\mathbf{0}, T(\beta)=\mathbf{0}$）和 $k \in F$，有：
    $$
    \begin{aligned}
    T(\alpha + \beta) &= T(\alpha) + T(\beta) = \mathbf{0} + \mathbf{0} = \mathbf{0} \\
    T(k\alpha) &= k T(\alpha) = k \mathbf{0} = \mathbf{0}
    \end{aligned}
    $$

因此，$N(T)$ 是 $V$ 的子空间。

---

## 💡 深度解析：如何形象理解这两个空间？

### 1. 直观比喻：手电筒与影子
想象从三维空间向二维地面打光（这是一个投影变换 $T$）：
* **像空间 $R(T)$（影子）**：
    三维物体投射下来，可能铺满了整个地面，也可能只是一条线。这个“能覆盖到的范围”就是像空间。
    * **关键词**：**输出能力**。
* **核空间 $N(T)$（被压扁的方向）**：
    垂直于地面的光线，投射下去变成了一个点（零向量）。所有竖直方向的向量，都被 $T$ “杀死了”。这些向量的集合就是核空间。
    * **关键词**：**信息丢失**。如果 $N(T)$ 只有零向量，说明变换没有“杀死”任何非零信息。

### 2. 考点与技巧
* **证明子空间的“三板斧”**：
    以后遇到让你证明某集合是子空间的题目，不要慌，直接套用这道题的证明模版：
    1.  **找零**：证明零向量在里面（非空）。
    2.  **加法**：证明 $a+b$ 还在里面。
    3.  **数乘**：证明 $ka$ 还在里面。
    
* **秩-零度定理 (Rank-Nullity Theorem)**：
    这是线性代数最核心的定理之一（虽然图中没写，但一定要联系起来）：
    $$
    \dim(\text{输入空间}) = \dim(\text{核空间}) + \dim(\text{像空间})
    $$
    * *解释：输入的总维数 = 损失掉的维数 + 保留下来的维数。*

    ## 例 2.2.7 抽象基底下的线性变换计算

## 题目描述

设 $\alpha_1, \alpha_2, \alpha_3$ 是线性空间 $V$ 的一组基，定义映射 $T$ 为：
$$
T(k_1\alpha_1 + k_2\alpha_2 + k_3\alpha_3) = (k_1 + k_2 + k_3)\alpha_1 + (k_2 + k_3)\alpha_2 + k_3\alpha_3
$$
式中：$k_1, k_2, k_3 \in F$。

1.  证明映射 $T$ 是 $V \rightarrow V$ 的线性变换。
2.  若 $\beta_0 = 2\alpha_1 + \alpha_2 + 3\alpha_3$，求 $T(\beta_0)$。

## 题目解答

### (1) 证明映射 $T$ 是线性变换
对任意向量 $x, y \in V$，令 $x = x_1\alpha_1 + x_2\alpha_2 + x_3\alpha_3$, $y = y_1\alpha_1 + y_2\alpha_2 + y_3\alpha_3$。

**加法保持性：**
$$
\begin{aligned}
T(x+y) &= T[(x_1+y_1)\alpha_1 + (x_2+y_2)\alpha_2 + (x_3+y_3)\alpha_3] \\
&= [(x_1+y_1) + (x_2+y_2) + (x_3+y_3)]\alpha_1 + \dots \\
&= T(x) + T(y)
\end{aligned}
$$
*(注：此处利用了系数的线性叠加性质，详细展开即对应系数相加)*

**数乘保持性：**
对任意 $k \in F$，
$$
\begin{aligned}
T(kx) &= T(kx_1\alpha_1 + kx_2\alpha_2 + kx_3\alpha_3) \\
&= (kx_1 + kx_2 + kx_3)\alpha_1 + (kx_2 + kx_3)\alpha_2 + kx_3\alpha_3 \\
&= kT(x)
\end{aligned}
$$
因此，映射 $T$ 是 $V \rightarrow V$ 的线性变换。

### (2) 求解 $T(\beta_0)$

**方法一（直接代入定义法）：**
令 $x_1=2, x_2=1, x_3=3$。则：
$$
\begin{aligned}
T(\beta_0) &= (2+1+3)\alpha_1 + (1+3)\alpha_2 + 3\alpha_3 \\
&= 6\alpha_1 + 4\alpha_2 + 3\alpha_3
\end{aligned}
$$

**方法二（矩阵坐标法 - 推荐）：**
对任意 $x = [x_1, x_2, x_3]^\top \in F^3$，将定义的系数关系写成矩阵形式：
$$
T(x_1\alpha_1 + x_2\alpha_2 + x_3\alpha_3) = [\alpha_1, \alpha_2, \alpha_3]
\begin{bmatrix}
1 & 1 & 1 \\
0 & 1 & 1 \\
0 & 0 & 1
\end{bmatrix}
\begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix}
$$
由于 $\beta_0$ 在基 $\alpha_1, \alpha_2, \alpha_3$ 下的坐标为 $x = [2, 1, 3]^\top$，将其代入上式得：
$$
\begin{bmatrix}
1 & 1 & 1 \\
0 & 1 & 1 \\
0 & 0 & 1
\end{bmatrix}
\begin{bmatrix} 2 \\ 1 \\ 3 \end{bmatrix}
= \begin{bmatrix} 2+1+3 \\ 1+3 \\ 3 \end{bmatrix}
= \begin{bmatrix} 6 \\ 4 \\ 3 \end{bmatrix}
$$
故 $T(\beta_0) = 6\alpha_1 + 4\alpha_2 + 3\alpha_3$。

---

## 💡 深度解析：从“笨办法”到“矩阵思维”

### 1. 题目在考什么？
这道题给出的变换定义非常**“代数化”**（一堆 $k$ 加来加去）。它实际上是在告诉你：线性变换本质上就是**坐标的线性变换**。

### 2. 方法二（矩阵法）是怎么来的？（重点！）
很多同学看不懂方法二里的那个矩阵 $\begin{bmatrix} 1 & 1 & 1 \\ 0 & 1 & 1 \\ 0 & 0 & 1 \end{bmatrix}$ 是怎么凑出来的。
其实非常简单，**看基向量变成了什么**（也就是上一题 2.3.1 的原理）：
* **看 $\alpha_1$**：令 $k_1=1, k_2=0, k_3=0$ 代入公式。
    得到 $1\alpha_1 + 0\alpha_2 + 0\alpha_3$。 $\rightarrow$ 矩阵第1列是 $\begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}$。
* **看 $\alpha_2$**：令 $k_1=0, k_2=1, k_3=0$ 代入公式。
    得到 $(0+1)\alpha_1 + 1\alpha_2 + 0\alpha_3$。 $\rightarrow$ 矩阵第2列是 $\begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix}$。
* **看 $\alpha_3$**：令 $k_1=0, k_2=0, k_3=1$ 代入公式。
    得到 $(0+0+1)\alpha_1 + (0+1)\alpha_2 + 1\alpha_3$。 $\rightarrow$ 矩阵第3列是 $\begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}$。

### 3. 做题技巧
* **首选矩阵法**：虽然方法一（代数字）在这道题里很快，但如果题目让你求 $T^n(\beta_0)$ 或者求逆变换，代数字就废了。**养成习惯，把线性变换写成矩阵形式**，是解决复杂问题的关键。
* **检查技巧**：如果你算出的矩阵是上三角或下三角矩阵（像这题一样），通常意味着变换具有某种“传递性”或“依赖性”（比如 $\alpha_3$ 的变换结果依赖 $\alpha_1, \alpha_2$，但 $\alpha_1$ 不依赖别人）。

## 例 2.2.8 线性变换的判定反例（数域的重要性）

### 题目描述

考察在其自身的线性空间 $\mathbb{Q}(\sqrt{3})$，定义映射 $T$ 为
$$
T(x + y\sqrt{3}) = x, \quad \forall x, y \in \mathbb{Q}
$$
判断 $T$ 是否为线性变换。

### 题目解答（原书证明过程）

映射 $T$ **不是**线性变换。

这是因为取标量 $k = \sqrt{3}$ 和向量 $v = \sqrt{3}$ 时，线性性质 $T(kv) = kT(v)$ 不成立：

$$
\begin{aligned}
\text{左边} &= T(\sqrt{3} \cdot \sqrt{3}) = T(3) = T(3 + 0\sqrt{3}) = 3 \\
\text{右边} &= \sqrt{3} \cdot T(\sqrt{3}) = \sqrt{3} \cdot T(0 + 1\sqrt{3}) = \sqrt{3} \cdot 0 = 0
\end{aligned}
$$

因为 $3 \neq 0$，所以 $T$ 不是线性变换。

---

### 💡 深度解析：为什么这个反例这么重要？

#### 1. 题目里的“陷阱”在哪里？
这道题乍一看，映射 $T$ 就是取实部（类似复数里取实部 $x$），取实部通常是线性的啊，为什么这里不是了？
**关键在于这句话：“在其自身的线性空间 $\mathbb{Q}(\sqrt{3})$”**。

这涉及到了线性代数最底层的定义：**线性空间是定义在某个数域 $F$ 上的**。
* **情况 A**：如果这道题说 $V = \mathbb{Q}(\sqrt{3})$ 是定义在 **有理数域 $\mathbb{Q}$** 上的空间。
    那么标量 $k$ 只能取有理数。你不能取 $k=\sqrt{3}$（因为它不是有理数）。在这种情况下，$T$ **是**线性变换！
* **情况 B（本题）**：题目说“在其自身”，意味着数域 $F = \mathbb{Q}(\sqrt{3})$。
    这意味着 $\sqrt{3}$ 被视为一个**标量（Scalar）**，而不仅仅是一个向量。作为线性变换，它必须对数域里所有的标量满足 $T(kv) = kT(v)$。

#### 2. 逻辑拆解
判定线性变换有两个核心条件：
1.  **加法性**：$T(u+v) = T(u) + T(v)$。
    * 这个映射满足吗？满足。
    * $T((x_1+y_1\sqrt{3}) + (x_2+y_2\sqrt{3})) = x_1+x_2 = T(\dots) + T(\dots)$。
2.  **齐次性（数乘）**：$T(k \cdot u) = k \cdot T(u)$。
    * 这个映射满足吗？**不满足**。
    * 正如题目所示，当标量 $k$ 带有 $\sqrt{3}$ 时，等式崩溃。

#### 3. 知识点巩固
做这类题时，一定要先问自己：**“我们现在的‘数’（Scalar）是谁？我们现在的‘向量’（Vector）是谁？”**

* 在此题中，$\sqrt{3}$ 既可以是向量（被变换的对象），也可以是标量（乘数）。
* 本题 $T$ 的定义实际上是把 $\sqrt{3}$ 这一项给“抹掉”了（变为0）。
* 但是如果你先用标量 $\sqrt{3}$ 去乘一个向量，原本的 $\sqrt{3}$ 变成了 $3$（变成了有理数部分），变换 $T$ 就无法把它“抹掉”了。这就导致了矛盾。

#### 4. 易错点总结
* **忽略基域**：看到 $T(x+y)=T(x)+T(y)$ 就急着下结论说是线性变换，忘记检查数乘性质。
* **混淆向量与标量**：在扩域问题（如 $\mathbb{C}$ 看作 $\mathbb{R}$ 上的空间 vs $\mathbb{C}$ 看作 $\mathbb{C}$ 上的空间）中，最容易搞错维数和线性性质。

## 定理 2.4.3 不同特征值的特征向量线性无关

### 定理描述

**矩阵 $A$ 的属于不同特征值的特征向量线性无关。**

### 证明过程

**证明**：
设 $\lambda_1, \cdots, \lambda_r$ 是 $n$ 阶矩阵 $A$ 的 $r$ ($2 \leqslant r \leqslant n$) 个互不相同的特征值，$\alpha_1, \cdots, \alpha_r$ 是分别属于特征值 $\lambda_1, \cdots, \lambda_r$ 的特征向量。

考察向量方程：
$$
k_1 \alpha_1 + \cdots + k_r \alpha_r = \mathbf{0} \quad \cdots\cdots (2.4.3)
$$
式中：$k_1, \cdots, k_r \in F$ 为待定系数。

对式 (2.4.3) 两端左乘矩阵 $A, A^2, \cdots, A^{r-1}$。
利用特征向量的性质 $A\alpha_i = \lambda_i \alpha_i$ 及 $A^k \alpha_i = \lambda_i^k \alpha_i$，得如下方程组：

$$
\begin{aligned}
k_1 \lambda_1 \alpha_1 + \cdots + k_r \lambda_r \alpha_r &= \mathbf{0} \\
k_1 \lambda_1^2 \alpha_1 + \cdots + k_r \lambda_r^2 \alpha_r &= \mathbf{0} \\
&\vdots \\
k_1 \lambda_1^{r-1} \alpha_1 + \cdots + k_r \lambda_r^{r-1} \alpha_r &= \mathbf{0}
\end{aligned}
$$

联立式 (2.4.3) 及上述方程组，写成矩阵形式：
$$
[k_1 \alpha_1, \cdots, k_r \alpha_r] D^T = \mathbf{0} \quad \cdots\cdots (2.4.4)
$$

其中 $D$ 为范德蒙德 (Vandermonde) 矩阵：
$$
D = \begin{bmatrix}
1 & 1 & \cdots & 1 \\
\lambda_1 & \lambda_2 & \cdots & \lambda_r \\
\vdots & \vdots & & \vdots \\
\lambda_1^{r-1} & \lambda_2^{r-1} & \cdots & \lambda_r^{r-1}
\end{bmatrix}
$$

由于范德蒙德行列式 $|D| = \prod_{1 \leqslant j < i \leqslant r} (\lambda_i - \lambda_j)$，而 $\lambda_i$ 互不相同，故 $|D| \neq 0$，即 $D$ 为可逆矩阵。
对式 (2.4.4) 两端右乘矩阵 $(D^T)^{-1}$ 得：
$$
[k_1 \alpha_1, \cdots, k_r \alpha_r] = \mathbf{0}
$$
即 $k_i \alpha_i = \mathbf{0} \ (i=1, \cdots, r)$。
由于 $\alpha_i$ 为特征向量（非零向量），故只能是系数 $k_i = 0 \ (i=1, \cdots, r)$。
因此，$\alpha_1, \cdots, \alpha_r$ 线性无关。证毕。

---

### 💡 深度解析：这个证明里的“神操作”

#### 1. 核心思想：范德蒙德矩阵的乱入
这个证明最精彩的地方在于如何处理那个看似无法解的方程 $k_1\alpha_1 + \dots = 0$。
* **困境**：只有一个方程，却有 $r$ 个未知数 $k_i$，正常是解不出来的。
* **破局**：利用 $A$ 的乘法！
    * 乘一次 $A$，每个 $\alpha_i$ 前面就多出来一个 $\lambda_i$。
    * 乘 $r-1$ 次，就制造出了 $r$ 个方程。
    * 这就强行构造出了一个**范德蒙德行列式**。
* **结论**：因为 $\lambda$ 互不相同，范德蒙德行列式不为0，这直接导致了唯一解 $k=0$。

#### 2. 几何直观（以二维为例）
想象 $\alpha_1$ 是 $X$ 轴方向，$\alpha_2$ 是 $Y$ 轴方向。
* 变换 $A$ 作用在 $X$ 轴上是拉伸 2 倍 ($\lambda_1=2$)。
* 变换 $A$ 作用在 $Y$ 轴上是拉伸 3 倍 ($\lambda_2=3$)。
* 如果这两个轴重合（线性相关），它们被 $A$ 拉伸的倍数必须一样。
* 但现在拉伸倍数不一样 (2 vs 3)，说明这两个轴绝对不可能重合。这就是“不同特征值对应特征向量线性无关”的直观解释。

#### 3. 该定理的威力（重要考点）
这个定理是判断矩阵是否可对角化的**充分条件**（但不是必要条件）：
1.  **推论 1**：如果 $n$ 阶矩阵 $A$ 有 $n$ 个**互不相同**的特征值，那么 $A$ 一定有 $n$ 个线性无关的特征向量。
2.  **推论 2**：进而，$A$ **一定可以对角化**。

> **注意**：如果特征值有重复（比如 $\lambda_1 = \lambda_2 = 1$），这个定理就用不上了。那时候我们需要检查“几何重数”（特征向量的个数）是否等于“代数重数”。但只要特征值不同，直接秒杀——必可对角化。

## 例 2.4.6 利用幂法求矩阵的特征值

### 题目描述

设 $A = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix}$，试利用幂法求 $A$ 的绝对值最大的特征值及其特征向量的近似值（$k$ 取到 5）。初始向量取 $x^{(0)} = \begin{bmatrix} 0 \\ 1 \end{bmatrix}$。

### 题目解答

**解**：由幂算法则知：

1.  **第 1 次迭代 ($k=0$)**:
    $$
    Ax^{(0)} = \begin{bmatrix} 1 \\ 3 \end{bmatrix}, \quad \mu_0 = 3
    $$
    *(注：此处书中计算隐含使用了 $x^{(0)}=[1,0]^T$，详见下文解析)*
    $$
    x^{(1)} = \frac{1}{3} \begin{bmatrix} 1 \\ 3 \end{bmatrix} = \begin{bmatrix} 0.3333 \\ 1 \end{bmatrix}
    $$

2.  **第 2 次迭代 ($k=1$)**:
    $$
    Ax^{(1)} = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix} \begin{bmatrix} 0.3333 \\ 1 \end{bmatrix} = \begin{bmatrix} 2.3333 \\ 5 \end{bmatrix}, \quad \mu_1 = 5
    $$
    $$
    x^{(2)} = \frac{1}{5} \begin{bmatrix} 2.3333 \\ 5 \end{bmatrix} = \begin{bmatrix} 0.4667 \\ 1 \end{bmatrix}
    $$

3.  **第 3 次迭代 ($k=2$)**:
    $$
    Ax^{(2)} = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix} \begin{bmatrix} 0.4667 \\ 1 \end{bmatrix} = \begin{bmatrix} 2.4667 \\ 5.4 \end{bmatrix}, \quad \mu_2 = 5.4
    $$
    $$
    x^{(3)} = \frac{1}{5.4} \begin{bmatrix} 2.4667 \\ 5.4 \end{bmatrix} = \begin{bmatrix} 0.4568 \\ 1 \end{bmatrix}
    $$

4.  **第 4 次迭代 ($k=3$)**:
    $$
    Ax^{(3)} = \begin{bmatrix} 2.4568 \\ 5.3704 \end{bmatrix}, \quad \mu_3 = 5.3704
    $$
    $$
    x^{(4)} = \begin{bmatrix} 0.4576 \\ 1 \end{bmatrix}
    $$

5.  **第 5 次迭代 ($k=4$)**:
    $$
    Ax^{(4)} = \begin{bmatrix} 2.4575 \\ 5.3724 \end{bmatrix}, \quad \mu_4 = 5.3724
    $$
    $$
    x^{(5)} = \begin{bmatrix} 0.4574 \\ 1 \end{bmatrix}
    $$

6.  **第 6 次迭代 ($k=5$)**:
    $$
    Ax^{(5)} = \begin{bmatrix} 2.4574 \\ 5.3723 \end{bmatrix}, \quad \mu_5 = 5.3723
    $$
    $$
    x^{(6)} = \begin{bmatrix} 0.4574 \\ 1 \end{bmatrix}
    $$

因此，$\mu_5 = 5.3723$ 是矩阵 $A$ 的绝对值最大的特征值估计，其特征向量估计为 $x^{(6)}$。

---

### 💡 深度解析：幂法 (Power Method) 是怎么工作的？

#### 1. 算法核心逻辑
幂法是一种寻找矩阵**主特征值**（绝对值最大的那个特征值）的迭代算法。
* **原理**：不管你从什么初始向量 $x^{(0)}$ 开始，只要不停地用 $A$ 去乘它（即计算 $A^k x^{(0)}$），结果向量的方向最终都会收敛到 $A$ 的主特征向量的方向。
* **直观理解**：矩阵 $A$ 就像一个“过滤器”，在反复乘的过程中，属于主特征值的成分会被放大得最快，其他成分相对就微不足道了。

#### 2. 归一化 (Normalization) 的作用
你在题目中看到，每一步算完 $Ax$ 后，都会除以最大元素 $\mu$（也就是 $x^{(k+1)} = Ax^{(k)} / \max(Ax^{(k)})$）。
* **目的**：防止数据溢出。如果不除这个数，向量里的数字会以 $5.37^k$ 的速度指数级爆炸，电脑很快就存不下了。
* **副产品**：每次提取出来的这个 $\mu$（最大分量），实际上就是在逼近特征值 $\lambda$。

#### 3. ⚠️ 书中的印刷错误（避坑指南）
请注意第一步计算：
* 题目给定 $x^{(0)} = \begin{bmatrix} 0 \\ 1 \end{bmatrix}$。
* 理论上：$Ax^{(0)} = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix} \begin{bmatrix} 0 \\ 1 \end{bmatrix} = \begin{bmatrix} 2 \\ 4 \end{bmatrix}$。
* **书中写的是**：$Ax^{(0)} = \begin{bmatrix} 1 \\ 3 \end{bmatrix}$。
* **原因分析**：$\begin{bmatrix} 1 \\ 3 \end{bmatrix}$ 其实是矩阵的第一列，这意味着作者在计算时实际使用的是 $x^{(0)} = \begin{bmatrix} 1 \\ 0 \end{bmatrix}$，但题目里误写成了 $\begin{bmatrix} 0 \\ 1 \end{bmatrix}$。
* **对结果的影响**：幂法对初始向量不敏感（除非运气极差选到了零空间里），所以即便初始向量写错了，迭代几次后结果依然会收敛到正确的 5.3723。

#### 4. 为什么收敛这么快？
* 幂法的收敛速度取决于 $|\lambda_2| / |\lambda_1|$ 的比值。
* 对于这个矩阵，$\lambda_1 \approx 5.372$, $\lambda_2 \approx -0.372$。
* 比值 $|-0.372 / 5.372| \approx 0.07$。这个数字非常小（接近0），说明干扰项消失得极快，所以只算了5步就得到了非常精确的结果。

## 例 3.1.2 利用初等变换求解线性方程组

### 题目描述

求解线性方程组 $Ax=b$，其中
$$
A = \begin{bmatrix} 1 & 1 & 1 \\ 1 & 1 & 2 \end{bmatrix}, \quad b = \begin{bmatrix} 1 \\ 2 \end{bmatrix}
$$

### 题目解答

**解**：由 $A$ 表达式易知 $\text{rank}(A)=2$，并对如下分块矩阵进行初等变换：

$$
\begin{bmatrix} A & I \\ I & * \end{bmatrix} = 
\left[
\begin{array}{ccc|cc}
1 & 1 & 1 & 1 & 0 \\ 
1 & 1 & 2 & 0 & 1 \\ 
\hline
1 & 0 & 0 & * & * \\ 
0 & 1 & 0 & * & * \\ 
0 & 0 & 1 & * & * \end{array}
\right]
\rightarrow
\left[
\begin{array}{ccc|cc} 
1 & 0 & 0 & 1 & 0 \\ 
0 & 1 & 0 & -1 & 1 \\ 
\hline
1 & -1 & -1 &  & \\ 
0 & 0 & 1 & * & \\ 
0 & 1 & 0 &  & 
\end{array}
\right]
$$

进而得：

$$
P^{-1} = \begin{bmatrix} 1 & 0 \\ -1 & 1 \end{bmatrix}, \quad Q^{-1} = \begin{bmatrix} 1 & -1 & -1 \\ 0 & 0 & 1 \\ 0 & 1 & 0 \end{bmatrix}, \quad P^{-1}b = \begin{bmatrix} 1 \\ 1 \end{bmatrix}
$$

由方程组 (3.1.5) 易得 $y = [1, 1, a]^\top$，进而解得

$$
x = Q^{-1}y = \begin{bmatrix} -a \\ a \\ 1 \end{bmatrix}
$$

上式即为线性方程组 $Ax=b$ 的解，其中 $a$ 为任意实数。

---

### 💡 深度解析：这是什么解题流派？

#### 1. 核心方法：初等变换法 (PAQ分解的变体)
我们在大一时习惯用**高斯消元法**（只做行变换）来解方程。但这道题使用的是**行列同时变换**的方法。
它的原理构建了一个大矩阵 $\begin{bmatrix} A & I_m \\ I_n & O \end{bmatrix}$：
* 对 $A$ 做**行变换**，右边的 $I_m$ 记录了行操作的过程，变成了 $P^{-1}$。
* 对 $A$ 做**列变换**，下边的 $I_n$ 记录了列操作的过程，变成了 $Q^{-1}$。
* 最终把 $A$ 变成了最简形式（标准形 $\begin{bmatrix} I_r & 0 \\ 0 & 0 \end{bmatrix}$）。

#### 2. 为什么这样做？（逻辑链条）
这个方法的逻辑非常漂亮，它把复杂问题转化为了“傻瓜”问题：

1.  **原问题**：$Ax = b$ （$A$ 纠缠不清，很难解）
2.  **坐标变换**：引入 $x = Q^{-1}y$ 和左乘 $P^{-1}$。
    方程变为：$(P^{-1} A Q^{-1}) y = P^{-1}b$。
3.  **中间状态**：
    * $P^{-1} A Q^{-1}$ 是个对角阵 $\begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \end{bmatrix}$（即变换后的 $A$）。
    * $P^{-1}b$ 是 $\begin{bmatrix} 1 \\ 1 \end{bmatrix}$（即变换后的 $b$）。
4.  **新问题**：
    $$
    \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \end{bmatrix} \begin{bmatrix} y_1 \\ y_2 \\ y_3 \end{bmatrix} = \begin{bmatrix} 1 \\ 1 \end{bmatrix}
    $$
    * 这眼就能看出：$y_1 = 1, y_2 = 1$。
    * $y_3$ 对应的系数全是0，所以 $y_3$ 是**自由变量**（任意实数 $a$）。
    * 得到 $y = [1, 1, a]^\top$。
5.  **还原**：最后用 $x = Q^{-1}y$ 把 $y$ 变回 $x$，就得到了最终通解。

#### 3. 易错点与技巧
* **自由变量漏写**：在解中间状态 $y$ 时，看到全0列，一定要反应过来这里有一个自由变量（$a$）。很多同学会直接写成 $[1, 1, 0]^\top$，那就错了，丢掉了通解的广泛性。
* **矩阵乘法顺序**：最后算 $x = Q^{-1}y$ 时，是一个 $3 \times 3$ 矩阵乘以 $3 \times 1$ 向量，千万别算错行列乘积。
    * 第一行：$1\cdot 1 + (-1)\cdot 1 + (-1)\cdot a = -a$
    * 第二行：$0\cdot 1 + 0\cdot 1 + 1\cdot a = a$
    * 第三行：$0\cdot 1 + 1\cdot 1 + 0\cdot a = 1$

## 例 3.2.1 矩阵的满秩分解

### 题目描述

求矩阵 $A = \begin{bmatrix} i & 1 & 1 \\ 1 & -i & 1 \end{bmatrix}$ 的满秩分解。

### 题目解答

**解**：
首先分析矩阵 $A$ 的列向量：
$$
a_1 = \begin{bmatrix} i \\ 1 \end{bmatrix}, \quad a_2 = \begin{bmatrix} 1 \\ -i \end{bmatrix}, \quad a_3 = \begin{bmatrix} 1 \\ 1 \end{bmatrix}
$$
观察发现 $a_2 = -i a_1$（因为 $-i \cdot i = 1, -i \cdot 1 = -i$），故 $a_1$ 和 $a_2$ 线性相关。
而 $a_1$ 和 $a_3$ 显然线性无关，故可选取 $a_1$ 和 $a_3$ 构成列空间 $R(A)$ 的一组基。

**步骤 1：构造矩阵 $B$**
$$
B = [a_1, a_3] = \begin{bmatrix} i & 1 \\ 1 & 1 \end{bmatrix}
$$

**步骤 2：构造矩阵 $C$**
向量 $a_1, a_2, a_3$ 在基 $a_1, a_3$ 下的坐标 $c_1, c_2, c_3$ 分别为：
* $a_1 = 1 \cdot a_1 + 0 \cdot a_3 \implies c_1 = \begin{bmatrix} 1 \\ 0 \end{bmatrix}$
* $a_2 = -i \cdot a_1 + 0 \cdot a_3 \implies c_2 = \begin{bmatrix} -i \\ 0 \end{bmatrix}$
* $a_3 = 0 \cdot a_1 + 1 \cdot a_3 \implies c_3 = \begin{bmatrix} 0 \\ 1 \end{bmatrix}$

因此，定义：
$$
C = [c_1, c_2, c_3] = \begin{bmatrix} 1 & -i & 0 \\ 0 & 0 & 1 \end{bmatrix}
$$

则 $A = BC$ 即为矩阵 $A$ 的满秩分解。

---

**【另一种解法】**
若选取 $a_2, a_3$ 作为基，则：
$$
B = [a_2, a_3] = \begin{bmatrix} 1 & 1 \\ -i & 1 \end{bmatrix}
$$
对应的坐标矩阵 $C$ 为（因为 $a_1 = i a_2$）：
$$
C = \begin{bmatrix} i & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}
$$
此亦为 $A$ 的满秩分解。

---

### 💡 深度解析：满秩分解的通法与技巧

#### 1. 什么是满秩分解？
对于一个 $m \times n$ 的矩阵 $A$（秩为 $r$），满秩分解就是把它拆成两个矩阵相乘：
$$
A = B_{m \times r} \cdot C_{r \times n}
$$
* **$B$ (列满秩矩阵)**：通常取 $A$ 的 $r$ 个线性无关的列。
* **$C$ (行满秩矩阵)**：通常是这些列对应的系数（坐标）。

#### 2. 万能解题模板（RREF法）
虽然这道题是靠“肉眼观察”找关系的，但面对更复杂的矩阵，推荐使用**行最简形 (RREF)** 方法：

1.  **求 RREF**：对 $A$ 做初等行变换，化为行最简形矩阵 $A_{RREF}$。
    * 本题中，RREF 实际上就是 $\begin{bmatrix} 1 & -i & 0 \\ 0 & 0 & 1 \end{bmatrix}$（即第一种解法里的 $C$）。
2.  **定 $B$**：看 RREF 中**主元（Pivot）**在哪几列。
    * 本题主元在第1列和第3列。
    * 所以 $B$ 就是原矩阵 $A$ 的第1列和第3列 ($[a_1, a_3]$)。
3.  **定 $C$**：**$C$ 就是 RREF 去掉全零行后的结果**。
    * 直接把 $A_{RREF}$ 的非零行抄下来，就是 $C$。

**结论**：用 RREF 法算出来的就是题中的第一种解法。这是一个标准化的流程，不需要靠猜。

#### 3. 易错点警示 ⚠️
* **复数运算**：这道题最大的坑在于虚数单位 $i$。
    * 记住 $1/i = -i$。
    * 检查线性相关时，$a_2 = k a_1$，如果看不出来，就算一下比值。
* **结果不唯一**：
    * 正如题中展示的，满秩分解不是唯一的。$B$ 选不同的基，$C$ 就会跟着变。
    * 如果你考试算出来的答案和标准答案不一样，不要急着划掉，先验算一下 $B \times C$ 是否等于 $A$。如果相等，通常也是对的。

## 例 3.3.1 

### 分解公式推导

在求得矩阵的 LU 分解 ($A = LU$) 后，如果 $A$ 是非奇异矩阵（即满秩），那么上三角矩阵 $U$ 的对角线元素 $u_{ii}$ 均不为 0。

此时，我们可以将 $U$ 进一步分解为**对角矩阵 $D$** 和**单位上三角矩阵 $V$** 的乘积，即 $U = DV$。

由此得到矩阵的 **LDU 分解**：
$$
A = L D V
$$
其中：
* **$L$**：单位下三角矩阵（对角线全是 1）。
* **$D$**：对角矩阵（存储了主元）。
* **$V$**：单位上三角矩阵（对角线全是 1）。

设上三角矩阵 $U$ 为：
$$
U = \begin{bmatrix} 
u_{11} & u_{12} & \cdots & u_{1n} \\
0 & u_{22} & \cdots & u_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & u_{nn} 
\end{bmatrix}
$$

提取对角线元素构成对角矩阵 $D = \mathrm{diag}(u_{11}, u_{22}, \dots, u_{nn})$则有：

$$
U = \begin{bmatrix} 
u_{11} & & & \\
& u_{22} & & \\
& & \ddots & \\
& & & u_{nn} 
\end{bmatrix}
\begin{bmatrix} 
1 & \frac{u_{12}}{u_{11}} & \cdots & \frac{u_{1n}}{u_{11}} \\
0 & 1 & \cdots & \frac{u_{2n}}{u_{22}} \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & 1 
\end{bmatrix}
= DV
$$

---

### 💡 深度解析：为什么要搞 LDU 分解？

#### 1. 唯一性定理
普通的 LU 分解（$A=LU$）中，通常规定 $L$ 是单位下三角，但 $U$ 不是单位上三角。这就显得有点“不对称”。
**LDU 分解**具有完美的**唯一性**：
> 对于任何非奇异矩阵 $A$，其 LDU 分解是**唯一**的。
> 即 $L$（单位下三角）、$D$（对角）、$V$（单位上三角）都是独一无二的。

#### 2. 对称矩阵的神器 ($LDL^T$)
如果矩阵 $A$ 是**实对称矩阵** ($A = A^T$)，那么它的 LU 分解会有很好的性质。
由于 $A = LDU$，转置后 $A^T = V^T D^T L^T$。
因为 $A=A^T$，且分解唯一，我们可以推出 $V = L^T$。
所以，对称矩阵可以分解为：
$$
A = L D L^T
$$
这被称为 **LDL 分解**（或改进的 Cholesky 分解）。它避免了标准 Cholesky 分解中需要“开根号”的麻烦（$D$ 中元素不需要开方），在计算机数值计算中非常受欢迎。

#### 3. 主元的物理意义
在 LDU 分解中，中间那个对角矩阵 $D$ 的元素，实际上就是高斯消元过程中产生的**主元 (Pivots)**。
* $d_{11}$ 是第一步的主元。
* $d_{22}$ 是消去第一列后，第二步的主元。
* 这些主元的乘积等于矩阵的行列式：$|A| = \prod d_{ii}$。

#### 4. 实战演练（基于例 3.3.1 的数据）
回到例 3.3.1，我们算出了：
$$
U = \begin{bmatrix} 1 & 2 & -1 \\
0 & -5 & 3 \\
0 & 0 & -\frac{12}{5} \end{bmatrix}
$$
要把它变成 $D V$：
1.  **提取对角线**：
    $$D = \begin{bmatrix} 1 & 0 & 0 \\
0 & -5 & 0 \\
0 & 0 & -\frac{12}{5} \end{bmatrix}$$
2.  **用 $U$ 的行除以对角线元素**：
    * 第 1 行：$[1, 2, -1] / 1 \rightarrow [1, 2, -1]$
    * 第 2 行：$[0, -5, 3] / (-5) \rightarrow [0, 1, -3/5]$
    * 第 3 行：$[0, 0, -12/5] / (-12/5) \rightarrow [0, 0, 1]$
    $$V = \begin{bmatrix} 1 & 2 & -1 \\
0 & 1 & -\frac{3}{5} \\
0 & 0 & 1 \end{bmatrix}$$

这样我们就得到了更“纯粹”的分解结构。

## 例 3.3.2 矩阵 LU 分解的存在性与唯一性

### 题目描述

求矩阵 $A_1$ 和 $A_2$ 的 LU 分解，其中
$$
A_1 = \begin{bmatrix} 0 & 1 \\
0 & 2 \end{bmatrix}, \quad
A_2 = \begin{bmatrix} 0 & 0 \\
1 & 1 \end{bmatrix}
$$

### 题目解答

**解**：$A_1$ 和 $A_2$ 可分解为如下表达式：

**(1) 对于 $A_1$：**
$$
A_1 = \begin{bmatrix} 0 & 1 \\
0 & 2 \end{bmatrix} = \begin{bmatrix} 1 & 0 \\
x & 1 \end{bmatrix} \begin{bmatrix} 0 & 1 \\
0 & y \end{bmatrix}
$$
展开右边矩阵乘积，对应元素需相等：
* $(2,1)$ 元：$x \cdot 0 + 1 \cdot 0 = 0$ (恒成立)
* $(2,2)$ 元：$x \cdot 1 + 1 \cdot y = x + y$
故只需满足 $x + y = 2$。
这意味着 $x, y$ 有无穷多组解（例如 $x=1, y=1$ 或 $x=0, y=2$ 等）。
**结论：矩阵 $A_1$ 的 LU 分解不唯一。**

**(2) 对于 $A_2$：**
假设存在 LU 分解：
$$
A_2 = \begin{bmatrix} 0 & 0 \\
1 & 1 \end{bmatrix} = \begin{bmatrix} 1 & 0 \\
* & 1 \end{bmatrix} \begin{bmatrix} 0 & 0 \\
0 & z \end{bmatrix}
$$
考察矩阵乘积的 $(2,1)$ 元素（即第二行第一列）：
* 左边 $A_2$ 的 $(2,1)$ 元是 **1**。
* 右边乘积的 $(2,1)$ 元是 $* \cdot 0 + 1 \cdot 0 = \mathbf{0}$。
* $1 \neq 0$，产生矛盾。
**结论：注意到 $*$ 不存在，故矩阵 $A_2$ 不能进行 LU 分解。**

---

### 💡 深度解析：为什么有的矩阵不能 LU 分解？

#### 1. 核心理论：顺序主子式 (Leading Principal Minors)
这道题触及了 LU 分解的**死穴**。
一个矩阵 $A$ 能进行标准 LU 分解（$A=LU$）的**充分条件**是：
> $A$ 的各阶顺序主子式（Leading Principal Minors）都**不为零**。

* **看 $A_2$**：它的左上角第一个元素（一阶顺序主子式）是 $0$。
    * 在消元过程中，这意味着我们要用 $0$ 做分母去消除下面的 $1$，这在数学上是不允许的。
    * 因此分解失败。这属于**“本质上无法分解”**。
    * *补救措施：必须引入置换矩阵 $P$（换行），进行 $PA=LU$ 分解。*

#### 2. 为什么 $A_1$ 也是 0 开头，却能分解？
* **看 $A_1$**：虽然它左上角也是 $0$，但注意它**正下方**那个元素也是 $0$。
    * 既然已经是 $0$ 了，我们就不需要“除以主元”去消它。这就巧妙地绕过了“不能除以0”的限制。
* **为什么不唯一？**
    * $A_1$ 是**奇异矩阵**（行列式为0）。
    * 对于奇异矩阵，其 LU 分解如果存在，通常是不唯一的。这是因为在构造过程中，某些列（如零列）无法约束变量，留下了“自由度”（比如本题中的 $x+y=2$）。

#### 3. 做题与避坑指南 ⚠️
拿到一个矩阵问“能不能 LU 分解”，不要想当然觉得都能分：

1.  **第一眼看左上角**：
    * 如果 $a_{11} = 0$，且第一列下面有非零数 $\rightarrow$ **绝对不能分解**（直接判死刑，如 $A_2$）。
    * 如果 $a_{11} \neq 0$，通常能分解（除非后面中间步骤遇到了除以0的情况）。
2.  **概念辨析**：
    * **存在性**：不是所有矩阵都有 LU 分解。
    * **唯一性**：只有当矩阵非奇异（满秩）且能分解时，LU 分解（规定 $L$ 对角线为1）才是唯一的。

    ## 例 3.4.1 利用 QR 算法求矩阵的特征值

## 题目描述

利用 QR 算法求矩阵的特征值，其中
$$
A = \begin{bmatrix} 2 & 1 & 0 \\
1 & 3 & 1 \\
0 & 1 & 4 \end{bmatrix}, \quad
B = \begin{bmatrix} 4 & 1 & -3 \\
-2 & 1 & 1 \\
2 & 1 & -1 \end{bmatrix}
$$

## 题目解答

### (1) 矩阵 A 的计算过程
令 $A = A_1$，并对矩阵 $A_1$ 进行 QR 分解：$A_1 = Q_1 R_1$，其中
$$
Q_1 = \begin{bmatrix} -0.8944 & 0.4082 & 0.1826 \\
-0.4472 & -0.8165 & -0.3561 \\
0 & -0.4082 & 0.9129 \end{bmatrix}
$$
$$
R_1 = \begin{bmatrix} -2.2361 & -2.2361 & -0.4472 \\
0 & -2.4495 & -2.4495 \\
0 & 0 & 3.2863 \end{bmatrix}
$$

构造矩阵 $A_2 = R_1 Q_1$，并对矩阵 $A_2$ 进行 QR 分解：$A_2 = Q_2 R_2$，其中
$$
A_2 = \begin{bmatrix} 3.0000 & 1.0954 & 0 \\
1.0954 & 3.0000 & -1.3416 \\
0 & -1.3416 & 3.0000 \end{bmatrix}
$$
$$
Q_2 = \begin{bmatrix} -0.9611 & 0.2235 & -0.1293 \\
-0.2582 & -0.8362 & 0.4839 \\
0 & 0.5008 & 0.8655 \end{bmatrix}, \quad
R_2 = \begin{bmatrix} -2.8983 & -1.5123 & 0.4124 \\
0 & -3.1891 & 2.4088 \\
0 & 0 & 1.0819 \end{bmatrix}
$$

同理，可得经过10次和20次迭代后的矩阵：
$$
A_{10} = \begin{bmatrix} 4.7285 & 0.0781 & 0 \\
0.0781 & 3.0035 & -0.0020 \\
0 & -0.0020 & 1.2680 \end{bmatrix}
$$
$$
A_{20} = \begin{bmatrix} 4.7321 & 0.0008 & 0 \\
0.0008 & 3.0000 & 0 \\
0 & 0 & 1.2679 \end{bmatrix}
$$

**结论**：观察 $A_{20}$，其非对角元素趋近于0，对角线元素趋近于特征值。
读者可与矩阵 $A$ 的精确特征值作对比，其中的精确特征值分别为：
$\lambda_1 = 3+\sqrt{3} \approx 4.7321$, $\lambda_2 = 3$, $\lambda_3 = 3-\sqrt{3} \approx 1.2679$。

---

### (2) 矩阵 B 的计算结果
对矩阵 $B$ 进行同样的 QR 迭代，部分计算结果如下：

$$
B_2 = \begin{bmatrix} 2.3333 & 0.3563 & 5.4554 \\
-0.7127 & 1.2381 & 0.5832 \\
-0.2182 & -0.2333 & 0.4286 \end{bmatrix}
$$
$$
B_{10} = \begin{bmatrix} 2.0496 & 1.0464 & 3.7781 \\
-0.0226 & 1.5219 & 3.9784 \\
-0.0151 & -0.3191 & 0.4286 \end{bmatrix}
$$
$$
B_{20} = \begin{bmatrix} 1.9992 & -0.8187 & 3.6550 \\
-0.0022 & -0.3326 & 3.7233 \\
-0.0007 & -0.7454 & 2.3333 \end{bmatrix}
$$
$$
B_{100} = \begin{bmatrix} 2.0000 & -0.8165 & 3.6515 \\
0 & -0.3333 & 3.7268 \\
0 & -0.7454 & 2.3333 \end{bmatrix}
$$

---

### 💡 深度解析：QR 算法在干什么？

#### 1. 核心魔法：为什么翻转 QR 能求特征值？
QR 算法的每一步迭代只做两件事：
1.  **分解**：$A_k = Q_k R_k$
2.  **反乘**：$A_{k+1} = R_k Q_k$

这看似简单的颠倒顺序，实际上做了一个**相似变换**：
$$
A_{k+1} = R_k Q_k = (Q_k^{-1} A_k) Q_k = Q_k^{-1} A_k Q_k
$$
**关键点**：相似矩阵拥有**相同的特征值**。
所以，无论你迭代多少次，$A_{100}$ 的特征值和原始矩阵 $A$ 的特征值是一模一样的！我们只是不断地把矩阵“揉”成更容易看出特征值的形状（上三角或准上三角）。

#### 2. 对比 A 和 B 的结果（实对称 vs 非对称）
这道题特意选了两个代表性的矩阵：

* **矩阵 A (实对称矩阵)**：
    * **现象**：$A_{20}$ 几乎变成了**对角矩阵**。
    * **原理**：实对称矩阵一定可以对角化。QR 算法会迅速把非对角元素“杀”成0。
    * **读数**：直接读对角线上的数字，就是特征值。

* **矩阵 B (非对称矩阵)**：
    * **现象**：$B_{100}$ 并没有变成对角阵。它的左下角有两个0，但右下角是一个 $2 \times 2$ 的非零块 $\begin{bmatrix} -0.3333 & 3.7268 \\ -0.7454 & 2.3333 \end{bmatrix}$。
    * **原理**：这是**实舒尔形式 (Real Schur Form)**。
    * **读数**：
        1.  第一列对角元是 **2.0000** $\rightarrow$ 这是第一个实特征值 $\lambda_1 = 2$。
        2.  右下角的 $2 \times 2$ 块对应一对**共轭复数特征值**。你需要求这个小矩阵的特征值（$\lambda_{2,3} = 1 \pm i$）。
    * **结论**：如果 QR 迭代收敛不到对角阵，说明原矩阵可能有复数特征值。

#### 3. 做题与应用技巧
* **别手算迭代**：考试通常不会让你手算 QR 迭代（除非只算一步 $A_2$），因为计算量巨大且全是小数。这道题是让你**理解过程**。
* **收敛性判断**：看左下角的元素（下三角部分）。当它们足够接近0时，就可以停止迭代了。
* **实际应用**：这是计算机（如 MATLAB 的 `eig()` 函数）计算特征值的核心算法的基础。它比求行列式 $| \lambda I - A | = 0$ 要稳定和快得多。

## 例 3.5.4 矩阵的高次幂计算

### 题目描述

设 $A = \begin{bmatrix} 2 & 1 \\ 2 & 3 \end{bmatrix}$，求 $A^{100}$。

### 题目解答

**解**：
由特征多项式 $f_A(\lambda) = |\lambda I - A| = (\lambda - 1)(\lambda - 4)$ 知，矩阵 $A$ 满足特征方程：
$$
A^2 - 5A + 4I = 0 \implies A^2 = 5A - 4I
$$
利用该式进行迭代计算（降次）：

$$
\begin{aligned}
A^3 &= A \cdot A^2 = A(5A - 4I) = 5A^2 - 4A \\
&= 5(5A - 4I) - 4A = 21A - 20I
\end{aligned}
$$

$$
\begin{aligned}
A^4 &= A \cdot A^3 = A(21A - 20I) = 21A^2 - 20A \\
&= 21(5A - 4I) - 20A = 85A - 84I
\end{aligned}
$$

$\vdots$

由归纳法可得通项公式（对于 $m=0, 1, \dots$）：
$$
A^m = \left( \frac{4^m - 1}{3} \right)A + \left( \frac{4 - 4^m}{3} \right)I
$$

由此，代入 $m=100$：

$$
\begin{aligned}
A^{100} &= \frac{4^{100}-1}{3} \begin{bmatrix} 2 & 1 \\ 2 & 3 \end{bmatrix} + \frac{4-4^{100}}{3} \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \\
&= \begin{bmatrix} 
\frac{2(4^{100}-1) + (4-4^{100})}{3} & \frac{(4^{100}-1)}{3} \\
\frac{2(4^{100}-1)}{3} & \frac{3(4^{100}-1) + (4-4^{100})}{3}
\end{bmatrix} \\
&= \begin{bmatrix} 
\frac{2 + 4^{100}}{3} & \frac{-1 + 4^{100}}{3} \\
\frac{-2 + 2 \times 4^{100}}{3} & \frac{1 + 2 \times 4^{100}}{3}
\end{bmatrix}
\end{aligned}
$$

> **注**：实际上，无论是可逆矩阵的逆矩阵还是幂矩阵，都可以看作矩阵函数的特例，我们将在第4章的4.7节矩阵函数中详细展开。

---

### 💡 深度解析：求 $A^n$ 的三种流派

这道题虽然给出了答案，但如果你在考场上遇到它，硬推归纳法公式（$A^m = c_1 A + c_0 I$）其实很难凑出那个系数。
这里我为你总结求 $A^n$ 的三种通用思路：

#### 1. 找规律法（本题方法）
* **适用场景**：$n$ 较小，或者 $A$ 的结构非常特殊（如幂零矩阵、对角阵）。
* **逻辑**：算出 $A^2, A^3$，观察系数的变化规律，然后用数学归纳法证明。
* **本题逻辑**：
    * $A^2 = 5A - 4I$
    * $A^3 = 21A - 20I$
    * 系数数列 $\{1, 5, 21, \dots\}$ 和 $\{0, -4, -20, \dots\}$ 其实都满足特征方程对应的递推关系。但现场推导通项公式压力很大。

#### 2. 对角化法（最通用、最推荐）* **原理**：如果 $A$ 能对角化，即 $A = P \Lambda P^{-1}$，那么 $A^n = P \Lambda^n P^{-1}$。
* **本题实战**：
    1.  **求特征值**：$\lambda_1 = 1, \lambda_2 = 4$。
    2.  **求特征向量**：
        * 对应 $\lambda_1=1$: $p_1 = [-1, 1]^\top$
        * 对应 $\lambda_2=4$: $p_2 = [1, 2]^\top$
    3.  **构造矩阵**：$P = \begin{bmatrix} -1 & 1 \\ 1 & 2 \end{bmatrix}$，求 $P^{-1}$。
    4.  **计算**：
        $$
        A^{100} = \begin{bmatrix} -1 & 1 \\ 1 & 2 \end{bmatrix} \begin{bmatrix} 1^{100} & 0 \\ 0 & 4^{100} \end{bmatrix} P^{-1}
        $$
    * **优点**：不需要猜系数，步骤死板但稳健。

#### 3. 西尔维斯特 (Sylvester) 公式法（秒杀法）
* **原理**：这是矩阵插值法的推论。如果 $A$ 有 $k$ 个互异特征值 $\lambda_i$，则：
    $$
    f(A) = \sum_{i=1}^k f(\lambda_i) \prod_{j \neq i} \frac{A - \lambda_j I}{\lambda_i - \lambda_j}
    $$
* **本题秒杀**：
    特征值 $\lambda_1=1, \lambda_2=4$，函数 $f(x)=x^{100}$。
    $$
    \begin{aligned}
    A^{100} &= 1^{100} \cdot \frac{A - 4I}{1 - 4} + 4^{100} \cdot \frac{A - 1I}{4 - 1} \\
    &= -\frac{1}{3}(A - 4I) + \frac{4^{100}}{3}(A - I)
    \end{aligned}
    $$
    合并同类项后：
    $$
    A^{100} = \left( \frac{4^{100}-1}{3} \right)A + \left( \frac{4-4^{100}}{3} \right)I
    $$
    **看！这一步直接得出了书上归纳法推了半天的公式。**

#### 4. 易错点与做题技巧 ⚠️
* **特征值 $1$ 的威力**：如果发现特征值有 1 或 0，计算会大幅简化。比如 $1^{100}=1$，项数直接变少。
* **公式检验**：算出 $A^{100}$ 的表达式后，代入 $n=1$ 试一下，看看等不等于 $A$。
    * 本题：$m=1 \implies \frac{4-1}{3}A + \frac{4-4}{3}I = A$。验证通过。

## 例 3.6.1 线性变换的对角化判定与计算

### 题目描述

设线性变换 $T \in \mathscr{L}(\mathbb{R}^3)$ 在 $\mathbb{R}^3$ 空间的一组基 $\zeta_1, \zeta_2, \zeta_3$ 下的矩阵为 $A$，即 $T[\zeta_1, \zeta_2, \zeta_3] = [\zeta_1, \zeta_2, \zeta_3]A$，其中
$$
A = \begin{bmatrix} 1 & 0 & -2 \\ 0 & 0 & 0 \\ -2 & 0 & 4 \end{bmatrix}
$$

问：
1.  线性变换 $T$ 可否对角化？
2.  若 $T$ 可对角化，试求满秩矩阵 $P$ 使 $P^{-1}AP$ 为对角矩阵。

### 题目解答

#### (1) 判定可否对角化

**方法一（最小多项式法）：**
由矩阵 $A$ 的特征多项式 $f_A(\lambda) = |\lambda I - A| = \lambda^2 (\lambda - 5)$ 得特征值 $\lambda_1 = \lambda_2 = 0, \lambda_3 = 5$。
验证矩阵多项式：
$$
A(A - 5I) = \begin{bmatrix} 1 & 0 & -2 \\ 0 & 0 & 0 \\ -2 & 0 & 4 \end{bmatrix} \begin{bmatrix} -4 & 0 & -2 \\ 0 & -5 & 0 \\ -2 & 0 & -1 \end{bmatrix} = O
$$
所以矩阵 $A$ 的最小多项式为 $m_A(\lambda) = \lambda(\lambda - 5)$。
注意到 $m_A(\lambda)$ 无重根（即全是互异的一次因式），故 $A$ 可对角化，即线性变换 $T$ 可对角化。

**方法二（几何重数法）：**
矩阵 $A$ 的特征多项式为 $f(\lambda) = \lambda^2(\lambda - 5)$，得到特征值 $\lambda_1 = \lambda_2 = 0, \lambda_3 = 5$。

* 对于重特征值 $\lambda = 0$：
    $$
    n - \text{rank}(0I - A) = 3 - \text{rank}\begin{bmatrix} -1 & 0 & 2 \\ 0 & 0 & 0 \\ 2 & 0 & -4 \end{bmatrix} = 3 - 1 = 2
    $$
    几何重数（2）等于代数重数（2）。

* 对于单特征值 $\lambda = 5$：
    $$
    n - \text{rank}(5I - A) = 3 - \text{rank}\begin{bmatrix} 4 & 0 & 2 \\ 0 & 5 & 0 \\ 2 & 0 & 1 \end{bmatrix} = 3 - 2 = 1
    $$
    几何重数（1）等于代数重数（1）。

故 $\sum_{i=1}^3 \dim E(\lambda_i) = 3$，故线性变换 $T$ 可对角化。

#### (2) 求解矩阵 P

* 对于 $\lambda_1 = \lambda_2 = 0$，解方程 $(0I - A)x = 0$：
    化简系数矩阵得到 $x_1 - 2x_3 = 0$，$x_2$ 任意。
    得到两个线性无关的特征向量，分别为 $x_1 = [2, 0, 1]^\top, x_2 = [0, 1, 0]^\top$。

* 对于 $\lambda_3 = 5$，解方程 $(5I - A)x = 0$：
    化简得 $2x_1 + x_3 = 0, x_2 = 0$。
    得到一个特征向量为 $x_3 = [1, 0, -2]^\top$。

令 $P = [x_1, x_2, x_3]$，即
$$
P = \begin{bmatrix} 2 & 0 & 1 \\ 0 & 1 & 0 \\ 1 & 0 & -2 \end{bmatrix}
$$
故 $P$ 为可逆矩阵，且满足
$$
P^{-1}AP = \text{diag}(0, 0, 5) = \begin{bmatrix} 0 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 5 \end{bmatrix}
$$

---

### 💡 深度解析：做题高手的“秘密武器”

#### 1. 秒杀技巧：对称矩阵必可对角化
拿到这道题，先别急着算秩。
**看一眼矩阵 $A$**：
$$
A = \begin{bmatrix} 1 & 0 & -2 \\ 0 & 0 & 0 \\ -2 & 0 & 4 \end{bmatrix}
$$
发现了吗？$A$ 是一个**实对称矩阵** ($A^T = A$)。
**定理**：**实对称矩阵一定可以对角化！** 且不同特征值对应的特征向量天然正交。
如果你知道这个定理，第一问直接一句话就能回答，完全不需要算秩或者最小多项式。

#### 2. 方法一 vs 方法二：哪个更好？
* **方法二（求秩法）**：这是最通用的“笨办法”。适用于所有矩阵。核心逻辑是检查“特征向量够不够用”（几何重数是否等于代数重数）。
* **方法一（最小多项式法）**：这更偏向理论技巧。如果矩阵能被一个无重根的多项式“零化”（代入后为0），它就能对角化。比如这里发现 $A(A-5I)=0$，说明 $A$ 像是一个投影或者简单的伸缩，结构很简单，一定能对角化。

#### 3. 计算细节：如何快速求零空间？
在求 $\lambda=0$ 的特征向量时，我们要解：
$$
\begin{bmatrix} -1 & 0 & 2 \\ 0 & 0 & 0 \\ 2 & 0 & -4 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} = 0
$$
* 观察第一行：$-x_1 + 2x_3 = 0 \implies x_1 = 2x_3$。
* 观察第二列：全是0。这意味着 $x_2$ 根本没有参与约束。**千万别把 $x_2$ 漏了！** $x_2$ 是自由变量，可以取任意值（比如1）。
* 所以我们才有了两个基向量：
    * 令 $x_3=1, x_2=0 \implies [2, 0, 1]^\top$
    * 令 $x_3=0, x_2=1 \implies [0, 1, 0]^\top$

#### 4. 易错点 ⚠️
* **P的顺序**：矩阵 $P$ 列向量的顺序必须和对角矩阵 $\Lambda$ 中特征值的顺序一一对应。
    * 如果你写 $\Lambda = \text{diag}(5, 0, 0)$，那么 $P$ 的第一列必须是 $[1, 0, -2]^\top$。
* **求逆矩阵**：题目只要求求 $P$，**没让你求 $P^{-1}$**！千万别手贱去算 $P$ 的逆矩阵，浪费时间且容易算错，除非题目明确要求验证。

## 例 3.9.1 矩阵 $A^H A$ 与 $A A^H$ 的特征值与奇异值

### 题目描述

设 $A = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \end{bmatrix}$，计算 $A^H A$ 和 $A A^H$ 的特征值。

### 题目解答

**解**：
1.  **计算 $A^H A$**：
    $$
    A^H A = \begin{bmatrix} 1 & 0 \\ 0 & 1 \\ 0 & 0 \end{bmatrix} \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \end{bmatrix} = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 0 \end{bmatrix} = \text{diag}(1, 1, 0)
    $$
    显然，$A^H A$ 的特征值为 $\lambda_1 = 1, \lambda_2 = 1, \lambda_3 = 0$。

2.  **计算 $A A^H$**：
    $$
    A A^H = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \end{bmatrix} \begin{bmatrix} 1 & 0 \\ 0 & 1 \\ 0 & 0 \end{bmatrix} = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} = I_2
    $$
    显然，$A A^H$ 的特征值为 $\lambda_1 = 1, \lambda_2 = 1$。

> **注 3**：在计算 $A^H A$ 或 $A A^H$ 的特征值时，可优先考虑计算**阶数较小**的矩阵。

---

## 📚 核心概念：奇异值定义 (Definition 3.9.1)

设 $A \in \mathbb{C}^{m \times n}$，$A^H A$ 的特征值满足 $\lambda_1 \ge \lambda_2 \ge \dots \ge \lambda_r > 0$，$\lambda_{r+1} = \dots = \lambda_n = 0$。
称
$$
\sigma_i = \sqrt{\lambda_i}
$$
为 $A$ 的**奇异值 (Singular Values)**。
特别地，称 $\sigma_i \ (i=1, \dots, r)$ 为 $A$ 的**正奇异值**。

---

## 💡 深度解析：SVD 的基石技巧

### 1. “大变小”的计算智慧
这道题的核心启示在“注3”。
* 矩阵 $A$ 是 $2 \times 3$ 的。
* $A^H A$ 是 **$3 \times 3$** 的矩阵（大）。
* $A A^H$ 是 **$2 \times 2$** 的矩阵（小）。

**重要性质**：$A^H A$ 和 $A A^H$ 拥有**完全相同**的非零特征值！
* $A^H A$ 的特征值：$\{1, 1, 0\}$
* $A A^H$ 的特征值：$\{1, 1\}$
* 区别仅仅在于多了几个 0。

**实战技巧**：
如果让你求一个 $100 \times 2$ 矩阵的奇异值：
* **笨办法**：算 $A^H A$ ($2 \times 2$)，求特征值开根号。 $\rightarrow$ **极快**。
* **死办法**：算 $A A^H$ ($100 \times 100$)，解 100 次多项式... $\rightarrow$ **算到天荒地老**。
* **结论**：永远先算维度小的那个方阵 (Gram Matrix)，算出非零特征值后，剩下的全补0即可。

### 2. 奇异值 $\sigma$ 的物理意义
为什么我们要定义 $\sigma_i = \sqrt{\lambda_i}$？
* 对于线性变换 $y = Ax$，奇异值 $\sigma$ 代表了变换在某些特定方向上的**“伸缩倍率”**。
* 最大奇异值 $\sigma_1$ 代表了矩阵 $A$ 能把向量拉长的最大倍数（即矩阵的 2-范数 $\|A\|_2$）。
* 这道题中 $\sigma_1 = \sqrt{1} = 1$，说明这个投影矩阵最厉害也就是保持长度不变（1倍），不会把向量拉长。

### 3. 易错点 ⚠️
* **一定要开根号**：题目求的是“奇异值”，计算过程用的是 $A^H A$ 的“特征值”。算出特征值 $\lambda$ 后，千万别忘了 $\sigma = \sqrt{\lambda}$。
* **非负性**：$A^H A$ 是半正定矩阵，其特征值一定是非负实数 ($\lambda \ge 0$)，所以开根号永远有意义。如果你算出了负特征值，那绝对是算错了。
