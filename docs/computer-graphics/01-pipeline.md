# 计算机体系结构期末复习：题目整理与参考答案

> 来源：用户提供的《体系结构期末复习(2)(1).docx》整理而来。  
> 说明：在尽量保留原意的前提下，对排版、术语与明显错误做了纠正；并为每题补充“我的答案”和简要分析。

## 题型与分值（原文整理）

- 选择题：10题，共20分  
- 判断题：10题，共20分  
- 名词解释：5题，共15分  
- 简答题：5题，共30分  
- 计算题：2题，共15分  

---

# 第一章 冯诺依曼结构与基础概念

## 1. 冯诺依曼结构的基本特点；与哈佛结构的区别

**归纳（考什么）**
- 说清冯诺依曼的组成与核心思想。
- 对比哈佛结构在“指令/数据存储与通路”上的差异及优缺点。

**整理后的要点（纠错+排版）**
- 冯诺依曼结构（Von Neumann）
  - 组成：CPU（控制单元CU、算术逻辑单元ALU、寄存器组、程序计数器PC 等）、主存储器、I/O 子系统。
  - 指令与数据**存放在同一存储器**，通常共享同一存储系统与访问通路（总线/通道）。
  - 指令执行总体呈顺序流（在分支/异常下改变）。
- 哈佛结构（Harvard）
  - 指令存储器与数据存储器**物理分离**，通常也有独立通路，可并行取指与访存。
  - 常见于嵌入式/数字信号处理；现代通用CPU多采用“改良哈佛”（L1 I/D Cache分离，低层统一）。

**我的答案**
- 冯诺依曼的核心是“存储程序思想”：指令与数据同等对待、统一存储、按地址访问；CPU按PC顺序取指执行。  
- 哈佛结构的核心差异是“指令/数据分离”：两套存储与通路使取指与数据访问可并行，提高吞吐，但在容量规划与灵活性上不如统一存储。

**分析**
- 对比题常见扣分点：只说“一个存储器/两个存储器”而不说**访问通路与并行性**；或忽略现代CPU多为“改良哈佛”。

---

## 2. “具有执行顺序指令的处理能力”是什么意思？

**归纳**
- 解释冯诺依曼机的顺序控制特征（顺序取指、顺序解释、顺序执行的抽象）。

**我的答案**
- 指CPU在程序控制下，默认按PC所指地址**顺序取出并执行**指令；下一条指令地址通常为 `PC ← PC + 指令长度`，仅在分支、跳转、异常、中断等情况下改变。

**分析**
- 这里的“顺序”是**程序语义层面**的顺序，并不否认流水线、乱序执行等实现层面的并行（它们需保证等价的可观察结果）。

---

## 3. 什么是“冯诺依曼瓶颈”？

**归纳**
- 定义瓶颈来源与表现（带宽/延迟限制、CPU-内存速度差距）。

**我的答案**
- 冯诺依曼瓶颈是指：指令与数据共享同一存储与通路，使CPU取指与数据访存争用带宽；加之CPU运算速度远高于主存，导致CPU常因等待内存而空转。

**分析**
- 常见扩展：缓存层次、预取、并行内存通道、增加带宽（HBM/DDR升级）、减少访存（局部性优化）等都是缓解手段。

---

## 4. 指令执行周期（取指-译码-执行）

**归纳**
- 描述典型指令周期步骤与关键部件（PC、IR、寄存器、ALU、存储器接口）。

**整理后的要点**
- 简化三阶段：取指(IF) → 译码(ID) → 执行(EX)
- 经典五阶段（流水线常用）：IF → ID → EX → MEM → WB（写回）

**我的答案**
1. **取指(IF)**：根据PC从指令存储/缓存取出指令到指令寄存器IR；PC更新到下一条指令地址。  
2. **译码(ID)**：控制单元解析操作码与字段，读寄存器操作数，生成控制信号。  
3. **执行(EX/MEM/WB)**：ALU完成算术/逻辑/地址计算；如需访存则MEM阶段访问数据缓存/主存；最后将结果写回寄存器或存储器。

**分析**
- 写清“PC→取指→PC更新”通常是得分点；把“访存/写回”补齐更完整。

---

## 5. 冯诺依曼模型的特征

**归纳**
- 列举模型特征并说明含义。

**我的答案（纠错版）**
- 以运算器为中心（早期模型描述；现代更强调控制/存储层次）。  
- 指令与数据在存储器中同等对待，统一编码（通常二进制）。  
- 存储器按地址访问、线性编址；每个存储单元位宽固定。  
- 指令由操作码与地址码/操作数字段组成。  
- 指令流在程序语义上顺序执行（可被分支/中断改变）。

**分析**
- 原文“以运算器为中心”是历史表述；答题时可补一句“现代CPU通过缓存/预测/乱序等优化，但不改变ISA语义”。

---

## 6. 哈佛模型：组织方式、优缺点与应用

**归纳**
- 给出组织结构与对比结论；说明典型应用场景。

**我的答案（整理+纠错）**
- 组织：处理器 + 指令存储器 + 数据存储器 + I/O。  
- 区别：指令/数据存储与通路分离，可并行取指与访存。  
- 优势：并行性更高，便于针对指令/数据分别优化（如不同位宽、不同存取时序）。  
- 劣势：灵活性较差（指令/数据空间不可动态互补），硬件更复杂。  
- 应用：MCU、DSP、部分高实时嵌入式系统；通用CPU多为“改良哈佛”。

---

## 7. 计算机体系结构设计思想（8个核心理念）

**归纳**
- 记忆型考点：八条理念及其一句话解释。

**我的答案（标准化表述）**
1. **面向摩尔定律的设计**：预判工艺迭代带来的晶体管/功耗/频率约束，做代际可演进设计。  
2. **面向抽象简化设计**：用抽象层隔离复杂度（ISA、虚拟内存、缓存一致性等）。  
3. **加速大概率事件（common case fast）**：优先优化常见路径，整体收益最大。  
4. **通过并行提高性能**：指令级/数据级/线程级/任务级并行。  
5. **通过流水线提高性能**：把指令执行分段重叠，提高吞吐。  
6. **通过预测提高性能**：分支预测、预取等减少等待。  
7. **存储器层次结构**：寄存器→Cache→主存→外存，利用局部性平衡容量/速度/成本。  
8. **通过冗余提高可靠性**：ECC、RAID、冗余计算与容错机制等。

**分析**
- “加速大概率事件”是常见简答题核心句；建议配一个例子（例如：缓存命中优化远比极端缺失路径优化更值）。

---

## 8. 摩尔定律；内存墙是什么？如何解决？

**归纳**
- 定义摩尔定律；解释“内存墙”并给出缓解手段。

**整理后的要点（纠错）**
- 摩尔定律常用表述：在成本相近条件下，芯片可集成的晶体管数量约每18-24个月翻倍（工程经验规律）。  
- 内存墙：CPU算力/频率提升速度长期快于DRAM延迟改善，导致内存访问成为性能瓶颈。

**我的答案**
- 缓解内存墙的典型方法：
  - **缓存层次与更大带宽**：多级Cache、更多内存通道、更高带宽内存（HBM等）。  
  - **降低有效延迟**：预取、乱序执行隐藏延迟、硬件多线程。  
  - **提升局部性与减少访存**：数据布局、分块（tiling）、软件缓存/算法优化。  
  - **并行化内存系统**：NUMA数据本地化、分布式内存/消息传递。

**分析**
- “解决”通常不是消除差距，而是通过层次化与隐藏延迟降低其对程序的可见成本。

---

## 9. 并发（Concurrency）与并行（Parallelism）的区别与例子

**归纳**
- 给出概念区分与典型例子。

**我的答案**
- **并发**：在同一时间段内处理多个任务，可能通过时间片切换“看起来同时”。例：单核CPU上的多任务OS、Web服务器并发连接。  
- **并行**：在同一时刻多个计算资源真正同时执行。例：多核并行、SIMD向量指令、GPU并行、集群并行。

**分析**
- 并发更关注“任务组织与响应”，并行更关注“同一时刻的计算资源利用”。

---

## 10. 流水线冒险（结构/数据/控制）与典型解决方法

**归纳**
- 分类、成因、解决思路（硬件/软件）。

**整理后的要点（补充更标准的分类）**
- 结构冒险：硬件资源冲突（同一周期争用同一端口/功能部件）。  
- 数据冒险：数据相关导致先后顺序约束（RAW/WAR/WAW）。  
- 控制冒险：分支/跳转导致取指不确定。

**原文（不删减）**

- 4.流水线冒险
- 1. 结构冒险（Structural Hazards）
- 结构冒险是指由于硬件资源冲突导致的流水线阻塞。例如，当多条指令需要同一硬件资源时，就会产生结构冒险。
- 解决方法一：增加硬件资源。通过增加资源来减少资源竞争，例如设置独立的指令存储器和数据存储器或设置独立的指令Cache和数据Cache。
- 解决方法二：流水线停顿（Stall）。在完成前一条指令对资源的访问时，暂停取后一条指令的操作，即插入空泡（Bubble）。
- 2. 数据冒险（Data Hazards）
- 数据冒险是指指令执行需要等待前一条指令的结果，即数据依赖。
- 解决方法一：插入空泡（Bubbles）。在指令间增加空泡周期，等待数据写回。
- 解决方法三：编译优化调度（静态调度）和动态调度。通过编译器优化指令顺序或在运行时动态调度来避免数据冒险。
- 3. 控制冒险（Control Hazards）
- 控制冒险是指由于分支指令导致CPU无法判断下一步需要执行哪些指令。
- 解决方法一：动态分支预测。通过动态分支预测指令进行解决，减少因分支预测错误导致的流水线刷新。
- 解决方法二：流水线停顿（Stall）。在分支指令的结果未确定前，暂停流水线，直到分支结果确定。
- 分支预测：
- ①静态分支预测：基于编译器的编译信息对分支指令进行预测，预测信息不再改变
- ②动态分支预测：依据程序运行的实时信息，不断对预测信息进行更新，具有较高的预测准确率


**我的答案**
- 结构冒险：增加资源（分离I/D Cache，多端口存储器）、或插入气泡（stall）。  
- 数据冒险：
  - **转发/旁路（forwarding/bypassing）**：减少等待写回。  
  - **插入气泡**：在依赖链上停顿。  
  - **编译调度/动态调度**：静态重排；或硬件动态调度（如Scoreboard、Tomasulo）。  
- 控制冒险：
  - **分支预测（静态/动态）** + **投机执行**；  
  - 或在分支决议前stall；  
  - 以及延迟分支（部分RISC历史方案）。

**分析**
- 答题时最好点出“RAW最常见”；并说明预测失败会带来“刷流水线”的代价。

---

## 11. Cache的目的；L1/L2/L3的位置与结构特点（必考）

**归纳**
- 说明Cache为何存在；多级Cache为何合理；各级典型位置/容量/共享属性。

**纠错提示（对原文明显错误的修正）**
- 现代通用CPU的L1/L2/L3通常都在**同一处理器芯片（die）内**；L3并非“CPU外部”，而是片上共享的最后级缓存（LLC）。  
- 原文给出的容量范围偏大：L1通常几十KB量级；L2通常数百KB到数MB；L3通常数MB到数十MB（依CPU而异）。

**我的答案**
- **目的**：利用时间/空间局部性，把“热点指令/数据”放在更快的存储层，降低平均访存时间（AMAT）。  
- **L1 Cache**：
  - 位置：每个核心内部；通常分离I-Cache与D-Cache（改良哈佛）。  
  - 特点：最小、最快、延迟最低（通常几个cycle）。  
- **L2 Cache**：
  - 位置：通常每核私有或小范围共享（依架构）。  
  - 特点：比L1大、比L1慢，用于降低L1缺失率/缺失代价。  
- **L3 Cache（LLC）**：
  - 位置：芯片内共享（多核共享更常见）。  
  - 特点：更大、更慢，缓冲跨核共享数据并降低访DRAM次数。

**分析**
- 多级Cache本质是在“延迟-容量-命中率”之间做折中：L1优化延迟，L2/L3优化缺失率与缺失损失。

---

## 12. 写直达（write-through）与写回（write-back）

**归纳**
- 两种写策略的定义；一致性、性能、硬件复杂度对比；常配合的写分配策略。

**我的答案**
- **写直达**：写命中时同时更新Cache与下层（内存/更低级Cache）。  
  - 优点：下层始终较新，实现简单、一致性更直观。  
  - 缺点：写带宽开销大，通常需要写缓冲（write buffer）。  
- **写回**：写命中时只更新Cache，并设置脏位（dirty）；替换时再回写下层。  
  - 优点：显著减少写下层次数，性能更好。  
  - 缺点：需要脏位与回写逻辑；掉电/一致性处理更复杂（工程上通过协议/ECC等保障）。  
- 常见组合：write-back + write-allocate；write-through + no-write-allocate（但并非唯一）。

**分析**
- 面试/考试常问“脏位”的作用：标记该Cache行是否被修改且未写回。

---

## 13. CPU性能：响应时间、吞吐率与CPU时间公式（必考/计算）

**归纳**
- 概念：响应时间 vs 吞吐率；用户/管理员关注点。  
- 公式：CPU时间与主频、CPI、指令条数的关系；如何做性能对比（speedup）。

**整理后的要点**
- 响应时间（Response time/Execution time）：完成单个任务的总耗时（用户更关心）。  
- 吞吐率（Throughput）：单位时间完成的任务数（数据中心更关心）。  
- 时钟周期 $T_{cycle}=1/f_{clock}$。

**我的答案**
- **CPU执行时间**（常用定量关系）：
  \[
  T_{CPU}=\text{Instruction Count}\times CPI\times T_{cycle}
         =\frac{\text{Instruction Count}\times CPI}{f_{clock}}
  \]
- **平均CPI**（按指令类型加权）：
  \[
  CPI_{avg}=\sum_i (CPI_i \cdot \text{freq}_i)
  \]
- **加速比**（同一工作负载对比）：
  \[
  Speedup=\frac{T_{old}}{T_{new}}
  \]

**分析**
- 计算题常见陷阱：混淆“主频提升”和“CPI变化”；或未做指令比例加权。

---

## 14. 基准测试程序：SPEC与Linpack

**归纳**
- 说清它们是什么、测什么、为什么有代表性/局限性。

**我的答案**
- **SPEC**：一套由标准性能评估组织发布的基准套件，尽量用真实/接近真实的应用负载衡量系统性能（常分整数/浮点等子集）。  
- **Linpack**：线性代数求解（密集矩阵）相关的基准，常以浮点运算性能（FLOPS）衡量；在高性能计算领域影响大。

**分析**
- 基准测试的核心是“可复现、可比较”；但它永远是“代表某类负载”，不能替代真实生产工作负载剖析。

---

## 15. ISA是什么？有什么特点？为什么是通用体系结构的关键抽象层？

**归纳**
- 定义：ISA作为软硬件边界；特点：稳定、可移植、兼容。

**我的答案**
- ISA（Instruction Set Architecture，指令集体系结构）是程序员可见的机器抽象：指令、寄存器、寻址方式、异常/中断、内存一致性语义等。  
- 作用：作为**软件与硬件的契约**。同一ISA可由不同微架构实现，使软件生态可长期演进与兼容。

**分析**
- 说“没有ISA软件无法使用硬件”是直观解释；更严谨的是：没有稳定ISA，就无法形成可移植的编译器/OS生态。

---

## 16. 微架构（Microarchitecture）是什么？与ISA的关系？

**归纳**
- 定义微架构；强调“同ISA不同实现”的性能差异来源。

**我的答案**
- 微架构是实现某一ISA的具体组织方式：流水线深度、乱序执行、分支预测、缓存层次、执行单元数量、发射宽度等。  
- 关系：ISA规定“做什么与可观察结果”，微架构决定“如何做与做到多快”。同一ISA在不同微架构上性能/功耗差异很大。

**分析**
- “微架构把ISA翻译成微操作”是许多CISC CPU的实现方式，但并非所有CPU都必须采用微操作。

---

## 17. 寻址空间 vs 内存空间（物理/虚拟）

**归纳**
- 区分“可产生的地址范围”与“实际安装/可用内存”；引入虚拟内存更稳妥。

**我的答案**
- **寻址空间（address space）**：CPU（或进程）能够表示并用于寻址的地址集合。例如32位字节寻址理论上可表示 $2^{32}$ 个字节地址。  
- **内存空间**：实际物理内存（RAM）容量与其物理地址范围；以及OS提供给进程的**虚拟地址空间**（通过页表映射到物理内存/外存）。  

**分析**
- 考试若只讲“16位地址=64KB”也能得分，但建议补一句“虚拟地址空间可能大于物理内存”。

---

## 18. 大端（Big-endian）与小端（Little-endian）

**归纳**
- 定义字节序；会判断存储布局。

**我的答案**
- 大端：高有效字节（MSB）存放在低地址。  
- 小端：低有效字节（LSB）存放在低地址。  
- 例：32位数 `0x11223344`  
  - 大端内存序（低→高地址）：`11 22 33 44`  
  - 小端内存序（低→高地址）：`44 33 22 11`

**分析**
- 常考点是“给出内存字节序，反推出整数值”或反过来。

---

## 19. 数据对齐（alignment）与数据打包（packing）（选择/计算）

**归纳**
- 对齐规则与结构体大小计算；对齐与性能/兼容的关系；打包的意义与风险。

**整理后的要点（标准规则）**
- 结构体成员的起始偏移需满足其对齐要求：`offset % align == 0`。  
- 成员对齐值通常为 `min(成员大小, 默认对齐数)`（不同ABI可不同）。  
- 结构体总大小需向最大对齐值取整（是最大对齐的整数倍）。  
- 打包（packed）通过降低对齐/取消填充节省空间，但可能带来未对齐访问开销甚至在部分架构上触发异常。

**我的答案**
- **数据对齐**：让数据按特定边界对齐，以匹配硬件访存粒度，提高访问效率并满足ISA/ABI要求。  
- **数据打包**：尽量减少结构体内部填充字节以节省空间（常用于网络协议/磁盘格式），但需谨慎处理跨平台与对齐访问问题。

**分析**
- 计算题建议写步骤：逐字段算偏移→插入padding→末尾向最大对齐取整。

---

## 20. 寻址模式（立即/寄存器/直接/间接/基址/变址/PC相对等）

**归纳**
- 列举常见寻址方式；说明适用场景（数组、结构体、分支）。

**我的答案（整理版）**
- 立即寻址：操作数在指令中（常量）。  
- 寄存器寻址：操作数在寄存器中。  
- 直接寻址：指令给出内存地址。  
- 间接寻址：地址存于寄存器或内存单元（指针）。  
- 基址+偏移：有效地址 `EA = Rb + imm`（结构体字段/栈帧）。  
- 基址+变址（索引）：`EA = Rb + Ri` 或 `EA = Rb + Ri*scale + imm`（数组访问）。  
- PC相对：`EA = PC + imm`（分支跳转、位置无关代码）。

**分析**
- “基址寄存器 vs 变址寄存器”的区分多为约定：基址更像对象/段起点，变址更像偏移/索引；本质上都可用于地址计算。

---

## 21. 位运算与原码/反码/补码；逻辑移位与算术移位（必考）

**归纳**
- 会做表示转换与移位；理解算术右移复制符号位；溢出判断。

**我的答案（纠错+简化）**
- 表示法：
  - 正数：原码=反码=补码（数值位相同）。  
  - 负数：
    - 反码：符号位不变，数值位按位取反。  
    - 补码：在反码基础上加1（或“符号位不变，数值位取反+1”）。  
- 移位：
  - 逻辑移位：左/右移空位补0（适用于无符号数）。  
  - 算术右移：复制符号位（适用于补码有符号数），近似等价于除以2（向负无穷取整）。  
  - 算术左移：空位补0，可能溢出（符号位改变或被移出位与符号不一致时）。  

**分析**
- 很多教材不强调“原码/反码的算术移位规则”，考试更常考的是**补码**及算术右移的符号扩展。

---

## 22. 二八法则与一九法则

**归纳**
- 记忆型：性能优化的经验规律。

**我的答案**
- 二八法则（80/20）：约20%的指令/代码消耗约80%的执行时间（强调热点）。  
- 一九法则（90/10）：约10%的代码路径贡献约90%的运行时间（另一种热点表述）。

**分析**
- 关联“common case fast”：先优化热点路径与关键循环。

---

## 23. RISC的特点（必考）

**归纳**
- 典型特征：load/store、固定长度、简单寻址、大寄存器、易流水。

**我的答案（结构化）**
- 指令设计：
  - 指令格式规则、倾向定长；操作简单、译码快。  
  - 寻址方式较少且简单。  
- 体系结构：
  - **Load/Store架构**：只有load/store访问内存，ALU操作只在寄存器间进行。  
  - 寄存器数量较多（大寄存器文件）。  
  - 常配合独立的I/D Cache（改良哈佛）以利流水。  
- 不足（常见）：代码密度可能较低；对编译器优化依赖较强；对旧软件二进制兼容性历史上较弱（取决于生态）。

**分析**
- 题目常把RISC与CISC对比：抓住“简单、规则、利于流水线/并行”的主线即可。

---

## 24. 局部性原理（时间/空间）与案例

**归纳**
- 定义两类局部性；给出案例说明其与缓存命中率的关系。

**我的答案**
- 时间局部性：刚访问过的数据/指令不久后可能再次访问（循环、函数热点）。  
- 空间局部性：访问某地址后，邻近地址也可能被访问（数组顺序遍历、顺序指令流）。  
- 案例：对二维数组，按行连续访问通常比按列跳跃访问命中率更高（取决于行主序/列主序与缓存行）。

**分析**
- 写出“缓存行（cache line）一次带入一段相邻字节”可以把空间局部性与Cache机制联系起来。

---

## 25. Cache基本术语：命中率、缺失率、缺失损失

**归纳**
- 给出定义与关系式。

**我的答案**
- 命中率 $h$：访问在该层Cache命中的比例。  
- 缺失率 $m$：$m = 1 - h$。  
- 缺失损失（miss penalty）：发生缺失时为从下层取回数据并恢复执行所付出的额外时间（含访问下层、填充、可能的回写等）。

**分析**
- 如需进一步定量，可写AMAT：$AMAT = HitTime + MissRate \times MissPenalty$（常见加分点）。

---

## 26. Cache设计：为什么L1小而快，L2/L3更大？

**归纳**
- 解释“延迟 vs 容量 vs 命中率”的权衡。

**我的答案**
- L1的目标是把**命中延迟**做到极低，因此容量不能太大（容量越大通常索引/标记比较与布线延迟越高）。  
- L2/L3通过更大容量降低缺失率，并作为L1缺失时的缓冲层，降低对DRAM的访问频率与缺失损失。

**分析**
- 一句话总结：L1优化**Hit time**，L2/L3优化**Miss rate / Miss penalty**。

---

# 第二章 并行计算

## 1. TOP500中常见操作系统？测试集？浮点单位？

**归纳**
- 记忆型：Linux、Linpack；FLOPS单位换算。

**我的答案（整理）**
- TOP500超算榜单中，最常见的操作系统为Linux（及其发行版/定制版）。  
- 经典测试基准：Linpack（常指HPL，高性能Linpack）。  
- 浮点性能单位：
  - $1\ \text{MFLOPS}=10^6$ 次浮点运算/秒  
  - $1\ \text{GFLOPS}=10^9$  
  - $1\ \text{TFLOPS}=10^{12}$  
  - $1\ \text{PFLOPS}=10^{15}$  

**分析**
- “千万亿=10^15=PFLOPS”是常考换算点。

---

## 2. 并行计算的三个基本条件

**归纳**
- 并行硬件、并行问题、并行编程环境三要素。

**我的答案**
1. 并行机：至少含两个及以上处理器/计算单元，并具备互连与通信能力。  
2. 问题具备并行度：可分解为可并行执行的子任务（并行算法设计）。  
3. 并行编程：在并行编程模型/环境下实现并行算法并正确运行（如MPI/OpenMP/CUDA等）。

**分析**
- 若子任务之间依赖强、通信占比高，则并行收益会大幅下降。

---

## 3. Flynn分类（SISD/SIMD/MISD/MIMD）（必考）

**归纳**
- 记四类，能说清“指令流/数据流”的含义与典型例子。

**我的答案**
- 指令流：处理器执行的指令序列；数据流：指令处理的数据序列。  
- 分类：
  - SISD：单指令流单数据流（传统单核顺序机的抽象）。  
  - SIMD：单指令流多数据流（向量指令、SIMD扩展、部分GPU执行模型）。  
  - MISD：多指令流单数据流（少见，更多是概念分类）。  
  - MIMD：多指令流多数据流（多核CPU、集群节点等主流并行机）。

**分析**
- 写例子更稳：CPU多核=MIMD；向量化/AVX=SIMD。

---

## 4. SIMD与SIMT的区别（必考）

**归纳**
- 共同点：单指令控制多数据；不同点：执行/寻址粒度与控制流。

**我的答案**
- 共同点：都追求数据级并行，在一个指令控制下对多份数据执行相同或相似操作。  
- SIMD（CPU常见）：
  - 指令显式操作“固定宽度向量寄存器”，数据通常需要组织成向量（连续/可打包）。  
- SIMT（GPU常见）：
  - 抽象为“单指令多线程”：线程各自有寄存器与地址空间，可对不同地址数据操作。  
  - 线程在硬件上以warp/wavefront等分组锁步执行；若分支发散会降低效率。

**分析**
- 一句话：SIMD是“向量宽度固定的指令级向量化”，SIMT是“线程抽象的数据并行”，更灵活但怕分支发散与非合并访存。

---

## 5. NUMA与ccNUMA的区别

**归纳**
- NUMA：非一致内存访问；ccNUMA：带缓存一致性的NUMA。

**我的答案**
- NUMA：物理内存分布在不同节点/处理器附近，访问本地内存延迟低，远端内存延迟高；逻辑上仍可共享地址空间。  
- ccNUMA：在NUMA基础上加入缓存一致性协议（cache-coherent），对程序员呈现更“共享内存式”的编程体验，但仍需关注数据本地化与一致性开销。

**分析**
- 性能关键：把数据放在“计算发生的节点附近”（first-touch策略、绑核、NUMA-aware分配）。

---

## 6. 阵列机（Array Processor）与向量机（Vector Processor）的区别

**归纳**
- 两者都与SIMD相关，但实现方式不同：多PE阵列 vs 向量流水/向量寄存器。

**我的答案（纠错+更清晰）**
- 向量机：提供向量数据表示与向量指令；通过向量功能部件的流水化，对向量元素成批处理（典型：早期Cray）。  
- 阵列机：由大量处理单元（PE）组成阵列，通常由单一控制部件广播同一指令，各PE对各自数据并行执行（SIMD阵列）。  
- 关系：两者都体现SIMD思想；向量机强调“向量流水/寄存器”，阵列机强调“资源复制的多PE并行”。

**分析**
- 简答题写“控制方式（单控制 vs 多控制）+ 数据组织（向量寄存器 vs 分布到PE本地）”通常就够。

---

## 7. socket / cluster / thread：概念与区别

**归纳**
- 区分硬件封装层级、集群级别与硬件线程。

**我的答案（纠错）**
- **Socket**：主板上的CPU插槽；对应一个物理CPU封装（package）。  
- **Core**（补充关键概念）：一个socket内可含多个核心，每核有独立的执行管线与部分私有缓存。  
- **Thread（硬件线程/超线程）**：一个核心可暴露多个逻辑执行上下文（共享部分执行资源，提升资源利用率）。  
- **Cluster**：由多台计算节点通过网络互连形成的系统（分布式/并行计算平台）。

**分析**
- 出题常用“1 socket = N cores = M hardware threads”；集群则是“多机”。

---

## 8. 分布式计算 vs 并行计算（必考）

**归纳**
- 耦合程度、通信频率、目标侧重点。

**我的答案**
- 并行计算：在一个系统或紧耦合环境下，用多个处理器加速求解同一问题；通信频繁、粒度更细、强调短执行时间。  
- 分布式计算：多台独立系统松耦合协同完成任务，强调可扩展性、高可用与容错；通信通常较粗粒度。

**分析**
- 记忆点：并行更关注“加速同一任务”，分布式更关注“规模与可靠性”。

---

## 9. 加速比与效率（必考）

**归纳**
- 公式与含义；会用于计算题。

**我的答案**
- 加速比：
  \[
  S=\frac{T_S}{T_P}
  \]
  其中 $T_S$ 为串行时间，$T_P$ 为并行时间。  
- 效率：
  \[
  E=\frac{S}{P}
  \]
  其中 $P$ 为处理器/并行度数量。

**分析**
- 若 $E$ 很低，通常意味着串行比例高或通信/同步开销大。

---

## 10. 阿姆达尔定律（必考）

**归纳**
- 串行比例限制并行加速；写出公式与结论。

**我的答案（纠错：给出标准公式）**
- 设程序串行部分占比为 $s$，可并行部分占比为 $1-s$，使用 $P$ 个处理器，则：
  \[
  S(P)=\frac{1}{s+\frac{1-s}{P}}
  \]
- 当 $P\to\infty$，最大加速比：
  \[
  S_{max}=\frac{1}{s}
  \]

**分析**
- 常见加分：提及Gustafson定律用于“扩大问题规模”的情境，但题目若只考阿姆达尔，上式即可。

---

## 11. 数据相关性：RAW / WAR / WAW（必考）

**归纳**
- 三类数据相关与含义；与流水线/乱序的联系。

**我的答案**
- RAW（Read After Write，写后读）：后续读依赖前面的写结果（真相关）。  
- WAR（Write After Read，读后写）：后续写不能早于前面的读（反相关，常由寄存器复用引起）。  
- WAW（Write After Write，写后写）：两次写的顺序必须保持（输出相关）。

**分析**
- 乱序执行通常通过寄存器重命名消除WAR/WAW，但RAW需要等待数据产生或通过转发解决。

---

## 12. 并行粒度：粗粒度 vs 细粒度（必考）

**归纳**
- 定义粒度与通信/计算比例。

**我的答案**
- 并行粒度是线程/进程间两次通信或同步之间的计算量尺度。  
- 粗粒度：计算多、通信少，开销占比低，易扩展。  
- 细粒度：通信频繁、同步开销高，但可暴露更多并行性（对低延迟互连与高效运行时要求高）。

**分析**
- 选择题常用“通信频率高/低”来区分。

---

## 13. 流水线：段、深度、瓶颈、通过时间、排空时间；流水线分类（必考）

**归纳**
- 术语定义 + 分类（静态/动态、线性/非线性、顺序/乱序）。

**我的答案（整理）**
- 段（stage）：流水线的子过程及其硬件部件。  
- 深度：段数。  
- 瓶颈：耗时最长的段决定最小周期（节拍）。  
- 通过时间（latency）：首个任务从进入到输出的时间（近似为段延迟之和）。  
- 排空时间：最后一个任务从进入到输出所需的时间（包含尾部段执行）。  
- 分类（按原文框架）：
  - 静态/动态：同一时刻是否只能执行一种功能连接方式。  
  - 线性/非线性：是否存在反馈回路。  
  - 顺序/乱序：任务输出顺序是否必须与输入顺序一致。

**分析**
- 计算题通常会给出各段时间，要求找瓶颈、求吞吐率/加速比/效率。

---

## 14. 流水线的指标：吞吐率、加速比、效率（必考）

**归纳**
- 写出核心公式（在常见“等段时间”假设下）。

**我的答案（常用模型）**
- 设流水线段数为 $k$，流水线节拍为 $\tau$（一般取最大段时间），连续处理 $n$ 个任务：  
  - 流水线完成时间：
    \[
    T_k=(k+n-1)\tau
    \]
  - 吞吐率（稳态）：
    \[
    \text{TP}\approx \frac{1}{\tau}
    \]
  - 加速比（与非流水顺序执行比较，顺序时间 $T_s=n\cdot k\tau$）：
    \[
    S=\frac{T_s}{T_k}=\frac{nk\tau}{(k+n-1)\tau}=\frac{nk}{k+n-1}
    \]
  - 效率：
    \[
    E=\frac{S}{k}=\frac{n}{k+n-1}
    \]

**分析**
- 当 $n\gg k$ 时，$S\to k$，效率 $E\to 1$；小任务批量时，填充/排空开销显著。

---

## 15. MPI / OpenMP / CUDA：适用场景与特点

**归纳**
- 分布式内存 vs 共享内存 vs 异构加速。

**我的答案（纠错：原文“openAl”应为OpenCL更贴切，此处按课程常见三件套回答）**
- MPI（消息传递）：适合分布式内存/集群；显式通信（点对点/集合通信），可扩展性强。  
- OpenMP（共享内存多线程）：适合单机多核；通过编译指示/库函数并行化循环与任务，开发成本较低。  
- CUDA（GPU并行）：适合数据并行、吞吐导向任务；Host-Device模型、kernel以grid/block/thread组织。

**分析**
- 面试常问：“MPI+OpenMP混合并行”“CUDA+MPI多GPU集群”——把适用层级说清即可。

---

## 16. CPU与GPU的区别

**归纳**
- 低延迟 vs 高吞吐；控制复杂度 vs 并行规模；内存带宽差异。

**我的答案**
- CPU：少量强核，复杂控制与大缓存，擅长低延迟与分支复杂任务。  
- GPU：大量简单算术单元，高内存带宽与海量并行，擅长规则的数据并行与吞吐型计算；控制流与分支发散成本高。

**分析**
- 形象比喻可用，但考试建议以“架构目标与资源配置”来解释更稳。

---

## 17. CUDA术语：host / device / grid / block / thread

**归纳**
- 能画出层次结构或文字说明执行组织。

**我的答案**
- Host：CPU端，负责调度、内存管理、启动kernel。  
- Device：GPU端，负责并行执行kernel。  
- Kernel：在GPU上运行的函数。  
- Grid：一次kernel启动形成的线程块集合。  
- Block：线程块，同一block内线程可用共享内存通信并可同步。  
- Thread：最小执行实例；GPU以warp/wavefront等方式成组调度。

**分析**
- 常见加分：指出“block之间通常不可直接同步/通信（需借助全局内存与多kernel阶段）”。

---

# 第三章 云计算相关技术

## 1. 云计算的五个特征、四个部署模型、三个服务模式（必考）

**归纳**
- 典型按NIST：5特征+4部署+3服务；会背会解释。

**我的答案（补全原文空缺）**
- 五个基本特征（NIST常见表述）：
  1. 按需自助服务（On-demand self-service）
  2. 广泛网络访问（Broad network access）
  3. 资源池化（Resource pooling，多租户）
  4. 快速弹性伸缩（Rapid elasticity）
  5. 可计量服务（Measured service）
- 四个部署模型：
  - 公有云（Public cloud）
  - 私有云（Private cloud）
  - 社区云（Community cloud）
  - 混合云（Hybrid cloud）
- 三个服务模式：
  - IaaS：基础设施即服务  
  - PaaS：平台即服务  
  - SaaS：软件即服务  

**分析**
- 题目若要求“举例”，可用OpenStack（IaaS）、Kubernetes/Heroku类（PaaS）、企业SaaS应用等。

---

## 2. 虚拟化的分类（必考）

**归纳**
- 按实现手段（软件/硬件辅助）、按客户机感知（全/半虚拟化）、按部署形态（Type-1/Type-2）。

**我的答案（纠错：原文Type1/Type2写反了）**
1. 按是否依赖硬件辅助：
   - 软件虚拟化：通过二进制翻译/陷入模拟等实现（如早期QEMU等模式）。  
   - 硬件辅助虚拟化：CPU提供VT-x/AMD-V等机制（Root/Non-root），降低VMM开销。  
2. 按客户机是否需修改：
   - 全虚拟化（Full virtualization）：Guest OS无需修改；敏感操作由VMM/硬件处理（如VMware、KVM等常见形态）。  
   - 半虚拟化（Para-virtualization）：Guest OS感知虚拟化并配合调用hypercall（典型：早期Xen PV）。  
3. 按Hypervisor部署形态（标准定义）：
   - **Type-1（裸机型）**：Hypervisor直接运行在硬件之上（如ESXi、Xen等）。  
   - **Type-2（宿主型）**：Hypervisor运行在宿主OS之上（如VMware Workstation、VirtualBox）。

**分析**
- “Type-1/Type-2”是高频踩坑点：记住Type-1=bare metal。

---

## 3. 0/1/3模型与特权级（ring）

**归纳**
- x86特权级ring0-3；Xen PV的0/1/3；硬件虚拟化引入ring -1。

**我的答案**
- x86传统有4个特权级：ring0（内核）到ring3（用户态）（ring1/2多数OS不使用）。  
- Xen早期半虚拟化常用“0/1/3”：
  - Hypervisor在ring0  
  - Guest OS内核降权运行在ring1  
  - 应用在ring3  
- 硬件虚拟化引入“ring -1”（VMX root mode）：Hypervisor在root mode，Guest OS可继续在其“看似ring0”的非root模式运行。

**分析**
- 该题的关键是说明“为什么要降权”：让Guest敏感操作可被捕获/控制。

---

## 4. 特权指令与敏感指令（必考）

**归纳**
- 定义两类指令；说明为何x86早期难以完全虚拟化；半虚拟化如何解决。

**我的答案（整理）**
- 特权指令：只能在最高特权级执行，否则触发异常。  
- 敏感指令：在虚拟化语境下，其行为依赖于系统状态/特权级，若不被VMM控制会破坏隔离或语义。  
- 关键点：在某些体系结构中，存在“敏感但不特权”的指令，它们在低特权级不会trap，导致VMM难以截获。  
- 半虚拟化通过修改Guest OS，把这些操作改为显式hypercall；硬件辅助虚拟化则让相关操作在硬件层面可被捕获。

**分析**
- 这题常考“Popek & Goldberg”判据：可虚拟化体系结构要求敏感指令是特权指令的子集（便于trap-and-emulate）。

---

## 5. SDN的特点（必考）

**归纳**
- 概念+三大特征：可编程、控制/转发分离、逻辑集中控制；可提OpenFlow。

**我的答案**
- SDN（Software Defined Networking）核心是把网络从“设备驱动”转为“软件定义”：
  - 控制平面与数据平面分离（交换设备主要做转发）。  
  - 逻辑集中控制（控制器统一下发策略）。  
  - 网络可编程（通过北向/南向接口，OpenFlow等协议实现转发表控制）。  

**分析**
- 简答题可补“好处”：策略统一、自动化运维、快速迭代；“挑战”：控制器可靠性、规模与一致性。

---

## 6. 容器（Docker）特点；与虚拟机的区别

**归纳**
- 容器是OS级虚拟化：共享内核；隔离机制；优缺点。

**我的答案**
- 容器：通过namespace/cgroup等机制实现进程级隔离与资源限制，把应用及其依赖打包成可迁移单元。  
- 与虚拟机对比：
  - 容器共享宿主机内核，启动快、开销小、密度高；但内核层隔离边界弱于完整虚拟机，且对内核版本依赖更强。  
  - 虚拟机提供硬件级抽象，Guest OS独立，隔离更强但开销更大、启动更慢。

**分析**
- 课程题常用一句：VM“虚拟硬件+独立OS”，容器“虚拟运行环境+共享内核”。

---

## 7. SOA与微服务的区别

**归纳**
- 维度：粒度、治理方式、通信/集成、部署与演进。

**我的答案（整理）**
- SOA：强调服务复用与服务编排，常配合统一治理与ESB（企业服务总线），服务粒度相对更粗，侧重企业级集成。  
- 微服务：强调小而自治的服务，独立部署、去中心化治理、轻量通信（REST/gRPC/消息队列），强调持续交付与弹性伸缩；服务粒度更细。  

**分析**
- 不是“SOA=旧、微服务=新”，而是适用场景不同：组织规模、治理成本、团队自治与交付节奏决定选型。

---

# 附：建议的复习与答题策略（可选）

- 计算题优先掌握：CPU时间公式、加速比/效率、阿姆达尔定律、流水线指标公式、对齐/结构体大小计算。  
- 简答题优先背诵：8大设计理念、RISC特点、SIMD vs SIMT、云计算5+4+3、SDN三特征、虚拟化Type-1/Type-2。  
- 选择/判断题高频坑：字节序、对齐规则、write-through vs write-back、RAW/WAR/WAW、并发vs并行、NUMA数据本地化。

---
